{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ec65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델내에서 사용되는 워드 임베딩 처리 및 학습\n",
    "# 학습 목표 \n",
    "# - 각 단어마다 vocab 전체와 확률 비교 → 정답과 비교 → 손실 계산 → 파라미터 업데이트 → logits 생성이라는 흐름으로 학습\n",
    "# - 모델학습 과정에서 임베딩이 점점 의미를 반영하게 되고, 비슷한 단어끼리 가까워지는 성질이 생긴다\n",
    "# - 임베딩 weight 업데이트, 임베딩 행렬의 각 벡터가 학습을 통해 의미 공간에서 위치를 바꾸는 것이다. \n",
    "# - 임베딩은 토큰화되어진 문자를 벡터로 변환하는것이 아니다\n",
    "\n",
    "# 1. 토크나이저 -> 인덱스 변환 \n",
    "# - 텍스트를 토큰 단위로 분리 (WordPiece, BPE, SentencePiece 등) \n",
    "# - 각 토큰을 고유 인덱스로 매핑\n",
    "# 2. 임베딩 레이어 생성 \n",
    "# - PyTorch의 nn.Embedding을 사용해 인덱스를 고정 길이 벡터로 변환 \n",
    "# - 학습 가능한 파라미터로 초기화 -> 학습 과정에서 업데이트 됨 \n",
    "# 3. 학습 루프 및 임베딩 학습 방식 \n",
    "# - 랜덤 초기화 후 학습 : 모델 학습 과정에서 임베딩이 점차 의미를 학습 \n",
    "# - 사전학습 임베딩 활용 : Word2Vec, GloVe, FastText 같은 사전학습 벡터를 초기값으로 사용 \n",
    "# - Transformer 기반 임베딩 : BERT, GPT 등 사전학습 모델의 임베딩 레이어를 가져와 파인 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d48cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 69])\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "# 1. 토크나이저 -> 인덱스 변환 \n",
    "# - 텍스트를 토큰 단위로 분리 (WordPiece, BPE, SentencePiece 등) \n",
    "# - 각 토큰을 고유 인덱스로 매핑\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 작은 코퍼스\n",
    "corpus = [\n",
    "    \"안녕하세요 오늘은 날씨가 맑습니다\",\n",
    "    \"저는 자연어 처리를 공부하고 있습니다\",\n",
    "    \"임베딩은 단어를 벡터로 표현하는 방법입니다\",\n",
    "    \"파이토치는 딥러닝을 위한 강력한 라이브러리입니다\",\n",
    "    \"언어 모델은 다음 단어를 예측하는 방식으로 학습합니다\",\n",
    "    \"작은 데이터셋으로도 실험을 시작할 수 있습니다\",\n",
    "    \"머신러닝은 데이터를 통해 패턴을 학습합니다\",\n",
    "    \"딥러닝은 인공신경망을 기반으로 합니다\",\n",
    "    \"토큰화는 문장을 단어 단위로 나누는 과정입니다\",\n",
    "    \"모델은 입력을 받아 출력을 생성합니다\",\n",
    "    \"하이퍼파라미터는 학습 성능에 큰 영향을 줍니다\",\n",
    "    \"에포크는 전체 데이터셋을 한 번 학습하는 단위를 의미합니다\",\n",
    "    \"배치 크기는 한 번에 처리하는 샘플 수입니다\",\n",
    "    \"손실 함수는 모델의 예측과 정답의 차이를 측정합니다\",\n",
    "    \"옵티마이저는 파라미터를 업데이트하는 알고리즘입니다\",\n",
    "    \"학습률은 파라미터를 얼마나 크게 조정할지 결정합니다\",\n",
    "    \"정규화는 과적합을 방지하는 방법입니다\",\n",
    "    \"드롭아웃은 일부 뉴런을 무작위로 끊어 학습을 안정화합니다\",\n",
    "    \"GPU는 대규모 연산을 빠르게 수행할 수 있습니다\",\n",
    "    \"실험을 반복하면 더 나은 결과를 얻을 수 있습니다\"\n",
    "]\n",
    "\n",
    "# 사전학습된 gpt2 모델의 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# gpt2 토크나이저 모델에서는 padding 토큰(pad_token)을 정의하지 않는다. 문자을 끝날때 EOS(end of sequence) 토큰만 사용\n",
    "# 배치 학습시 길이 맞추기, 여러 문장을 한 배치로 학습하려면 길이를 맞추어야 한다\n",
    "# 짧은 문장은 padding을 넣어 길이를 맞추는데, gpt2 모델에는 pad 토큰이 없으니 EOS 토큰을 대신 사용한다\n",
    "# gpt2 모델은 pad 토큰을 따로 처리하지 않기 때문에, pad를 eos로 설정하면 모델이 '문장이 끝났다'는 의미로 자연스럽게 인식한다\n",
    "# loss 계산시 pad 토큰을 무시하거나 특별히 다루지 않아도 된다\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 토큰화, padding=True 가장 긴 문장 길이에 맞추어 나머지 문장들을 pad 토큰으로 채워준다\n",
    "inputs = tokenizer(corpus, return_tensors='pt', padding=True)\n",
    "\n",
    "input_ids = inputs['input_ids'] \n",
    "vocab_size = tokenizer.vocab_size # 사전학습된 gpt2 모델의 토크나이저 vocabulary 50257 보유\n",
    "embed_dim = 128 # embedding 128 차원\n",
    "\n",
    "# 결과 확인\n",
    "# print(inputs)\n",
    "print(inputs['input_ids'].shape)\n",
    "print(vocab_size)\n",
    "# print(inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b390bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 공간내 벡터 위치 학습 모델\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleLM(nn.Module):\n",
    "    # vocabulary 50257, embed_dim 128\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # 각 단어 ID를 벡터로 변환\n",
    "        self.decoder = nn.Linear(embed_dim, vocab_size) # 임베딩 벡터를 vocab 크기 만큼 logits으로 변환\n",
    "    \n",
    "    # 모델의 학습과정을 반복하면 학습을 통해 의미 기반 벡터 공간을 형성하는게 목적이다\n",
    "    # 비슷한 의미/맥락의 단어 -> 임베딩 공간에서 서로 가까워지며, 다른 의미/맥락의 단어 -> 서로 떨어짐\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        logits = self.decoder(emb)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe21771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 모델, 손실함수, 옵티마이저 객체 생성\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "model = SimpleLM(vocab_size=vocab_size, embed_dim=embed_dim).to(device) # vocabulary 50257, embed_dim 128\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 진행\n",
    "\n",
    "# next-token prediction: 입력과 라벨 어긋나게\n",
    "inputs_shift = input_ids[:, :-1].to(device)\n",
    "labels_shift = input_ids[:, 1:].to(device)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()    \n",
    "    logits = model(inputs_shift)\n",
    "    # print(f'logits: {logits}, logits.shape: {logits.shape}')\n",
    "    # print(f'labels_shift: {labels_shift}, labels_shift.shape: {labels_shift.shape}')\n",
    "\n",
    "    # loss = loss_fn(logits.view(-1, vocab_size), labels_shift.view(-1))\n",
    "    # 학습 루프에서는 .reshape()를 쓰는 게 더 안정적\n",
    "    # 1. logits -> Softmax -> 확률 분포 변환, 2. CrossEntropyLoss 정답 토큰 ID와 예측 확률 분포를 비교\n",
    "    loss = loss_fn(logits.reshape(-1, vocab_size), labels_shift.reshape(-1))\n",
    "    # print(logits.shape, labels_shift.shape)\n",
    "    # print(logits.reshape(-1, vocab_size).shape, labels_shift.reshape(-1).shape)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae6ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False # 마이너스 기호 깨짐 방지\n",
    "\n",
    "# 관심 단어\n",
    "words = ['이재명', '대통령', '인공지능', 'AI', 'GPU']\n",
    "\n",
    "# 토큰 ID 변환\n",
    "word_ids = [ tokenizer.encode(w, add_special_tokens=False)[0] for w in words ]\n",
    "\n",
    "# 학습된 임베딩 벡터 추출\n",
    "vectors = model.embedding.weight[word_ids].detach().cpu().numpy()\n",
    "\n",
    "# PCA로 2차원 축소\n",
    "pca = PCA(n_components=2) # 차원을 2개로 줄이겠다는 의미, 고차원 데이터를 시각화 또는 중요한 축만 남겨서 분석하는 목정\n",
    "reduced = pca.fit_transform(vectors) # 변환, 예시) vector (5, 7678) -> (5, 2)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(x=reduced[:, 0], y=reduced[:, 1], s=50) # x좌표, y좌표, 점의 크기는 50\n",
    "\n",
    "texts = []\n",
    "for i, word in enumerate(words):\n",
    "    texts.append(plt.text(x=reduced[i,0], y=reduced[i,1], s=word, fontsize=12))\n",
    "\n",
    "# 라벨 자동 조정\n",
    "adjust_text(texts=texts, arrowprops=dict(arrowstyle=\"->\", color='gray'))\n",
    "\n",
    "plt.title(\"학습된 단어 임베딩 PCA 시각화 (라벨 겹침 방지)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f25ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 단어 임베딩 PCA 시각화\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "\n",
    "\n",
    "# 관심 단어들\n",
    "words = [\"학교\",\"학생\",\"교실\",\"커피\",\"카페\",\"에스프레소\",\"강아지\",\"고양이\",\"동물\"]\n",
    "\n",
    "# 토큰 ID 변환\n",
    "word_ids = [tokenizer.encode(w, add_special_tokens=False)[0] for w in words]\n",
    "\n",
    "# 학습된 임베딩 벡터 추출\n",
    "vectors = model.embedding.weight[word_ids].detach().cpu().numpy()\n",
    "\n",
    "# PCA로 2차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(vectors)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(reduced[:,0], reduced[:,1], s=50)\n",
    "\n",
    "texts = []\n",
    "for i, word in enumerate(words):\n",
    "    texts.append(plt.text(reduced[i,0], reduced[i,1], word, fontsize=12))\n",
    "\n",
    "# 라벨 자동 조정\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray'))\n",
    "\n",
    "plt.title(\"학습된 단어 임베딩 PCA 시각화 (라벨 겹침 방지)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d507ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 단어 임베딩 t-SNE 시각화\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# 관심 단어들\n",
    "words = [\"학교\",\"학생\",\"교실\",\"커피\",\"카페\",\"에스프레소\",\"강아지\",\"고양이\",\"동물\"]\n",
    "\n",
    "# 토큰 ID 변환\n",
    "word_ids = [tokenizer.encode(w, add_special_tokens=False)[0] for w in words]\n",
    "\n",
    "# 학습된 임베딩 벡터 추출\n",
    "vectors = model.embedding.weight[word_ids].detach().cpu().numpy()\n",
    "\n",
    "# t-SNE로 2차원 축소\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5, max_iter=1000)\n",
    "reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(reduced[:,0], reduced[:,1], s=50)\n",
    "\n",
    "texts = []\n",
    "for i, word in enumerate(words):\n",
    "    texts.append(plt.text(reduced[i,0], reduced[i,1], word, fontsize=12))\n",
    "\n",
    "# 라벨 자동 조정\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray'))\n",
    "\n",
    "plt.title(\"학습된 단어 임베딩 t-SNE 시각화 (라벨 겹침 방지)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 행렬\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 관심 단어들\n",
    "words = [\"파이토치\",\"딥러닝\",\"GPU\"]\n",
    "\n",
    "# 토큰 ID 변환\n",
    "word_ids = [tokenizer.encode(w, add_special_tokens=False)[0] for w in words]\n",
    "\n",
    "# 학습된 임베딩 벡터 추출\n",
    "vectors = model.embedding.weight[word_ids].detach().cpu().numpy()\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "sim_matrix = cosine_similarity(vectors)\n",
    "print(\"코사인 유사도 행렬:\")\n",
    "print(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치-딥러닝-GPU 임베딩 위치 (PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# PCA로 2차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "reduced = pca.fit_transform(vectors)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(reduced[:,0], reduced[:,1], s=80)\n",
    "\n",
    "texts = []\n",
    "for i, word in enumerate(words):\n",
    "    texts.append(plt.text(reduced[i,0], reduced[i,1], word, fontsize=12))\n",
    "\n",
    "adjust_text(texts, arrowprops=dict(arrowstyle=\"->\", color='gray'))\n",
    "plt.title(\"파이토치-딥러닝-GPU 임베딩 위치 (PCA)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
