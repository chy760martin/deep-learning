{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f2905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Language Model(SLLM) \n",
    "\n",
    "# Step 03 - 트랜스포머 허깅페이스 데이터셋(ShareGPT-KO)을 적용하여 Small Language Model(SLLM) 구축 및 텍스트 생성\n",
    "# - Hugging Face에서 제공하는 FreedomIntelligence/sharegpt-korean 데이터셋을 기반으로 기존 Transformer 언어 모델 구조에 맞춰 구성\n",
    "# - ShareGPT-KO는 GPT와 사용자 간의 대화 데이터를 한국어로 번역한 고품질 대화형 데이터셋으로, 문장 생성, 대화 모델 학습, 응답 생성 등에 매우 적합함\n",
    "\n",
    "# 라이브러리 GPU 설정 -> 데이터셋 -> 데이터로더 -> 토크나이저/토큰화 -> SLLM 모델 정의 -> 학습 -> 텍스트 생성 -> 테스트\n",
    "\n",
    "# 핵심 목적 - 한국어 데이터셋을 사용하여 문장 생성 및 단일문장/대화 흐름을 기반으로 다음 단어 예측\n",
    "# - 기존 WikiText-2 기반 Transformer 언어 모델 → ShareGPT-KO 대화 데이터셋으로 변경\n",
    "# - 문장 생성 및 문맥 예측을 위한 시퀀스 학습\n",
    "# - 단일 문장 또는 대화 흐름을 기반으로 다음 단어 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6575fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 2.2.2, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1) 라이브러리 GPU 설정\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Pytorch Version: {torch.__version__}, Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe74e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ShareGPT-KO 데이터셋 로드\n",
    "\n",
    "# Hugging Face에서 ShareGPT-KO 데이터셋 로딩\n",
    "dataset = load_dataset('FreedomIntelligence/sharegpt-korean', split='train') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811995e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'lang', 'conversations'],\n",
      "    num_rows: 6014\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 3) 데이터셋 내의 대화에서 텍스트 추출\n",
    "print(dataset)\n",
    "\n",
    "texts = []\n",
    "for item in dataset:\n",
    "    for turn in item['conversations']:\n",
    "        texts.append(turn['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bb29c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f0d1ab4a647db8ed384d6d17506b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b3d5cb9bcb44308213be05174da65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c1fbfd7bd34c8ba9b3b89e5f672ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) 토크나이저 모델 로드\n",
    "# tokenizer_model = 'monologg/kobert'\n",
    "tokenizer_model = 'beomi/KcBERT-base'  # 안정적인 한국어 토크나이저\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "834915dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (395 > 300). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 5) 토크나이저를 이용하여 토큰화\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize(text):\n",
    "    return auto_tokenizer.tokenize(text)\n",
    "\n",
    "tokens = []\n",
    "for text in texts: # 텍스트 -> 토큰화\n",
    "    tokens.extend(tokenize(text))\n",
    "\n",
    "# 딕셔너리 생성\n",
    "vocab = sorted(set(tokens))\n",
    "word2idx = { word:idx for idx, word in enumerate(vocab) } # 단어 -> 인덱스\n",
    "idx2word = { idx:word for word, idx in word2idx.items() } # 인덱스 -> 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3b3ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16538\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97e2d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) 시퀀스 생성 - 입력 시퀀스, 출력 시퀀스 생성\n",
    "sequence_length = 32\n",
    "data = []\n",
    "\n",
    "# 입력 출력 - 32, 32\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    input_seq = tokens[i:i+sequence_length] # 입력 시퀀스\n",
    "    target_seq = tokens[i+1:i+sequence_length+1] # 출력 시퀀스\n",
    "    # 입력, 출력 - 단어 -> 인덱스 -> 텐서 변환\n",
    "    data.append(\n",
    "        (\n",
    "            # torch.tensor( [ word2idx[word] for word in input_seq ] ),\n",
    "            # torch.tensor( [ word2idx[word] for word in target_seq ] )\n",
    "            torch.tensor( [ word2idx.get(word, word2idx.get('[PAD]', 0)) for word in input_seq ] ),\n",
    "            torch.tensor( [ word2idx.get(word, word2idx.get('[PAD]', 0)) for word in target_seq ] )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18b56c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Dataset 및 DataLoader 생성\n",
    "\n",
    "# Dataset 설정\n",
    "class ShareGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# DataLoader 설정\n",
    "dataloader = DataLoader(ShareGPTDataset(data=data[:50000]), batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c71aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Transformer 모델 정의\n",
    "class TransformerSLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len):\n",
    "        super().__init__()\n",
    "        # 단어 인덱스를 고정된 차원의 벡터로 변환하는 임베딩 레이어를 정의 vocab_size(단어 사전의 크기), embed_dim(각 단어를 표현할 벡터의 차원, 예시 128차원)\n",
    "        # 즉 단어 인덱스는 의미를 담고 있지 않음 그래서 임베딩 벡터는 의미적 유사성을 학습, 이 레이어는 vocab_size x embed_dim 크기의 임베딩 행렬을 학습한다\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # 8086, 128\n",
    "\n",
    "        # 위지 정보(positional encoding) 입력 시쿼스의 위치 정보를 따로 제공해야 하며, 각 위치에 대해 학습 가능한 위치 벡터를 생성하는 임베딩 레이어\n",
    "        # max_len(32), embed_dim(128) 위치 정보를 명시적으로 추가해야 문맥을 이해할 수 있음\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim) # 32, 128\n",
    "\n",
    "        # d_model(128 입력 및 출력 임베딩 차원), nhead(4 어텐셔 헤드의 수), dim_feedforward(256 내부의 은닉층 크기)\n",
    "        # 즉 이 레이어는 입력 텐서의 shape이 [seq_len, batch_size, embed_dim]일때 동일한 shape의 출력을 반환하면서 문맥 정보를 강화한다\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Transformer 모델의 인코더 블록 전체를 구성하는 핵심 코드 앞서 정의한 encoder_layer를 여러번 반복해서 깊이 있는 문맥 이해를 가능하게 하는 구조\n",
    "        # nn.TransformerEncoder 는 하나의 encoder_layer 를 num_layers 만큼 반복하여 전체 인코더 스택을 구성한다\n",
    "        # Input → EncoderLayer 1 → EncoderLayer 2 → ... → EncoderLayer N → Output\n",
    "        # 각 레이어는 입력 시퀀시를 받아 문맥 정보를 강화하고, 다음 레이어로 전달한다(반복 횟수가 많을수록 복잡한 문맥과 의미 관계를 더 잘 학습할 수 있음)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers) # encoder_layer, 2\n",
    "\n",
    "        # 모델의 출력층 정의하는 부분으로 모델이 예측한 임베딩을 단어 사전의 확률 분포로 변환하는 역할, Transformer의 마지막 출력은 [batch_size, seq_len, embed_dim] 형태의 문맥 벡터이다\n",
    "        # 이 벡터를 vocab_size 차원의 로짓(logits)으로 변환하여, 각 위치에서 어떤 단어가 올지 에측한다, Linear 레이어는 임베딩 공간 -> 단어 공간으로 매핑하는 분류기 역할을 한다\n",
    "        # embed_dim(Transformer의 출력 차원 예시 128), vocab_size(전체 단어 사전 크기 에시 10000)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size) # 128, 8086\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x는 [batch_size, sequence_length] 형태의 텐서, 예를 들어, x.shape = [8, 32]라면:\n",
    "        # x.size(0) → 배치 크기 (8)\n",
    "        # x.size(1) → 시퀀스 길이 (32)\n",
    "        # 즉, seq_len = 32가 된다\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # seq_len = 32이면 [0, 1, 2, ..., 31] 형태의 위치 인덱스 벡터를 생성\n",
    "        # 텐서의 shape을 [1, seq_len]으로 변환 → 이렇게 하면 배치 차원과 맞춰서 broadcasting이 가능\n",
    "        position = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "\n",
    "        # 단어 임베딩과 위치 임베딩을 더해서 최종 입력 벡터를 만들고, 이 합산은 Transformer가 단어 의미 + 위치 정보를 동시에 인식할 수 있게 해준다\n",
    "        x = self.token_embedding(x) + self.pos_embedding(position)\n",
    "\n",
    "        # Transformer 인코더에 입력을 전달하여 문맥 정보를 강화\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Transformer의 출력은 [batch_size, seq_len, embed_dim] 형태의 문맥 벡터이며, fc_out 레이어를 통해 각 위치에서 다음에 올 단어의 확률 분포를 예측\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "129102bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 20215.6324\n",
      "Epoch 2, Loss: 17601.9132\n",
      "Epoch 3, Loss: 17304.4040\n",
      "Epoch 4, Loss: 17121.7981\n",
      "Epoch 5, Loss: 16988.9319\n",
      "Epoch 6, Loss: 16869.8428\n",
      "Epoch 7, Loss: 16778.3830\n",
      "Epoch 8, Loss: 16692.0304\n",
      "Epoch 9, Loss: 16619.1102\n",
      "Epoch 10, Loss: 16565.8797\n"
     ]
    }
   ],
   "source": [
    "# 9) 학습\n",
    "model = TransformerSLLM(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    max_len=sequence_length\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # 손실함수 갑\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 옵티마이저 Adam\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device) # 입력 값\n",
    "        targets = targets.to(device) # 정답 값\n",
    "\n",
    "        outputs = model(inputs) # 모델 예측 값\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1)) # 손실함수 계산 값\n",
    "\n",
    "        # 오차역전파\n",
    "        optimizer.zero_grad() # 가중치, 바이어스 파리미터 초기화\n",
    "        loss.backward() # 미분 연산\n",
    "        optimizer.step() # 미분 연산 후 가중치,바이어스 파라미터 업데이트\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4021077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) 텍스트 생성 및 저장\n",
    "\n",
    "# torch.topk()로 상위 top_k개의 로짓만 남기고 나머지는 제거, 제거된 로짓은 -inf로 설정되어 softmax에서 확률이 0이 된다.\n",
    "def top_k_filtering(logits, top_k=50):\n",
    "    values, _ = torch.topk(logits, top_k)\n",
    "    min_value = values[:, -1].unsqueeze(1)\n",
    "    return torch.where(logits < min_value, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "# softmax 후 누적 확률이 top_p를 넘는 토큰을 제거, top_p=0.9이면 누적 확률이 90%를 넘는 이후의 토큰은 선택되지 않음, 더 자연스럽고 다양성 있는 문장 생성에 효과적이다.\n",
    "def top_p_filtering(logits, top_p=0.9):\n",
    "    # logits: [batch_size, vocab_size]\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # top_p 초과하는 부분 제거\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0 # 가장 높은 확률은 유지\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[0, indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "def generate_text(model, start_text, word2idx, idx2word, max_length=50, temperature=1.0, top_k=40, top_p=0.85):\n",
    "    model.eval()\n",
    "    tokens = auto_tokenizer.tokenize(start_text)\n",
    "    # input_ids = [ word2idx.get(word, 0) for word in tokens ]\n",
    "    input_ids = [ word2idx.get(token, word2idx.get('[PAD]', 0)) for token in tokens ]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            next_token_logits = output[:, -1, :] / temperature\n",
    "            logits = top_k_filtering(next_token_logits, top_k)\n",
    "            logits = top_p_filtering(logits, top_p)\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            input_ids.append(next_token_id)\n",
    "            input_ids = input_ids[-sequence_length:]\n",
    "            input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_words = [idx2word.get(int(idx), \"[UNK]\") for idx in input_ids]\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'sharegpt_korean_transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "767221ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerSLLM(\n",
       "  (token_embedding): Embedding(8086, 128)\n",
       "  (pos_embedding): Embedding(32, 128)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=8086, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11) 모델 재생성 후 불러오기\n",
    "model = TransformerSLLM(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    max_len=sequence_length\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"sharegpt_korean_transformer.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📢 생성된 문장:\n",
      "봄 날 서울 에서 가장 아름다운 장소 는 어디 인 가 요 ? O p ic k a ch e x i th on 이 접근 할 수 있습니다 . t ra y p en d 방법 은 추가 하는 클 래 스를 사용 하여 P ri c 2 . 이러한 방법 의 합니다 . 이는 블로그 글 , 9\n"
     ]
    }
   ],
   "source": [
    "# 12) 테스트\n",
    "import re\n",
    "\n",
    "# 출력 전 후처리 (SentencePiece 토큰 제거 등)\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"▁\", \"\")  # SentencePiece 경계 제거\n",
    "    text = text.replace(\"[UNK]\", \"\")  # 알 수 없는 토큰 제거\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9.,!? ]\", \"\", text)  # 특수문자 제거\n",
    "    text = re.sub(r\"(.)\\s+\\1+\", r\"\\1\", text)\n",
    "    return ' '.join(text.split()).strip()\n",
    "\n",
    "# 시작 문장 설정\n",
    "# start_text = \"서울의 봄날, 벚꽃이 만개한 거리를 걸으며\"\n",
    "start_text = \"봄날 서울에서 가장 아름다운 장소는 어디인가요?\"\n",
    "# start_text = \"벚꽃이 만개한 거리를 걸으며 느낀 감정을 말해줘.\"\n",
    "\n",
    "# 텍스트 생성 실행\n",
    "generated = generate_text(model, start_text, word2idx, idx2word, max_length=50)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"📢 생성된 문장:\")\n",
    "print(clean_text(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
