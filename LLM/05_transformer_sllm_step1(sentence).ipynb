{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Language Model(SLLM) \n",
    "# Step 01 - 가장 기초적인 문장 데이터셋 적용, 즉 개념 정의\n",
    "# 데이터셋 -> 데이터로더 -> GPU 설정 -> SLLM 모델 정의 -> 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version : 2.7.1+cu118, Device : cuda\n"
     ]
    }
   ],
   "source": [
    "# import 및 GPU 설정\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Pytorch Version : {torch.__version__}, Device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e01b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'world': 0, 'language': 1, 'pytorch': 2, 'hello': 3, 'model': 4}\n",
      "[([3, 0], [0, 3]), ([0, 3], [3, 2]), ([3, 2], [2, 1]), ([2, 1], [1, 4])]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 토큰화 및 시퀀시 생성\n",
    "# - Step 01 에서는 아주 간단한 텍스트 데이터를 사용해 토큰화하고, 시퀀시를 만들어 학습용으로 변환한다.\n",
    "\n",
    "# 예시 텍스트 데이터\n",
    "text = \"hello world hello pytorch language model\"\n",
    "\n",
    "# 단어 단위로 토큰화\n",
    "tokens = text.split() # split\n",
    "# print(tokens)\n",
    "vocab = list(set(tokens)) # 분리된 단어 -> 리스트\n",
    "# print(vacab)\n",
    "word2idx = { word:idx for idx, word in enumerate(vocab) } # 단어 리스트 -> 딕셔너리(value:key), index 값 출력\n",
    "print(word2idx)\n",
    "idx2word = { idx:word for word, idx in word2idx.items() } # 딕셔너리(key:value), word value 값 출력\n",
    "# print(idx2word)\n",
    "\n",
    "# 시퀀스 생성(입력: [hello, world], 출력: [world, hello] 등)\n",
    "sequence_length = 2\n",
    "data = []\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    input_seq = tokens[i:i+sequence_length]\n",
    "    # print('input_seq : ', input_seq)\n",
    "    target_seq = tokens[i+1:i+sequence_length+1]\n",
    "    # print('target_seq : ', target_seq)\n",
    "    data.append((\n",
    "        [ word2idx[word] for word in input_seq ],\n",
    "        [ word2idx[word] for word in target_seq ] \n",
    "    ))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "545d649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 데이터로더 생성\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# dataset\n",
    "dataset = TextDataset(data)\n",
    "\n",
    "# dataloader\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365cbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLLM 모델 정의\n",
    "class SmallLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(SmallLanguageModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, _ = self.rnn(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1869d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss : 1.6555\n",
      "Epoch 2, Loss : 1.4917\n",
      "Epoch 3, Loss : 1.3273\n",
      "Epoch 4, Loss : 1.0931\n",
      "Epoch 5, Loss : 1.0393\n",
      "Epoch 6, Loss : 0.9532\n",
      "Epoch 7, Loss : 0.7608\n",
      "Epoch 8, Loss : 0.4111\n",
      "Epoch 9, Loss : 0.5966\n",
      "Epoch 10, Loss : 0.5328\n",
      "Epoch 11, Loss : 0.1902\n",
      "Epoch 12, Loss : 0.3651\n",
      "Epoch 13, Loss : 0.3194\n",
      "Epoch 14, Loss : 0.3882\n",
      "Epoch 15, Loss : 0.1814\n",
      "Epoch 16, Loss : 0.2015\n",
      "Epoch 17, Loss : 0.2410\n",
      "Epoch 18, Loss : 0.3619\n",
      "Epoch 19, Loss : 0.1777\n",
      "Epoch 20, Loss : 0.1930\n",
      "Epoch 21, Loss : 0.1942\n",
      "Epoch 22, Loss : 0.1727\n",
      "Epoch 23, Loss : 0.1748\n",
      "Epoch 24, Loss : 0.0116\n",
      "Epoch 25, Loss : 0.2124\n",
      "Epoch 26, Loss : 0.2217\n",
      "Epoch 27, Loss : 0.2240\n",
      "Epoch 28, Loss : 0.1634\n",
      "Epoch 29, Loss : 0.1956\n",
      "Epoch 30, Loss : 0.0071\n",
      "Epoch 31, Loss : 0.1814\n",
      "Epoch 32, Loss : 0.2005\n",
      "Epoch 33, Loss : 0.3533\n",
      "Epoch 34, Loss : 0.3531\n",
      "Epoch 35, Loss : 0.2091\n",
      "Epoch 36, Loss : 0.1708\n",
      "Epoch 37, Loss : 0.1916\n",
      "Epoch 38, Loss : 0.3516\n",
      "Epoch 39, Loss : 0.2073\n",
      "Epoch 40, Loss : 0.1672\n",
      "Epoch 41, Loss : 0.1822\n",
      "Epoch 42, Loss : 0.1926\n",
      "Epoch 43, Loss : 0.3509\n",
      "Epoch 44, Loss : 0.1800\n",
      "Epoch 45, Loss : 0.0041\n",
      "Epoch 46, Loss : 0.1828\n",
      "Epoch 47, Loss : 0.1883\n",
      "Epoch 48, Loss : 0.1813\n",
      "Epoch 49, Loss : 0.3500\n",
      "Epoch 50, Loss : 0.1769\n",
      "Epoch 51, Loss : 0.1845\n",
      "Epoch 52, Loss : 0.1897\n",
      "Epoch 53, Loss : 0.3497\n",
      "Epoch 54, Loss : 0.0034\n",
      "Epoch 55, Loss : 0.3497\n",
      "Epoch 56, Loss : 0.1989\n",
      "Epoch 57, Loss : 0.3511\n",
      "Epoch 58, Loss : 0.2040\n",
      "Epoch 59, Loss : 0.3506\n",
      "Epoch 60, Loss : 0.1787\n",
      "Epoch 61, Loss : 0.1962\n",
      "Epoch 62, Loss : 0.2090\n",
      "Epoch 63, Loss : 0.2126\n",
      "Epoch 64, Loss : 0.1642\n",
      "Epoch 65, Loss : 0.1840\n",
      "Epoch 66, Loss : 0.0027\n",
      "Epoch 67, Loss : 0.1701\n",
      "Epoch 68, Loss : 0.1947\n",
      "Epoch 69, Loss : 0.1695\n",
      "Epoch 70, Loss : 0.3491\n",
      "Epoch 71, Loss : 0.3490\n",
      "Epoch 72, Loss : 0.0025\n",
      "Epoch 73, Loss : 0.1867\n",
      "Epoch 74, Loss : 0.0025\n",
      "Epoch 75, Loss : 0.1840\n",
      "Epoch 76, Loss : 0.1756\n",
      "Epoch 77, Loss : 0.0024\n",
      "Epoch 78, Loss : 0.3487\n",
      "Epoch 79, Loss : 0.1904\n",
      "Epoch 80, Loss : 0.1743\n",
      "Epoch 81, Loss : 0.1855\n",
      "Epoch 82, Loss : 0.0022\n",
      "Epoch 83, Loss : 0.1851\n",
      "Epoch 84, Loss : 0.1759\n",
      "Epoch 85, Loss : 0.1878\n",
      "Epoch 86, Loss : 0.1752\n",
      "Epoch 87, Loss : 0.0021\n",
      "Epoch 88, Loss : 0.1847\n",
      "Epoch 89, Loss : 0.0021\n",
      "Epoch 90, Loss : 0.0020\n",
      "Epoch 91, Loss : 0.1843\n",
      "Epoch 92, Loss : 0.1888\n",
      "Epoch 93, Loss : 0.1900\n",
      "Epoch 94, Loss : 0.0019\n",
      "Epoch 95, Loss : 0.1902\n",
      "Epoch 96, Loss : 0.1660\n",
      "Epoch 97, Loss : 0.1983\n",
      "Epoch 98, Loss : 0.2064\n",
      "Epoch 99, Loss : 0.3504\n",
      "Epoch 100, Loss : 0.0018\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "model = SmallLanguageModel(vacab_size=len(vocab), embed_dim=16, hidden_dim=32).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1))\n",
    "\n",
    "        # 오차역전파\n",
    "        optimizer.zero_grad() # 미분 파리미터 초기화\n",
    "        loss.backward() # 미분 연산\n",
    "        optimizer.step() # 미분 연산 후 가중치,바이어스 파라미터 업데이트\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss : {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
