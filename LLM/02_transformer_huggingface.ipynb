{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90abaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 패키지 설치\n",
    "# !pip install transformers==4.50.0 datasets==3.5.0 huggingface_hub==0.29.0 -qqq \n",
    "# !pip install --upgrade transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e50da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1646230cd5b74be7a8a1839c2383d5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 14:00:04.124552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cf3f714dee4b91b9115559f13b9b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30317879490849bd94bf5872e889dff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e1d9c78bec4935b3adbc0b37a0f8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8356fe334abc4691932ec805ca995c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BERT 와 GPT-2 모델을 활용할때 허깅페이스 트랜스포머 코드 비교\n",
    "\n",
    "# 허깅페이스 트렌스포머를 활용하면 서로 다른 조직에서 개발한 BERT와 GPT-2 모델을 거의 동인한 인터페이스로 활용 가능\n",
    "# AutoModel, AutoTokenizer 클래스를 사용해 BERT 및 GPT-2 모델과 토큰나이저를 불러오고 토큰화를 수행해서 모델에 입력으로 넣어준다\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "text = 'What is Huggingface Transformers?'\n",
    "\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased') # pre-trained model(BERT) 불러오기\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # 토크나이저 불러오기\n",
    "encoded_input =  bert_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "bert_output = bert_model(**encoded_input) # 모델에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6438e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd138621b3e44eba154d7d29d5849b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ca1d875d3244a88464db0f5c74d8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547828a3a25b471c92c0e7ce6a1d5af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3793b0a45a614d9e8f5205aac9c711be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ebcdd663a3479a830f7b0001575853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fca10f5e4b434aae400fc471ecbb11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GPT-2 모델 활용\n",
    "gpt_model = AutoModel.from_pretrained('gpt2') # pre-trained model(GPT) 불러오기\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2') # 토크나이저 불러오기\n",
    "encoded_input = gpt_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "gpt_output = gpt_model(**encoded_input) # 모델에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "366d15d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 트랜스포머 라미이브러리\n",
    "# - 허깅페이스에는 모델을 바디(body), 헤드(head)로 구분, 같은 바디를 사용하면서 다른 작업에 사용 할 수 이도록 만들기 위함\n",
    "# 예) 구글 BERT 모델을 사용하지만 다른 헤드를 사용 할 수가 있다\n",
    "# - 문장 전체가 긍정인지 부정인지를 분류하는 모델, 이때는 바디가 반환하는 여러 잠재 상태(hidden state)중 가장 앞에 있는 [CLS] 토큰의 데이터만 받아 예측\n",
    "# - 각 토큰이 사람이나 장소에 해당하는지 판단하는 개처명 인식(named entity recognition) 모델인데, 각 토큰에 대해 판단해야 하기 때문에 모든 토튼의\n",
    "# - 데이터를 받아 각각 사람인지 장소인지를 예측\n",
    "\n",
    "# 모델 아이디로 모델 불러오기 코드 - 모댈의 바디만 불어오는 코드\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model.save_pretrained('./transformers_models/klue/roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae1c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 헤드가 포함된 모델 불러오기 - SamLowe/roberta-base-go_emotions 저장소에서 텍스트 분류 모델을 내려 받아 classification_model 변수에 저장\n",
    "# SamLowe/roberta-base-go_emotions 모델은 분류 헤드가 포함(입력 문장이 어떤 감성을 나타내는지 분류하는데, 감탄/즐거움/화 등과 같은 감정이 포함되어 있는지 분류)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model.save_pretrained('./transformers_models/SamLowe/roberta-base-go_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8815b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 분류 헤드가 랜덤으로 초기화된 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model.save_pretrained('./transformers_models/klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf76ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47e7d66730544be986ee34fb925da23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/375 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0c59b459904165b24ef562d2975116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e245146bb3de47d7b2295d4022b8d116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16e466cda964ad5849d2d8512b52e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토큰나이저 불러오기\n",
    "# AutoTokenizer 클래스를 통해 앞서 모델을 불러올때도 사용한 klue/roberta-base 저장소의 토큰나이저를 불러온다\n",
    "# config_json 파일에서 확인\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b16055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 1793, 2855, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 18, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토', '##큰', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '.', '[SEP]']\n",
      "[CLS] 토큰나이저는 텍스트를 토큰 단위로 나눈다. [SEP]\n",
      "토큰나이저는 텍스트를 토큰 단위로 나눈다.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 사용하기\n",
    "\n",
    "# input_ids는 토큰화했을때 각 토큰이 토크나이저 사전에 몇번째 항목인지 나타냄\n",
    "# input_ids의 첫번째 항목은 0이고 두번째 항목은 9157인데 각각 [CLS]와 토크에 대응되는 것을 확인 할 수 있다\n",
    "tokenized = tokenizer('토큰나이저는 텍스트를 토큰 단위로 나눈다.')\n",
    "print(tokenized)\n",
    "# {'input_ids': [0, 1793, 2855, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 18, 2], \n",
    "# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "# attention mask가 1이면 패딩이 아닌 실제 토큰임을 의미, token_type_ids가 0이면 일반적으로 첫번째 문장임을 의미\n",
    "# attention mask는 실제 텍스트 길이를 맞추기 위해 추가한 패딩인지 알려주는 mask\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "# ['[CLS]', '토', '##큰', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '.', '[SEP]']\n",
    "\n",
    "# 토큰 아이디를 다시 텍스트로 돌리고 싶다면 토크나이저의 decode 메서드를 사용\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "# [CLS] 토큰나이저는 텍스트를 토큰 단위로 나눈다. [SEP]\n",
    "\n",
    "# 특수 문자 토큰을 제외하고 싶다면 skip_special_tokens=True 설정\n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e6a3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저에 여러 문장 넣기\n",
    "# 토크나이저에 한번에 여러 문장을 처리 할 수도 있다, '첫 번째 문장', '두 번째 문장'을 리스트로 함께 넣었다\n",
    "# 출력 결과를 확인하면 모두 각 문장을 토큰화해 2개 리스트를 반환\n",
    "tokenizer(['첫 번째 문장', '두 번째 문장'])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe8c3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하나의 데이터에 여러 문장이 들어가는 경우\n",
    "# 한번에 2개의 문장을 모델에 넣어야 하는 경우 - 2개의 문장이 서로 원인과 결과 관계인지 학습시키기 싶다면, 두 문장을 한번에 모델에 입력해야됨\n",
    "# 이때 2개의 문장이 하나의 데이터라는 것을 표시하기 위해 리스트로 한번더 감싸준다\n",
    "tokenizer([ ['첫 번째 문장', '두 번째 문장'] ])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9c2e93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]]\n",
      "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
      "[[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]]\n",
      "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 토큰 아이디를 문자열로 복원\n",
    "\n",
    "# 토크나이저의 batch_decode() 메서드를 사용하면 input_ids부분의 토큰 아이디를 문자열로 복월 할 수 있는데\n",
    "# 기본적으로 토큰화를 하면 ['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
    "# 2개의 문자을 한번에 토큰화하면 ['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n",
    "\n",
    "first_tokenized_result = tokenizer( ['첫 번째 문장', '두 번째 문장'] )['input_ids']\n",
    "print(first_tokenized_result)\n",
    "print(tokenizer.batch_decode(first_tokenized_result))\n",
    "\n",
    "second_tokenized_result = tokenizer( [ ['첫 번째 문장', '두 번째 문장'] ] )['input_ids']\n",
    "print(second_tokenized_result)\n",
    "print(tokenizer.batch_decode(second_tokenized_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7707a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저와 RoBERTa 토크나이저\n",
    "\n",
    "# BERT는 학습할때 2개의 문장이 서로 이어지는지 맞추는 NSP(next sentence prediction) 작업을 수행하기위해 문장을 구분하는 토큰 타입 아이디를 만들었다\n",
    "# 첫번째 문장의 토큰 아이디는 0, 두번째 문장의 토큰 아이디 1\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "print(bert_tokenizer( [ ['첫 번째 문장', '두 번째 문장'] ] ))\n",
    "# {'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "# RoBERTa 모델의 경우 NSP 작업을 학습 과정에서 제거\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "print(roberta_tokenizer( [ ['첫 번째 문장', '두 번째 문장'] ] ))\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "# 실제 영어 버전의 roberta-base 모델에는 항목 자체가 없음\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(en_roberta_tokenizer( [ ['first sentence', 'second sentence'] ] ))\n",
    "# {'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6764dd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장은 짧다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 두 번째 문장은 첫 번째 문장보다 더 길다. [SEP]']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_mask 확인\n",
    "# 해당 토큰이 패딩 토큰인지 실제 데이터인지에 대한 정보, \n",
    "# 패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해 추가하는 토큰, padding='longest' 입력한 문장중 가장 긴 문장에 맞춰 패딩 토큰을 추가\n",
    "\n",
    "tokenizer_result = tokenizer( ['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest' )['input_ids']\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "print(tokenizer( ['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest' ))\n",
    "\n",
    "tokenizer.batch_decode(tokenizer_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
