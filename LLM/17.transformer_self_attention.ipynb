{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ec65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 구조 이해 및 모델 구축\n",
    "# 학습 목표\n",
    "# - 1. Encoder\n",
    "# - Scaled Dot-Product Attention\n",
    "# - Multi-Head Attention\n",
    "# - Transformer Encoder Block(Attention -> FFN -> Residual -> LayerNorm 구조)\n",
    "# - Positional Encoding\n",
    "# - Transformer Encoder\n",
    "# - 2. Decoder\n",
    "# - Masked Multi-Head Attention\n",
    "# - Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9d48cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 - 라벨링된 코퍼스, 라벨링된 코퍼스(긍정=1, 부정=0)\n",
    "corpus = [\n",
    "    (\"안녕하세요 오늘은 날씨가 맑습니다\", 1),   # 긍정\n",
    "    (\"저는 자연어 처리를 공부하고 있습니다\", 1), # 긍정\n",
    "    (\"임베딩은 단어를 벡터로 표현하는 방법입니다\", 1), # 긍정\n",
    "    (\"파이토치는 딥러닝을 위한 강력한 라이브러리입니다\", 1), # 긍정\n",
    "    (\"언어 모델은 다음 단어를 예측하는 방식으로 학습합니다\", 1), # 긍정\n",
    "    (\"작은 데이터셋으로도 실험을 시작할 수 있습니다\", 1), # 긍정\n",
    "    (\"머신러닝은 데이터를 통해 패턴을 학습합니다\", 1), # 긍정\n",
    "    (\"딥러닝은 인공신경망을 기반으로 합니다\", 1), # 긍정\n",
    "    (\"토큰화는 문장을 단어 단위로 나누는 과정입니다\", 1), # 긍정\n",
    "    (\"모델은 입력을 받아 출력을 생성합니다\", 1), # 긍정\n",
    "    (\"하이퍼파라미터는 학습 성능에 큰 영향을 줍니다\", 0), # 부정 (실험용)\n",
    "    (\"에포크는 전체 데이터셋을 한 번 학습하는 단위를 의미합니다\", 0), # 부정\n",
    "    (\"배치 크기는 한 번에 처리하는 샘플 수입니다\", 0), # 부정\n",
    "    (\"손실 함수는 모델의 예측과 정답의 차이를 측정합니다\", 0), # 부정\n",
    "    (\"옵티마이저는 파라미터를 업데이트하는 알고리즘입니다\", 0), # 부정\n",
    "    (\"학습률은 파라미터를 얼마나 크게 조정할지 결정합니다\", 0), # 부정\n",
    "    (\"정규화는 과적합을 방지하는 방법입니다\", 0), # 부정\n",
    "    (\"드롭아웃은 일부 뉴런을 무작위로 끊어 학습을 안정화합니다\", 0), # 부정\n",
    "    (\"GPU는 대규모 연산을 빠르게 수행할 수 있습니다\", 1), # 긍정\n",
    "    (\"실험을 반복하면 더 나은 결과를 얻을 수 있습니다\", 1) # 긍정\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c27ac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 128]) torch.Size([4, 128]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - tokenizer, Dataset 생성, DataLoader 생성, -> 학습데이터 생성\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# AutoTokenizer 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\") # bert-base-multilingual-cased 모델 vocab_size 119547\n",
    "\n",
    "# Custom Dataset 로드\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, corpus, tokenizer, max_len=128):\n",
    "        self.sentences = [ c[0] for c in corpus ] # 문장\n",
    "        self.labels = [ c[1] for c in corpus ] # 라벨\n",
    "        self.encodings = tokenizer( # AutoTokenizer 사용해 문장을 토큰화\n",
    "            self.sentences,\n",
    "            padding='max_length', # 문장 길이 부족하면, <PAD> 로 채운다\n",
    "            truncation=True, # 문장일 길면 자른다\n",
    "            max_length=max_len, # 문장 길이 128\n",
    "            return_tensors='pt' # 토큰화 결과(input_ids, attention_mask)를 PyTorch 텐서로 저장\n",
    "        )\n",
    "    \n",
    "    def __len__(self): # 전체 데이터셋의 크기를 반환\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # self.encodings.items() 에서 input_ids, attention_mask 추출 -> item 적재\n",
    "        item = { key: val[idx] for key, val in self.encodings.items() }\n",
    "        # self.labels[idx] 에서 labels 추출 -> item 적재\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Dataset\n",
    "dataset = TextDataset(corpus=corpus, tokenizer=tokenizer)\n",
    "# print(dataset.encodings)\n",
    "\n",
    "# DataLoader\n",
    "dataLoader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "for batch in dataLoader:\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape)\n",
    "    # print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4096735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "# - Query ='오늘은' \n",
    "# - Attention 확률이 '날씨'와 '맑습니다'에 높게 나오면, \"날씨\"(0.6), \"맑습니다\"(0.3), \"오늘은\"(0.1)\n",
    "# - out(\"오늘은\") = 0.6*V(\"날씨\") + 0.3*V(\"맑습니다\") + 0.1*V(\"오늘은\")\n",
    "# - '오늘은'의 최종 표현 벡터는 '날씨가 맑다'라는 맥락을 반영한 새로운 벡터로 업데이트 한다\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scaled Dot-Product Attention 함수\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    # Query와 Key의 내적(dot product)을 통해 유사도 점수를 계산\n",
    "    # - Q.shape (batch, num_heads, seq_len, d_k), K.transpose(-2, -1).shape (batch, num_heads, d_k, seq_len) -> 유사도 행렬 계산\n",
    "    # - Q.size(-1) 마지막 차원 크기 d_k, **5 제곱근 연산 -> d_k로 나누어 스케일링 계산\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5) # (batch, num_heads, seq_len, seq_len)\n",
    "    if mask is not None:\n",
    "        if mask.dtype == torch.bool: # True인 위치\n",
    "            scores = scores.masked_fill(mask, float('-inf')) # True 인 위치를 -inf로 채워 Softmax에서 확률이 0이 되도록 한다\n",
    "        else:\n",
    "            scores = scores + mask # mask 값을 유사도 점수에 더해준다\n",
    "    \n",
    "    # 유사도 행렬을 확률 분포로 변환, Query가 Key 전체 중 어디에 집중할지 확률로 표현\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # Attention 확률(attn)을 Value(V)에 곱해 최종 컨텍스트 벡터 생성, (batch,seq_len,d_v)\n",
    "    # 각 Query 토큰은 Key와의 유사도에 따라 Value 정보를 섞어서 새로운 표현을 얻게 된다\n",
    "    out = torch.matmul(attn, V) # (batch, num_heads ,seq_len,d_v)\n",
    "    return out, attn # out Attention 결과(컨텍스트 벡터), attn Attention 확률 분포(시각화할때 유용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f56f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Head Attention(MHA), Scaled Dot-Product Attention을 여러개의 Head로 병렬 처리하는 구조\n",
    "class MultHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0 # d_model은 num_heads로 나누어 떨어져야 한다\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # 각 head의 차원\n",
    "\n",
    "        # Head별 Q, K, V 변환을 위한 선형 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 모든 head 결합 후 최종 출력 변환\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # Head 나누기\n",
    "    def _split_heads(self, x):\n",
    "        # (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\n",
    "        b, L, _ = x.size()\n",
    "        return x.view(b, L, self.num_heads, self.d_k).transpose(1,2)\n",
    "    \n",
    "    # Head 결과를 합치기\n",
    "    def _combine_heads(self, x):\n",
    "        # (batch, num_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
    "        b, h, L, d_k = x.size()\n",
    "        return x.transpose(1,2).contiguous().view(b, L, h * d_k)\n",
    "        \n",
    "    def forward(self, x_q, x_kv=None, mask=None):\n",
    "        # Self-Attenton : x_q만 입력(x_kv=None)\n",
    "        # Cross-Attention : x_q, x_kv 모두 입력\n",
    "        if x_kv is None:\n",
    "            # Self-Attention\n",
    "            Q = self._split_heads(self.W_q(x_q))\n",
    "            K = self._split_heads(self.W_k(x_q))\n",
    "            V = self._split_heads(self.W_v(x_q))\n",
    "        else:\n",
    "            # Cross-Attention\n",
    "            Q = self._split_heads(self.W_q(x_q))\n",
    "            K = self._split_heads(self.W_k(x_kv))\n",
    "            V = self._split_heads(self.W_k(x_kv))\n",
    "\n",
    "        # 각 head별 Attention 수행\n",
    "        context, attn = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "        # head 결합\n",
    "        context = self._combine_heads(context)\n",
    "\n",
    "        # 최종 선형 변환\n",
    "        out = self.W_o(self.dropout(context))\n",
    "        return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49f26db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Block 구조\n",
    "# - Encoder Block 은 Attention -> FFN -> Residual -> LayerNorm 구조\n",
    "# - 여러 Block을 쌓으면 Transformer Encoder 완성, 이 블록은 문맥을 깊게 이해하는 핵심 모듈\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-Head Attention, 단어 간 관계 학습 - 문맥 반영\n",
    "        self.attn = MultHeadAttention(d_model, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        # Feed Forward Network, 각 단어 벡터 자체를 비선형 변환 - 표현력 강화\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), # 차원 확장, 예시) 차원 확장을 하기위해 d_model=512, d_ff=2048 \n",
    "            nn.ReLU(), # 또는 GELU\n",
    "            nn.Linear(d_ff, d_model) # 다시 원래 차원 축소\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None): # x (batch,seq_len,d_model)\n",
    "        # 1. Multi-Head Attention + Residual + Norm\n",
    "        attn_out, _ = self.attn(x, mask) # 입력 x를 Attention에 넣어 문맥 반영된 출력 attn_out을 얻음\n",
    "        x = self.norm1(x + self.dropout1(attn_out)) # Residual 연결 x + attn_out\n",
    "\n",
    "        # 2. Feed Forward + Residual + Norm\n",
    "        ffn_out = self.ffn(x) # Attention 결과를 FFN 넣어 토큰 자체 표현 강화\n",
    "        x = self.norm2(x + self.dropout2(ffn_out)) # Residual 연결 x + ffn_out\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d530a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position :  tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]]) torch.Size([5, 1])\n",
      "div_term :  tensor([1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03]) torch.Size([4])\n",
      "position * div_term :  tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 1.0000e-01, 1.0000e-02, 1.0000e-03],\n",
      "        [2.0000e+00, 2.0000e-01, 2.0000e-02, 2.0000e-03],\n",
      "        [3.0000e+00, 3.0000e-01, 3.0000e-02, 3.0000e-03],\n",
      "        [4.0000e+00, 4.0000e-01, 4.0000e-02, 4.0000e-03]]) torch.Size([5, 4])\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding 테스트\n",
    "import torch\n",
    "import math\n",
    "\n",
    "d_model = 8\n",
    "max_len =5\n",
    "\n",
    "# 위치 인덱스(0~4)\n",
    "position = torch.arange(0, max_len).unsqueeze(1) # shape(5,1)\n",
    "\n",
    "# 차원별 주파수 스케일\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "# position * div_term -> (5,4) 행렬\n",
    "pos_div = position * div_term\n",
    "\n",
    "print('position : ', position, position.shape)\n",
    "print('div_term : ', div_term, div_term.shape)\n",
    "print('position * div_term : ', pos_div, pos_div.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a392f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Encoder 구조\n",
    "# - Embedding(단어->벡터) -> Positional Encoding(벡터 + 순서 정보 추가) -> Encoder Blocks(문맥 반영 + 표현 강화)\n",
    "import math\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) # 예시 (5000, 16) 크기의 0의 행렬\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # 0~4999 단어의 위치 인덱스 생성, (max_len, 1) 2차원 행렬 차원 추가\n",
    "        # torch.arange(0, d_model, 2) 0,2,4,~,14 짝수 인덱스 추출\n",
    "        # -(math.log(10000.0) 위치 인덱스를 스케일링하기 위한 상수, 10000 포지셔널 인코딩에서 사용되는 기준값\n",
    "        # d_model 로 나누어 차원 크기를 맞게 조정, 즉 차원마다 다른 스케일 다른 주파수를 가지게 된다\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 짝수 sin\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 홀수 cos\n",
    "        pe = pe.unsqueeze(0) # 입력값 x와 shape을 맞추기 위함 (batch,seq_len,d_model)\n",
    "        self.register_buffer('pe', pe) # 학습되지 않는 텐서를 저장, gradient 파라미터 업데이트가 되지 않음\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, num_heads, d_ff)\n",
    "            for _ in range(num_layers) # num_layers 지정 수 만큼 반복해서 블록을 생성\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x): # 문장 -> 토큰화 입력값 x (batch, seq_len)\n",
    "        x = self.embedding(x) # 토큰화 ID d_model 차원의 벡터, embedding shape (batch, seq_len, d_model)\n",
    "        x = self.pos_encoding(x) # 벡터 + 위치정보(sin/cos 패턴), pos_encoding shape (batch, seq_len, d_model)\n",
    "        for layer in self.layers: # Multi-Head Attention 문맥 반영, Feed Forward Network 표현 강환, Residual + LayerNorm 안정적 학습\n",
    "            x = layer(x) # TransformerEncoderBlock shape (batch, seq_len, d_model)\n",
    "        return x\n",
    "\n",
    "# 작은 입력 (batch=1, seq_len=5)\n",
    "tokens = torch.tensor([[1,2,3,4,5]])\n",
    "encoder = TransformerEncoder(vocab_size=100, d_model=16, num_heads=4, d_ff=32, num_layers=2)\n",
    "output = encoder(tokens)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcb6b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Multi-Head Attention\n",
    "# - Decoder Attention, 미래 토큰을 보면 안됨, 마스크(mask) 로 가림, 예시) [I, love, ___] -> 다음 단어 예측시 뒤 단어는 가려져야 함\n",
    "# - 구현에서는 Upper-traiangular mask를 만들어서 현재 시점 이후의 값들을 -inf로 처리 -> softmax에서 0으로 됨\n",
    "# - Decoder는 왼쪽만 보고 오른쪽을 예측하는 구조이다\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1): # d_model 입력 벡터 차원, num_heads 어텐션 헤드 개수, dropout 드롭아웃 비율\n",
    "        super().__init__()\n",
    "        self.attn = MultHeadAttention(d_model, num_heads, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 입력값 x (batch,seq_len,d_model)\n",
    "        seq_len = x.size(1) # seq_len 의 문장 길이\n",
    "\n",
    "        # 마스크 생성(상삼각 행렬), 대각선 위로만 1 나머지는 0, bool() True/False 형태로 반환\n",
    "        # 예시) mask = False 볼 수 있는 부분(현재/과거 토큰), True 가려지는 부분(미래 토큰)\n",
    "        # [[False  True  True  True  True ], - 1번째 토큰은 자기 자신만 보고, 나머지 2~5는 모두 가려짐\n",
    "        # [False False  True  True  True ], - 2번째 토큰은 자기 자신 + 1번째 토큰만 보고, 3~5는 가려짐\n",
    "        # [False False False  True  True ], - 3번째 토큰은 자기 자신 + 1,2번째 토큰만 보고, 4~5는 가려짐\n",
    "        # [False False False False  True ], - 4번째 토큰은 자기 자신 + 1,2,3번째 토큰만 보고, 5는 가려짐\n",
    "        # [False False False False False]]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "\n",
    "        # mask : (seq_len, seq_len), True = 가려질 부분\n",
    "        attn_out, _ = self.attn(x, mask=mask)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc798252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Attention\n",
    "# - Decoder는 자기 자신 토큰들끼리만 참고 한다, 미래 토큰은 마스크 처리되어 볼 수 없기 때문에, 독자적으로는 전체 의미를 완전히 알 수 없다\n",
    "# - Cross Attention은 Decoder가 Encoder 출력값(Key/Value)를 참고 한다, 이렇게 해야 Decoder가 입력 문장의 의미를 반영 할 수 있다\n",
    "# - Q = Decoder 상태, K/V = Encoder 출력\n",
    "# - 즉, Decoder 혼자서는 불완전한 정보만 갖고 있으므로, Encoder의 의미 벡터를 참고해서 문장을 생성한다\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "    def forward(self, x_dec, enc_out, mask=None):\n",
    "        # x_dec는 Decoder에서, enc_out는 Encoder에서 가져옴\n",
    "        out, attn = self.mha(x_dec, x_kv=enc_out, mask=mask)\n",
    "        return out, attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
