{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ff125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 개념과 동작원리\n",
    "# RNN 모델은 인간의 언어나 날씨처럼 시간에 따라서 변하는 시퀀스 데이터를 분석하고 예측하는데 사용되기 때문에, 이러한 RNN 입력 데이터에는\n",
    "# 반드시 시간(또는 순서) 정보가 포함되어 있어야 함\n",
    "\n",
    "# 즉 RNN 입력 데이터는 다음과 같이 시간 정보(time steps)가 포함되어 있는 같이 3차원 테서로 만들어서 RNN 모델의 입력 층으로 넣어주는 것이 일반적임\n",
    "# RNN 입력 데이터 - 시간 정보 time_steps 포함된 3차원 텐서, (batch_size, time_steps, feature_nums)\n",
    "# batch_size - time_steps 으로 분리되어 있는 데이터의 개숙 총 몇개인지를 나타냄\n",
    "# time_steps - 몇개의 feature를 모아서 최종 정답을 만들어 내는지를 나타냄. 즉 몇개의 시점(time)을 이용해서 최종 정답을 만들것인지를 정의하는 항목으로 시간 정보를 나타냄\n",
    "# feature_nums -  하나의 시정(time)에 RNN 모델의 input_layer로 들어가는 데이터 개수, 즉 RNN input layer로 한번에 들어가는 데이터 개수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f523666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version :  2.7.1+cu118 , device :  cuda\n",
      "(2, 3, 2) (2,)\n",
      "torch.Size([2, 3, 2]) torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# RNN 데이터 만들기 위해서는 먼저 \n",
    "# 1)feature -> 2)time step -> 3)batch 순서로 리스트(list) 입력 데이터를 만들고\n",
    "# -> 4)time step 개수와 일치하는 정답(label) 리스트(list)를 만든 후에, 입력과 정답데이터를 이용해서 \n",
    "# -> 5)numpy 또는 TensorDataset 을 생성함\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import numpy as np\n",
    "# GPU\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('pytorch version : ', torch.__version__, ', device : ', DEVICE)\n",
    "\n",
    "# input data\n",
    "input_list = [ [ [1,2], [2,3], [3,4] ], [ [5,6], [6,7], [7,8] ] ]\n",
    "label_list = [10, 20]\n",
    "\n",
    "# numpy data\n",
    "x_data = np.array(input_list)\n",
    "y_data = np.array(label_list)\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# tensor data\n",
    "x_data = torch.FloatTensor(input_list)\n",
    "y_data = torch.FloatTensor(label_list)\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "# TensorDataset\n",
    "dataset = TensorDataset(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ac59c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch RNN API 설명\n",
    "# torch.nn.RNN(input_size, hidden_size, num_layers, batch_first)\n",
    "# input_size - RNN input layer로 들어가는 특성(feature) 값의 개수, 즉 feature 개수가 1개라면 input_size=1, feature 개수가 3개라면 input_size=3 으로 입력\n",
    "# hidden_size - hidden_state 개수, 즉 RNN 레이어 출력 개수를 나타냄, RNN layer 출력 개수가 1개라면 hidden_size=1, RNN layer 출력 개수가 3개라면 hidden_size=3\n",
    "# num_layers - RNN layer 겹겹이 쌓아올릴 수 있으며 num_layers는 몇개의 RNN layer가 쌓여 있는지를 나타냄, 즉 RNN layer 2개가 쌓여 있으면 num_layers=2(기본은 1)\n",
    "# batch_first - 입력 받은 데이터의 shape중 첫 번째 차원을 batch로 간주할 것인지를 설정하는데, 일반적으로 pytorch에서 입력데이터는 batch를 첫번째 차원으로 지정하기 때문에 batch_first=True로 설정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64bcb9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3])\n",
      "tensor([[[-0.2992,  0.0338, -0.2878],\n",
      "         [-0.3816, -0.0949, -0.1021],\n",
      "         [-0.2650,  0.1048, -0.1406]]], grad_fn=<TransposeBackward1>)\n",
      "==============================================================================\n",
      "torch.Size([1, 1, 3])\n",
      "tensor([[[-0.2650,  0.1048, -0.1406]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# RNN, LSTM, GRU API\n",
    "input_data = torch.tensor([ [ [1], [2], [3] ] ]).float() # (1, 3, 1) shape을 가지는 입력데이터\n",
    "\n",
    "# 간단한 RNN, LSTM, GRU 모델\n",
    "MyRNNModel = torch.nn.RNN(input_size=1, # input 레이어로 들어가는 특성(feature) 개수\n",
    "                          hidden_size=3, # RNN 출력 개수\n",
    "                          batch_first=True) # 입력 받은 데이터 shape 중 첫번째 차원을 batch로 간주 여부\n",
    "\n",
    "# 모델 추론 값\n",
    "# 첫번째 리턴값 outputs 모든 시점(time steps) hidden state, [h1, h2, h3]\n",
    "# 두번째 리턴값 last_hs 마지막 시점(time steps) hidden state, [h3]\n",
    "outputs, last_hs = MyRNNModel(input_data)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(outputs)\n",
    "print('==============================================================================')\n",
    "print(last_hs.shape)\n",
    "print(last_hs)\n",
    "\n",
    "# tensor([[[-0.5678,  0.3293, -0.6016], h1\n",
    "#         [-0.8632,  0.5324, -0.5242], h2\n",
    "#         [-0.9531,  0.7018, -0.5318]]], grad_fn=<TransposeBackward1>) h3\n",
    "\n",
    "# tensor([[[-0.9531,  0.7018, -0.5318]]], grad_fn=<StackBackward0>) h3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
