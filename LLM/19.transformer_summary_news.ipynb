{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer News Summary 요약 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터 로드 & 확인: 결측치 제거(None, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 및 json 파일 추출\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = './llm_data/ai_hub_news_summary' # 데이터셋 경로\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in os.listdir(data_dir): # 파일명 추출\n",
    "    if file_name.endswith('.json'): # .json 파일 추출\n",
    "        with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as f: # 파일 일기\n",
    "            data = json.load(f) # json 파일 로드\n",
    "            all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f384e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파싱: 본문과 요약 추출\n",
    "dataset = [] # 최종적으로 본문과 요약문을 담는 리스트\n",
    "\n",
    "for item in all_data: # JSON 데이터셋    \n",
    "    passage = item['Meta(Refine)']['passage'] # 본문\n",
    "\n",
    "    summaries = [] # 요약문\n",
    "    if item['Annotation'].get('summary1'):\n",
    "        summaries.append(item['Annotation'].get('summary1'))\n",
    "    if item['Annotation'].get('summary2'):        \n",
    "        summaries.append(item['Annotation'].get('summary2'))\n",
    "\n",
    "    # 요약이 없는 경우 제외\n",
    "    if passage and summaries:\n",
    "        dataset.append({'text': passage, 'summary': summaries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "def clean_text(text):\n",
    "    # 줄바꿈 -> 공백으로 바꾼다, 문장열 앞뒤 불필요한 공백 제거\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    return text\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['text'] = clean_text(sample['text']) # 문자열\n",
    "    sample['summary'] = [ clean_text(s) for s in sample['summary'] ] # 리스트\n",
    "\n",
    "# 데이터 확인\n",
    "print(len(dataset), dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 적용 데이터셋 -> DataLoader 생성, collate_fn 적용\n",
    "import torch\n",
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# BART 토크라이저 로드, Bart vocab size: 50265\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# print('Bart vocab size:', tokenizer.vocab_size)\n",
    "\n",
    "# collate_fn 정의\n",
    "def collate_fn(batch):\n",
    "    texts = [ item['text'] for item in batch ]\n",
    "    summaries = [ ' '.join(item['summary']) for item in batch ]\n",
    "\n",
    "    # 입력과 요약을 토큰화\n",
    "    inputs = tokenizer(\n",
    "        texts, # 본문 리스트\n",
    "        max_length=1024, # BART 입력 최대 길이\n",
    "        truncation=True,\n",
    "        padding='max_length', # padding='max_length' max_length까지 모든 문장을 강제 패딩, padding=True 배치 내에서 가장 긴 문장 길이에 맞추어 패딩\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        summaries, # 요약 리스트를 하나의 문자열로 합치기\n",
    "        max_length=128, # 요약 최대 길이\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': labels['input_ids']        \n",
    "    }\n",
    "dataset_small = dataset[:1000] # 데이터 축소\n",
    "total_size = len(dataset_small) # 전체 데이터 길이\n",
    "train_size = int(0.8 * total_size) # 비율 설정(예시: train 80%, val 10%, test 10%)\n",
    "val_size = int(0.1 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "generator = torch.Generator().manual_seed(42) # 시드 고정\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_small, [train_size, val_size, test_size], generator=generator) # 데이터 분리\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 배치 확인\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape, batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.amp import autocast, GradScaler\n",
    "import os\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "# BartForConditionalGeneration 베이스 모델\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Feature Extraction\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name: # LoRA 모듈이 아닌 경우\n",
    "        param.requires_grad = False # 모델 본체 동결\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "\n",
    "# 모델 확인, trainable params: 2,359,296 || all params: 408,649,728 || trainable%: 0.5773\n",
    "model.print_trainable_parameters() # LoRA 적용 확인\n",
    "print(model)\n",
    "\n",
    "# 최적화 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 반복횟수\n",
    "num_epochs = 3\n",
    "\n",
    "# Early Stopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/19_transformer_summary_news/best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = valid_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !='' and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        \n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'Best model saved at {self.path}')\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프: autocast(속도 향상) 적용, GradScaler(안정적 학습) 적용\n",
    "# autocast 적용: 연산을 FP16(half precision)과 FP32(full precision)중 적절히 선택해서 실행\n",
    "# - 속도 향상: 대부분의 연산을 FP16으로 처리해 GPU 연산 속도를 높인다\n",
    "# - 안정성 유지: 손실이 큰 연산(예시:소프트맥스,레이어정규화)은 FP32로 자동 변환해 정확도를 보장한다\n",
    "# GradScaler 적용: FP16 학습에서는 작은 값이 underflow(0으로 사라짐)될 위험이 있다\n",
    "# - 안정적 학습 보장: GradScaler는 손실(loss)를 크게 스케일링해서 역전파 시 그래디언트가 사라지지 않도록 한다\n",
    "# - 이후 업데이트 단계에서 다시 원래 크기로 되돌려 안정적인 학습을 보장한다. 즉 FP16 학습에서 발생할 수 있는 수치 불안정 문제를 해결하는 역할\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]'):\n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "\n",
    "        # 학습데이터 GPU 지정\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # 모델 예측\n",
    "            loss = outputs.loss # 손실값\n",
    "            logits = outputs.logits # Softmax 계산된 logits값\n",
    "        \n",
    "        # 오차역전파(GradScaler로 안정성 확보)\n",
    "        scaler.scale(loss).backward() # 미분연산\n",
    "        scaler.step(optimizer) # 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() # 손실 누적\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # validation\n",
    "    model.eval() # 검증/추론 모드 지정\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f'Epoch {epoch+1} [Validation]'):\n",
    "            # 검증데이터 GPU 지정\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) # 모델 예측\n",
    "                loss = outputs.loss # 손실값\n",
    "                # logits = outputs.logits # Softmax 계산된 logits값\n",
    "            \n",
    "                val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    # Early Stopping 객체 호출\n",
    "    early_stopping(val_loss, model)\n",
    "    status = 'STOP' if early_stopping.early_stop else 'CONTINUE' # # Early Stopping status 상태값\n",
    "\n",
    "    print(f'Epoch {epoch+1} | Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Early Stopping 체크\n",
    "    if early_stopping.early_stop: # early_stop=True 학습 종료\n",
    "        print('Early stopping triggered')\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835af3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 최적 모델 로드\n",
    "\n",
    "# torch.load() 파일에서 파라미터(가중치) 딕셔너리를 불러옴\n",
    "# model.load_state_dict() 불러온 파리미터를 모델 구조에 맞게 적용\n",
    "model.load_state_dict(torch.load('./llm_models/19_transformer_summary_news/best_model.pt'))\n",
    "\n",
    "# 모델을 실행할 디바이스(GPU or CPU)에 올린다\n",
    "model.to(device)\n",
    "\n",
    "# 검증/추론 모드 전환 \n",
    "# - model.eval() 검증/추촌 모드 에서는 Dropout 등이 비활성화되어 일관된 추론 결과를 보장한다, \n",
    "# - model.train() 학습 모드 에서는 Dropout, BatchNorm 등이 활성화되어 파라미터 업데이트를 준비 한다\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "model.eval() # 검증/추론 모드 지정\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad(): # 미분 연산 하지 않음\n",
    "    for batch in tqdm(test_loader, desc=f'Epoch {epoch+1} [Test]'): # 테스트 데이터 GPU 지정\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss        \n",
    "        test_loss += loss.item() # 손실값 누적    \n",
    "avg_test_loss = test_loss / len(test_loader) # 평균 손실값 게산\n",
    "print(f'Final Test Loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 요약 생성\n",
    "sample = test_dataset[0] # 테스트 데이터셋에서 첫번째 샘플\n",
    "text = sample['text'] # 기사 본문 추출\n",
    "\n",
    "# 본문을 토크나이저로 인코딩 -> GPU로 이동\n",
    "inputs = tokenizer( \n",
    "    text, # 본문(text)을 BART 모델이 이해할 수 있는 토큰ID(input_ids)와 어텐션 마스크(attention_mask)로 변환\n",
    "    max_length=1024, # 입력 문장을 최대 1024 토큰까지 자름\n",
    "    truncation=True,\n",
    "    padding='max_length', # 모든 문장을 동일 길이(1024)로 맞추기 위해 패딩 추가\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# 학습된 모델로 요약 생성\n",
    "summary_ids = model.generate(\n",
    "    inputs=inputs['input_ids'], # input_ids, attention_mask를 모델에 넣어 요약 생성\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=128, # 요약 최대 길이\n",
    "    num_beams=4, # Beam Search로 다양한 후보 탐색\n",
    "    early_stopping=True # EOS 토큰 나오면 조기 종료\n",
    ")\n",
    "\n",
    "# 결과 디코딩, 모델이 생성한 토큰 ID를 사람이 읽을 수 있는 문자열로 변환, <pad>/<eos> 같은 특수 토큰 제거\n",
    "print('Generated Summary:', tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "print('Reference Summary:', ' '.join(sample['summary']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
