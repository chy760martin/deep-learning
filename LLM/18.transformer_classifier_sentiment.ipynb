{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ec65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer Sentiment Classifier 감정 분류 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터 로드 & 확인: 결측치 제거(None, \"\")\n",
    "# - 2. 토크나이저 적용: Hugging Face DistilBertTokenizer 베이스 모델 사용\n",
    "# - 3. 데이터셋 -> DataLoader 변환: DistilBertTokenizer 베이스 모델 토크나이저에서 DataLoader 로 바로 변환, Custom Dataset 필요 없음\n",
    "# - 4. 모델정의 & GPU설정 & 전이학습 & 본체 동결(Feature Extraction) + LoRA Fine-tuning 조합 & EarlyStopping 클래스 정의\n",
    "# - 전이 학습: DistilBertForSequenceClassification 베이스 모델(distilbert-base-uncased), num_labels=2 긍정/부정 2개 클래스\n",
    "# - 본체 동결: model.distilbert.parameters()는 사전학습된 본체(embedding + transformer 블록)의 모든 파라미터를 의미, 따라서 학습은 classifier 레이어(pre_classifier, classifier)만 진행된다\n",
    "# - LoRA: Attention의 q_lin, v_lin 레이어에서 LoRA가 768차원 → 8차원 축소 → 768차원 복원 과정을 거쳐 업데이트를 추가하는 구조\n",
    "# - EarlyStopping: - 과적합 방지 + 최적 모델 확보 + 자원 절약 + EarlyStopping 발동 시점에서 최적 모델 가중치를 자동 저장\n",
    "# - 5. 최적화 설정 & 학습 루프 & 검증 루프 EarlyStopping 클래스 적용\n",
    "# - 최적화 설정: autocast(속도 향상) GradScaler(안정적 학습) 적용\n",
    "# - 6. 최적 모델 로드: GPU 설정, 검증/추론 모드 적용\n",
    "# - 7. 테스트 데이터 평가: 사이킷런 평가 지표 적용\n",
    "# - classification_report 정확도/정밀도/재현율/F1-socre 확인\n",
    "# - confusion_matrix 오분류 패턴 분석\n",
    "# - 8. Confusion Matrix Heatmap: Confusion Matrix를 Heatmap 그래프로 시각화 적용, 테스트 데이터 평가 데이터를 활용\n",
    "# - 9. 추론 테스트\n",
    "# - 10. FastAPI 추론 서비스\n",
    "# - /llm_app/transformer_classifier_sentiment_18_app.py\n",
    "# - FastAPI 구동: 터미널에서 구동, uvicorn transformer_classifier_sentiment_18_app:app --reload\n",
    "# - 윈도우 파워쉘: Invoke-RestMethod -Uri \"http://127.0.0.1:8000/predict\" -Method Post -ContentType \"application/json\" -Body '{\"text\":\"I really love this movie, it was fantastic!\"}'\n",
    "# - Postman app\n",
    "# - API 코드로 테스트: Python, Java...\n",
    "\n",
    "# DistilBert 구조 특징 \n",
    "# - BERT 계열 모델은 Transformer의 Encoder 부분만 사용한다\n",
    "# - 입력 문장을 임베딩 -> 여러층의 Transformer Encoder 블록 -> [CLS]토큰 벡터 추출 -> 분류기(Classifier)\n",
    "\n",
    "# DistilBertForSequenceClassification 내부 흐름\n",
    "# 입력(DataLoader 배치): \n",
    "# - input_ids:(batch_size,seq_len)->(32,128)\n",
    "# - attention_mask: (32,128)\n",
    "# - labels:(32)\n",
    "# DistilBertForSequenceClassification(\n",
    "#   (distilbert): DistilBertModel(\n",
    "#     (embeddings): Embeddings(\n",
    "#       (word_embeddings): Embedding(30522, 768, padding_idx=0) : (32,128) -> (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#       (position_embeddings): Embedding(512, 768) : (32,128) -> (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#       (dropout): Dropout(p=0.1, inplace=False) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#     )\n",
    "#     (transformer): Transformer(\n",
    "#       (layer): ModuleList(\n",
    "#         (0-5): 6 x TransformerBlock( : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#           (attention): DistilBertSdpaAttention(\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#           )\n",
    "#           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#           (ffn): FFN( : (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "#             (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "#             (activation): GELUActivation()\n",
    "#           )\n",
    "#           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#         )\n",
    "#       )\n",
    "#     ) : Transformer 출력(output_layer_norm 이후) (32,128,768) (batch_size,seq_len,hidden_dim) shape을 가지며\n",
    "#         내부 forward()에서 hidden_states[:, 0, :] 선택([CSL] 벡터를 추출) -> (32,768) (batch_size,hidden_dim) 변경 한다\n",
    "#   ) \n",
    "#   (pre_classifier): Linear(in_features=768, out_features=768, bias=True) : (32,768) -> (32,768) (batch_size,hidden_dim)\n",
    "#   (classifier): Linear(in_features=768, out_features=2, bias=True) : (32,768)@(768,2) -> (32,2) 입력 벡터와 가중치 행렬의 내적 계산\n",
    "#   (dropout): Dropout(p=0.2, inplace=False) : (32,768) (batch_size,hidden_dim) 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c229bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본체 동결(Feature Extraction) + LoRA Fine-tuning 조합\n",
    "# - 원래 q_lin / v_lin은 Linear(768 → 768) 구조: LoRA는 이 weight를 그대로 두고, 추가로 저차원 랭크 행렬을 학습\n",
    "# - Attention의 q_lin, v_lin 레이어에서 LoRA가 768차원 → 8차원 축소 → 768차원 복원 과정을 거쳐 업데이트를 추가하는 구조\n",
    "\n",
    "# trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n",
    "# PeftModelForSequenceClassification(\n",
    "#   (base_model): LoraModel(\n",
    "#     (model): DistilBertForSequenceClassification(\n",
    "#       (distilbert): DistilBertModel(\n",
    "#         (embeddings): Embeddings(\n",
    "#           (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "#           (position_embeddings): Embedding(512, 768)\n",
    "#           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#         (transformer): Transformer(\n",
    "#           (layer): ModuleList(\n",
    "#             (0-5): 6 x TransformerBlock(\n",
    "#               (attention): DistilBertSdpaAttention(\n",
    "#                 (dropout): Dropout(p=0.1, inplace=False)\n",
    "#                 (q_lin): lora.Linear(\n",
    "#                   (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
    "#                   (lora_dropout): ModuleDict(\n",
    "#                     (default): Dropout(p=0.1, inplace=False)\n",
    "#                   )\n",
    "#                   (lora_A): ModuleDict(\n",
    "#                     (default): Linear(in_features=768, out_features=8, bias=False)\n",
    "#                   )\n",
    "#                   (lora_B): ModuleDict(\n",
    "#                     (default): Linear(in_features=8, out_features=768, bias=False)\n",
    "#                   )\n",
    "#                   (lora_embedding_A): ParameterDict()\n",
    "#                   (lora_embedding_B): ParameterDict()\n",
    "#                   (lora_magnitude_vector): ModuleDict()\n",
    "#                 )\n",
    "#                 (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#                 (v_lin): lora.Linear(\n",
    "#                   (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
    "#                   (lora_dropout): ModuleDict(\n",
    "#                     (default): Dropout(p=0.1, inplace=False)\n",
    "#                   )\n",
    "#                   (lora_A): ModuleDict(\n",
    "#                     (default): Linear(in_features=768, out_features=8, bias=False)\n",
    "#                   )\n",
    "#                   (lora_B): ModuleDict(\n",
    "#                     (default): Linear(in_features=8, out_features=768, bias=False)\n",
    "#                   )\n",
    "#                   (lora_embedding_A): ParameterDict()\n",
    "#                   (lora_embedding_B): ParameterDict()\n",
    "#                   (lora_magnitude_vector): ModuleDict()\n",
    "#                 )\n",
    "#                 (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#               )\n",
    "#               (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#               (ffn): FFN(\n",
    "#                 (dropout): Dropout(p=0.1, inplace=False)\n",
    "#                 (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "#                 (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "#                 (activation): GELUActivation()\n",
    "#               )\n",
    "#               (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#             )\n",
    "#           )\n",
    "#         )\n",
    "#       )\n",
    "#       (pre_classifier): ModulesToSaveWrapper(\n",
    "#         (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
    "#         (modules_to_save): ModuleDict(\n",
    "#           (default): Linear(in_features=768, out_features=768, bias=True)\n",
    "#         )\n",
    "#       )\n",
    "#       (classifier): ModulesToSaveWrapper(\n",
    "#         (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
    "#         (modules_to_save): ModuleDict(\n",
    "#           (default): Linear(in_features=768, out_features=2, bias=True)\n",
    "#         )\n",
    "#       )\n",
    "#       (dropout): Dropout(p=0.2, inplace=False)\n",
    "#     )\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909c366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 & 확인\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face datasets 라이브러리 함수(CSV,JSON.. 데이터 로드)\n",
    "dataset = load_dataset( # DatasetDict 형태로 반환\n",
    "    'csv', # csv 포멧 지정\n",
    "    data_files={ # train/test 데이터 각각 지정\n",
    "        'train': \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
    "        'test': \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    },\n",
    "    delimiter='\\t' # 구분자, 현재 데이터에서는 탭으로 구분\n",
    ")\n",
    "\n",
    "# 결측치 제거\n",
    "all_clean_train = dataset[\"train\"].filter(lambda x: x[\"document\"] is not None and x[\"document\"].strip() != \"\")\n",
    "all_clean_test = dataset[\"test\"].filter(lambda x: x[\"document\"] is not None and x[\"document\"].strip() != \"\")\n",
    "\n",
    "# 데이터 축소\n",
    "clean_train = all_clean_train.select(range(10000))\n",
    "clean_test = all_clean_test.select(range(5000))\n",
    "\n",
    "print(clean_train)\n",
    "print(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959e7d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9976970, 'label': 0, 'input_ids': [101, 1463, 30006, 1457, 30008, 29996, 30019, 30025, 1012, 1012, 100, 100, 1459, 30011, 30020, 29997, 30011, 29994, 30019, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'id': 6270596, 'label': 1, 'input_ids': [101, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 적용\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenizer_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"document\"],   # 문자열 리스트만 들어옴\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128 # 시퀀스 길이, 연산량 절반 이상 감소\n",
    "    )\n",
    "\n",
    "tokenized_train = clean_train.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")\n",
    "\n",
    "tokenized_test = clean_test.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")\n",
    "\n",
    "print(tokenized_train[0])\n",
    "print(tokenized_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7af3042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([32, 128]) torch.Size([32, 128]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 -> DataLoader 변환\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# collate_fn 함수 : Hugging Face Dataset에서 꺼낸 샘플은 파이썬 dict 형태, collate_fn() 각 샘플을 모아 PyTorch 텐서로 변환\n",
    "def collate_fn(batch): # batch 샘플 리스트 : [{'input_ids':[...],'attention_mask':[...],'label':[...]}, {...}, ...]\n",
    "    input_ids = torch.tensor([ item['input_ids'] for item in batch ]) # 토큰화된 문장\n",
    "    attention_mask = torch.tensor([ item['attention_mask'] for item in batch ]) # 패딩 여부\n",
    "    labels = torch.tensor([ item['label'] for item in batch ]) # 감성 분류 라벨(0=부정/1=긍정)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# train: 학습용 데이터, 배치크기 16, epoch 마다 데이터 순서 섞음\n",
    "train_loader = DataLoader(tokenized_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# valid: 검증용 데이터, 학습 데이터에서 10%를 검증용으로 분리 Hugging Face에서 제공하는 train_test_split() 메서드\n",
    "valid_loader = DataLoader(tokenized_train.train_test_split(test_size=0.1)['test'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# test: 테스트 데이터, 성능 최종 평가용\n",
    "test_loader = DataLoader(tokenized_test, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# train_loader 데이터 확인\n",
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07545c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): DistilBertSdpaAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.amp import autocast # 최신 API\n",
    "from torch.amp import GradScaler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# DistilBertForSequenceClassification 베이스 모델(distilbert-base-uncased), num_labels=2 긍정/부정 2개 클래스\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "# DistilBERT 본체 동결(Feature Extraction, Parameter Freezing, Weight Freezing)\n",
    "# - model.distilbert.parameters()는 사전학습된 본체(embedding + transformer 블록)의 모든 파라미터를 의미 한다\n",
    "# - requires_grad = False로 설정하면 역전파 시 이 파라미터들은 업데이트되지 않는다\n",
    "# - 따라서 학습은 classifier 레이어(pre_classifier, classifier)만 진행된다\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# LoRA config & warp\n",
    "# - trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925, 학습 가능한 파라미터 수: 739,586(전체의 1.0925%)\n",
    "# - Attention의 q_lin, v_lin 레이어에서 LoRA가 768차원 → 8차원 축소 → 768차원 복원 과정을 거쳐 업데이트를 추가하는 구조\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_lin', 'v_lin'], # DistilBERT attention\n",
    "    bias='none',\n",
    "    task_type='SEQ_CLS'\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 모델 확인\n",
    "model.print_trainable_parameters() # LoRA 적용 확인용\n",
    "print(model)\n",
    "\n",
    "# 최적화 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 반복횟수\n",
    "num_epochs = 3\n",
    "\n",
    "# EarlyStopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/18_transformer_classifier_sentiment/best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = valid_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !=\"\" and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            \n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'Best model saved at {self.path}')\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d7c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 313/313 [02:03<00:00,  2.54it/s]\n",
      "1 [Valid]: 100%|██████████| 32/32 [00:04<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ./llm_models/18_transformer_classifier_sentiment/best_model.pt\n",
      "Epoch 1 | Tran Loss: 0.6783 | Train Acc: 0.5610 | Valid Loss: 0.6737 | Valid Acc: 0.5980 | EarlyStopping: CONTINUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 313/313 [02:02<00:00,  2.56it/s]\n",
      "2 [Valid]: 100%|██████████| 32/32 [00:04<00:00,  6.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ./llm_models/18_transformer_classifier_sentiment/best_model.pt\n",
      "Epoch 2 | Tran Loss: 0.6579 | Train Acc: 0.5983 | Valid Loss: 0.6412 | Valid Acc: 0.6230 | EarlyStopping: CONTINUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 313/313 [02:02<00:00,  2.56it/s]\n",
      "3 [Valid]: 100%|██████████| 32/32 [00:04<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ./llm_models/18_transformer_classifier_sentiment/best_model.pt\n",
      "Epoch 3 | Tran Loss: 0.6509 | Train Acc: 0.6093 | Valid Loss: 0.6332 | Valid Acc: 0.6270 | EarlyStopping: CONTINUE\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프: autocast(속도 향상) 적용, GradScaler(안정적 학습) 적용\n",
    "# autocast 적용: 연산을 FP16(half precision)과 FP32(full precision)중 적절히 선택해서 실행\n",
    "# - 속도 향상: 대부분의 연산을 FP16으로 처리해 GPU 연산 속도를 높인다\n",
    "# - 안정성 유지: 손실이 큰 연산(예시:소프트맥스,레이어정규화)은 FP32로 자동 변환해 정확도를 보장한다\n",
    "# GradScaler 적용: FP16 학습에서는 작은 값이 underflow(0으로 사라짐)될 위험이 있다\n",
    "# - 안정적 학습 보장: GradScaler는 손실(loss)를 크게 스케일링해서 역전파 시 그래디언트가 사라지지 않도록 한다\n",
    "# - 이후 업데이트 단계에서 다시 원래 크기로 되돌려 안정적인 학습을 보장한다. 즉 FP16 학습에서 발생할 수 있는 수치 불안정 문제를 해결하는 역할\n",
    "from tqdm import tqdm # 시각화(진행바)\n",
    "\n",
    "# DistilBert 구조 특징 \n",
    "# - BERT 계열 모델은 Transformer의 Encoder 부분만 사용한다\n",
    "# - 입력 문장을 임베딩 -> 여러층의 Transformer Encoder 블록 -> [CLS]토큰 벡터 추출 -> 분류기(Classifier)\n",
    "# DistilBertForSequenceClassification 내부 흐름\n",
    "# 1. 입력: (batch_size,seq_len)\n",
    "# 2. Encoder(DistilBERT 본체) \n",
    "# - Embedding: (batch_size,seq_len,hidden_dim) \n",
    "# - Transformer Block(6개): (batch_size,seq_len,hidden_dim)\n",
    "# 3. [CLS] 토큰 추출: (batch_size,hidden_dim)\n",
    "# 4. Classifier 레이어: (batch_size,num_labels)\n",
    "# 5. 출력: logits, loss\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1} [Train]'):        \n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "        # 학습데이터 GPU 지정\n",
    "        input_ids = batch['input_ids'].to(device) # [32, 128]\n",
    "        attention_mask = batch['attention_mask'].to(device) # [32, 128]\n",
    "        labels = batch['labels'].to(device) # [32]\n",
    "\n",
    "        # 모델 forward(autocast 영역) 내부 shape 변환\n",
    "        # - 임베딩 레이어: (32,128) -> (32,128,768) \n",
    "        # - Transformer 블록: (32,128,768) -> (32,128,768)\n",
    "        # - CLS 토큰 추출: (32,768) \n",
    "        # - Classifier 레이어: (32,768) -> (32,2) \n",
    "        # - outputs.logits.shape: (32,2) \n",
    "        # - outputs.loss: scalar\n",
    "        \n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        # with autocast('cuda'):\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): # 최신 버전 GPU에서 FP16 연산\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits # (32,2)        \n",
    "        # 오차역전차\n",
    "        scaler.scale(loss).backward() # 미분 연산\n",
    "        scaler.step(optimizer) # 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "        scaler.update()        \n",
    "        # 손실 누적\n",
    "        total_loss += loss.item()\n",
    "        # Accuracy 계산\n",
    "        preds = logits.argmax(dim=-1) # 예측 클래스 (32,)\n",
    "        correct += (preds == labels).sum().item() # 맞춘 개수\n",
    "        total += labels.size(0)    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # validation\n",
    "    model.eval() # 검증/추론 모드\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 검증/추론 모드에서는 미분 연산 처리 하지 않음\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'{epoch + 1} [Valid]'):\n",
    "            # 검증 데이터 GPU에 할당\n",
    "            input_ids = batch['input_ids'].to(device) # (32,128) (batch_size,seq_len), 토큰화된 문장 ID(각 문장을 토큰 단위로 숫자로 변환한 것)\n",
    "            attention_mask = batch['attention_mask'].to(device) # (32,128) (batch_size,seq_len), 패딩 토큰을 무시하기 위한 마스크 1=실제 토큰 0=패딩\n",
    "            labels = batch['labels'].to(device) # (32,) (batch_size,), 각 문장의 정답 라벨(긍정=1,부정=0 클래스)\n",
    "\n",
    "            # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "            # with autocast('cuda'):\n",
    "            with autocast(device_type='cuda', dtype=torch.float16): # 최신 버전 GPU에서 FP16 연산\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits            \n",
    "            # 손실 누적\n",
    "            total_loss += loss.item()\n",
    "            # Accuracy 계산\n",
    "            # logits (32,2) (batch_size,num_labels), logits.argmax(dim=-1) (32,) 32개 각 샘플중 2개의 점수중 더 큰쪽의 인덱스(0또는1) 선택\n",
    "            preds = logits.argmax(dim=-1) # (32,)\n",
    "            # (preds == labels) 예측/정답을 비교해 True/False 벡터 생성\n",
    "            # ->.sum() True=1/False=0 계산해 맞춘 개수 합산(PyTorch 텐서)\n",
    "            # ->.item() 파이썬 숫자로 변환(파이썬 숫자 int/float)\n",
    "            correct += (preds == labels).sum().item() # (맞춘 개수 누적)\n",
    "            total += labels.size(0) # (32,)\n",
    "        # 손실 계산\n",
    "        # - 예시) 전체 검증데이터가 320개, 1배치가 32 -> 10번 배치 반복을 한다\n",
    "        # - 계산) total_loss 손실 누적 합산값 / 10(10번 배치), 즉 손실을 배치 개수로 나누어 배치 평균 손실을 구한다\n",
    "        valid_loss = total_loss / len(valid_loader)\n",
    "        valid_acc = correct / total # 정확도 계산\n",
    "\n",
    "        # Early Stopping 객체 호출\n",
    "        early_stopping(valid_loss, model)\n",
    "        status = 'STOP' if early_stopping.early_stop else 'CONTINUE' # # Early Stopping status 상태값\n",
    "\n",
    "        print(f'Epoch {epoch + 1} | Tran Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}'\n",
    "              f' | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f} | EarlyStopping: {status}')\n",
    "        \n",
    "        # Early Stopping 체크\n",
    "        if early_stopping.early_stop: # early_stop=True 학습 종료\n",
    "            print('Early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e98212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적 모델 로드\n",
    "\n",
    "# torch.load() 파일에서 파라미터(가중치) 딕셔너리를 불러옴\n",
    "# model.load_state_dict() 불러온 파리미터를 모델 구조에 맞게 적용\n",
    "model.load_state_dict(torch.load('./llm_models/18_transformer_classifier_sentiment/best_model.pt'))\n",
    "\n",
    "# 모델을 실행할 디바이스(GPU or CPU)에 올린다\n",
    "model.to(device)\n",
    "\n",
    "# 검증/추론 모드 전환 \n",
    "# - model.eval() 검증/추촌 모드 에서는 Dropout 등이 비활성화되어 일관된 추론 결과를 보장한다, \n",
    "# - model.train() 학습 모드 에서는 Dropout, BatchNorm 등이 활성화되어 파라미터 업데이트를 준비 한다\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb7681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 [Test]: 100%|██████████| 157/157 [00:23<00:00,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.54      0.58      2478\n",
      "    positive       0.60      0.68      0.64      2522\n",
      "\n",
      "    accuracy                           0.61      5000\n",
      "   macro avg       0.61      0.61      0.61      5000\n",
      "weighted avg       0.61      0.61      0.61      5000\n",
      "\n",
      "[[1344 1134]\n",
      " [ 814 1708]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "\n",
    "# 사이킷런 평가 지표 함수 라이브러리 호출\n",
    "# - classification_report 정확도/정밀도/재현율/F1-socre 확인\n",
    "# - confusion_matrix 오분류 패턴 분석\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 전체 예측값, 실제 라벨을 저장 리스트 초기화\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "# 검증/추론시에는 역전파(gradient 계산)가 필요 없으므로 메모리/연산 절약\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=f'{epoch + 1} [Test]'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        # with autocast('cuda'):\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): # 최신 버전 GPU에서 FP16 연산\n",
    "            # 모델 예측\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # 각 샘플에서 가장 큰 점수의 인덱스(0=Negative, 1=Positive) 선택\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy() # PyTorch Tensor -> numpy 변환\n",
    "        labels = labels.cpu().numpy() # PyTorch Tensor -> numpy 변환\n",
    "\n",
    "        all_preds.extend(preds) \n",
    "        all_labels.extend(labels)\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=['Negative', 'positive']))\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6a4883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix를 Heatmap 그래프로 시각화, 테스트 데이터 평가 데이터를 활용\n",
    "import matplotlib.pyplot as plt # 그래프 그리기 기본 라이브러리\n",
    "import seaborn as sns # 시각화 스타일을 더 깔끔하게 만들어주는 라이브러리\n",
    "import numpy as np # Confunsion Matrix 데이터를 배열 형태로 관리\n",
    "\n",
    "# Confusion Matrix 데이터, 2x2 행렬 구조 실제 라벨과 예측 라벨의 매칭 결과\n",
    "# cm = np.array([ [1562, 916],\n",
    "#                [973, 1549] ])\n",
    "cm = np.array(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# 클래스 이름\n",
    "labels = ['Negative', 'Positive']\n",
    "\n",
    "# Heatmap 시각화\n",
    "plt.figure(figsize=(6, 5))\n",
    "# cm 데이터, annot=True 각 셀에 숫자 표시, fmt='d' 정수 형태로 출력, cmap='Blues' 파란색 계열 색상 맵 선택, xticklabels/yticklabels 축 라벨에 클래스 이름 푝시\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48b8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I really love this movie, it was fantastic!\n",
      "Prediction: Positive\n",
      "\n",
      "Text: This product is terrible and I will never buy it again.\n",
      "Prediction: Negative\n",
      "\n",
      "Text: The service was okay, not too bad but not great either.\n",
      "Prediction: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추론 함수\n",
    "\n",
    "def predict(text, tokenizer, model, device):\n",
    "    model.eval() # 검증/추론 모드 전환\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    # 검증/추론시 미분 연산 하지 않음\n",
    "    with torch.no_grad():\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): # GPU에서 FP16 연산으로 추론\n",
    "            outputs = model(**inputs) # 모델 예측\n",
    "            pred = outputs.logits.argmax(dim=-1).item()\n",
    "    return 'Positive' if pred == 1 else 'Negative'\n",
    "\n",
    "# 추론 테스트\n",
    "test_texts = [\n",
    "    \"I really love this movie, it was fantastic!\",\n",
    "    \"This product is terrible and I will never buy it again.\",\n",
    "    \"The service was okay, not too bad but not great either.\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    result = predict(text, tokenizer, model, device)\n",
    "    print(f'Text: {text}\\nPrediction: {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "391e6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"results\":{\"text\":\"I really love this movie, it was fantastic!\",\"sentiment\":\"Positive\"}}\n",
      "{'results': {'text': 'I really love this movie, it was fantastic!', 'sentiment': 'Positive'}}\n"
     ]
    }
   ],
   "source": [
    "# FastAPI 추론 서비스\n",
    "# - /llm_app/transformer_classifier_sentiment_18_app.py\n",
    "# - FastAPI 구동: 터미널에서 구동, uvicorn transformer_classifier_sentiment_18_app:app --reload\n",
    "# - 윈도우 파워쉘: Invoke-RestMethod -Uri \"http://127.0.0.1:8000/predict\" -Method Post -ContentType \"application/json\" -Body '{\"text\":\"I really love this movie, it was fantastic!\"}'\n",
    "# - Postman app\n",
    "# - API 코드로 테스트: Python, Java...\n",
    "\n",
    "# 단일 문장 추론\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/predict\"\n",
    "data = {\"text\": \"I really love this movie, it was fantastic!\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.status_code)\n",
    "print(response.text) # 원본 응답 확인\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07b7607f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"results\":[{\"text\":\"I really love this movie, it was fantastic!\",\"sentiment\":\"Positive\"},{\"text\":\"This product is terrible and I will never buy it again.\",\"sentiment\":\"Negative\"},{\"text\":\"The service was okay, not too bad but not great either.\",\"sentiment\":\"Positive\"}]}\n",
      "{'results': [{'text': 'I really love this movie, it was fantastic!', 'sentiment': 'Positive'}, {'text': 'This product is terrible and I will never buy it again.', 'sentiment': 'Negative'}, {'text': 'The service was okay, not too bad but not great either.', 'sentiment': 'Positive'}]}\n"
     ]
    }
   ],
   "source": [
    "# 여러 문장 추론\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/predict_batch\"\n",
    "data = {\n",
    "    \"texts\": [\n",
    "        \"I really love this movie, it was fantastic!\", \n",
    "        \"This product is terrible and I will never buy it again.\",\n",
    "        \"The service was okay, not too bad but not great either.\"\n",
    "]}\n",
    "\n",
    "response = requests.post(url=url, json=data)\n",
    "print(response.status_code)\n",
    "print(response.text) # 원본 응답 확인\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
