{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 : Transformer 기반 한국어 대화 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12261cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version : 2.7.1+cu118, Device : cuda\n"
     ]
    }
   ],
   "source": [
    "# 1) 환경 설정\n",
    "# !pip install pandas numpy torch transformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Pytorch Version : {torch.__version__}, Device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17381cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능: True\n",
      "GPU 이름: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(\"CUDA 사용 가능:\", torch.cuda.is_available())\n",
    "# print(\"GPU 이름:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"GPU 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cff66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(51201, 768)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) 데이터셋 로딩 및 전처리\n",
    "# ChatbotData.csv 다운로드\n",
    "url = \"https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# 간단한 전처리\n",
    "df = df.dropna()\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # 셔플\n",
    "\n",
    "# 토크나이저 설정 (KoGPT2 기반)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "kogpt_model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\").to(device)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "kogpt_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9178bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Dataset 클래스 정의\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=40):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q = self.df.iloc[idx][\"Q\"]\n",
    "        a = self.df.iloc[idx][\"A\"]\n",
    "        q_ids = tokenizer.encode(q, max_length=self.max_len, padding='max_length', truncation=True)\n",
    "        a_ids = tokenizer.encode(a, max_length=self.max_len, padding='max_length', truncation=True)\n",
    "        return torch.tensor(q_ids), torch.tensor(a_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bd27c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Transformer 모델 정의\n",
    "class TransformerChatbot(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead,\n",
    "                                          num_encoder_layers=num_layers,\n",
    "                                          num_decoder_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src).permute(1, 0, 2)  # [seq_len, batch, d_model]\n",
    "        tgt = self.embedding(tgt).permute(1, 0, 2)\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output)\n",
    "        return output.permute(1, 0, 2)  # [batch, seq_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fef9712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AI\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 6.1420\n",
      "Epoch 2 Loss: 4.0712\n",
      "Epoch 3 Loss: 3.0455\n",
      "Epoch 4 Loss: 2.2801\n",
      "Epoch 5 Loss: 1.6892\n",
      "Epoch 6 Loss: 1.2364\n",
      "Epoch 7 Loss: 0.8854\n",
      "Epoch 8 Loss: 0.6199\n",
      "Epoch 9 Loss: 0.4307\n",
      "Epoch 10 Loss: 0.2974\n",
      "Epoch 11 Loss: 0.2053\n",
      "Epoch 12 Loss: 0.1430\n",
      "Epoch 13 Loss: 0.1035\n",
      "Epoch 14 Loss: 0.0842\n",
      "Epoch 15 Loss: 0.0744\n",
      "Epoch 16 Loss: 0.0721\n",
      "Epoch 17 Loss: 0.0633\n",
      "Epoch 18 Loss: 0.0590\n",
      "Epoch 19 Loss: 0.0550\n",
      "Epoch 20 Loss: 0.0526\n",
      "Epoch 21 Loss: 0.0541\n",
      "Epoch 22 Loss: 0.0524\n",
      "Epoch 23 Loss: 0.0457\n",
      "Epoch 24 Loss: 0.0417\n",
      "Epoch 25 Loss: 0.0422\n",
      "Epoch 26 Loss: 0.0415\n",
      "Epoch 27 Loss: 0.0430\n",
      "Epoch 28 Loss: 0.0430\n",
      "Epoch 29 Loss: 0.0398\n",
      "Epoch 30 Loss: 0.0369\n"
     ]
    }
   ],
   "source": [
    "# 5) 학습 진행\n",
    "dataset = ChatDataset(df, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "model = TransformerChatbot(vocab_size=vocab_size).to(device)\n",
    "model.embedding = nn.Embedding(len(tokenizer), 512).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = loss_fn(output.reshape(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33088dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) 대화 생성 함수\n",
    "\n",
    "def kogpt_generate(text, max_len=50):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        output = kogpt_model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_len,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.8,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3744ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘 기분이 어때요?\"\n",
      "\"아뇨. 그럼 제가 대신 마무리를 잘 하겠습니다. 저는 내일 다시 뵙게 될 겁니다. 고맙습니다, 선생님.\"\n",
      "오랜만에 만난 남매는 서로 부러운 눈빛\n"
     ]
    }
   ],
   "source": [
    "# 7) 테스트\n",
    "\n",
    "print(kogpt_generate(\"오늘 기분이 어때요?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
