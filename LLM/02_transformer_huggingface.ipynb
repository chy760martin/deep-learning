{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90abaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 패키지 설치\n",
    "# !pip install transformers==4.50.0 datasets==3.5.0 huggingface_hub==0.29.0 -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e50da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT 와 GPT-2 모델을 활용할때 허깅페이스 트랜스포머 코드 비교\n",
    "\n",
    "# 허깅페이스 트렌스포머를 활용하면 서로 다른 조직에서 개발한 BERT와 GPT-2 모델을 거의 동인한 인터페이스로 활용 가능\n",
    "# AutoModel, AutoTokenizer 클래스를 사용해 BERT 및 GPT-2 모델과 토큰나이저를 불러오고 토큰화를 수행해서 모델에 입력으로 넣어준다\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "text = \"What is Huggingface Transformers?\"\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_input = bert_tokenizer(text, return_tensors='pt')\n",
    "bert_output = bert_model(**encoded_input)\n",
    "# GPT-2 모델 활용\n",
    "gpt_model = AutoModel.from_pretrained('gpt2')\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "encoded_input = gpt_tokenizer(text, return_tensors='pt')\n",
    "gpt_output = gpt_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6438e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 아이디로 모델 불러오기\n",
    "from transformers import AutoModel\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "366d15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 트랜스포머 라미이브러리\n",
    "# - 허깅페이스에는 모델을 바디(body), 헤드(head)로 구분, 같은 바디를 사용하면서 다른 작업에 사용 할 수 이도록 만들기 위함\n",
    "# 예) 구글 BERT 모델을 사용하지만 다른 헤드를 사용 할 수가 있다\n",
    "# - 문장 전체가 긍정인지 부정인지를 분류하는 모델, 이때는 바디가 반환하는 여러 잠재 상태(hidden state)중 가장 앞에 있는 [CLS] 토큰의 데이터만 받아 예측\n",
    "# - 각 토큰이 사람이나 장소에 해당하는지 판단하는 개처명 인식(named entity recognition) 모델인데, 각 토큰에 대해 판단해야 하기 때문에 모든 토튼의\n",
    "# - 데이터를 받아 각각 사람인지 장소인지를 예측\n",
    "\n",
    "# 분류 헤드가 포함된 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae1c4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 분류 헤드가 랜덤으로 초기화된 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f70777af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 불러오기\n",
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "887d9318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n",
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n",
      "토크나이저는 텍스트를 토큰 단위로 나눈다\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 사용하기\n",
    "tokenized = tokenizer(\"토크나이저는 텍스트를 토큰 단위로 나눈다\")\n",
    "print(tokenized)\n",
    "# {'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2],\n",
    "#  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "# ['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n",
    "\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "# [CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n",
    "\n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "# 토크나이저는 텍스트를 토큰 단위로 나눈다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0342ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저에 여러 문장 넣기\n",
    "tokenizer(['첫 번째 문장', '두 번째 문장'])\n",
    "\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ec1710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하나의 데이터에 여러 문장이 들어가는 경우, 한번더 리스트 사용하여 데이터를 묶는다\n",
    "tokenizer([['첫 번째 문장', '두 번째 문장']])\n",
    "\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbefd6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰 아이디를 문자열로 복원\n",
    "first_tokenized_result = tokenizer(['첫 번째 문장', '두 번째 문장'])['input_ids']\n",
    "tokenizer.batch_decode(first_tokenized_result)\n",
    "# ['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
    "\n",
    "second_tokenized_result = tokenizer([['첫 번째 문장', '두 번째 문장']])['input_ids']\n",
    "tokenizer.batch_decode(second_tokenized_result)\n",
    "# ['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a28794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 토크나이저와 RoBERTa 토크나이저\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "bert_tokenizer([['첫 번째 문장', '두 번째 문장']])\n",
    "# {'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "roberta_tokenizer([['첫 번째 문장', '두 번째 문장']])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]],\n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "en_roberta_tokenizer([['first sentence', 'second sentence']])\n",
    "# {'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ed47cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_mask 확인\n",
    "tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장 보다 더 길다.'], padding='longest')\n",
    "\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1],\n",
    "# [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 3632, 831, 647, 2062, 18, 2]],\n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
    "# [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf447c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE MRC 데이터셋 다운로드 코드\n",
    "from datasets import load_dataset\n",
    "# ds = load_dataset('klue', 'mrc', split='train')\n",
    "# print(ds.features)\n",
    "\n",
    " # 데이터셋 이름 klue, 서브셋 이름 mrc를 load_dataset 함수에 인자로 전달\n",
    "# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train') # split='train' 학습, split='validation' 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "751cc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬의 데이터 활용하기\n",
    "\n",
    "from datasets import load_dataset\n",
    "# 로컬의 데이터 파일을 활용\n",
    "# dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n",
    "\n",
    "# 파이썬 딕셔너리 활용\n",
    "from datasets import Dataset\n",
    "my_dict = {\"a\": [1, 2, 3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "# 판다스 데이터프레임 활용\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
