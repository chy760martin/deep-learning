{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer 대화형 챗봇(Dialogue Chatbot) 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터셋 로드 및 데이터 분리\n",
    "# - AI Hub 대화데이터: 한국어 SNS 멀티턴 대화 데이터\n",
    "# - 데이터 분리: train, validation\n",
    "# - 정상 파일 추출\n",
    "# - 2. JSON 파일 직접 파싱\n",
    "# - 3. 사전 토크나이징 및 저장\n",
    "# - 4. Dataset 클래스 정의\n",
    "# - 5. 사전 토크나이징 불러오기(이전대화/현재대화) 및 DataLoader 생성\n",
    "# - 6. 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "# - 최적화 설정: optimizer, GradScaler, autocast\n",
    "# - Early Stopping 클래스 정의\n",
    "# - 최적 모델 가중치 저장\n",
    "# - 7. 학습/검증 루프\n",
    "# - 딕셔너리 형태 학습데이터를 그대로 모델에 전달하는 코드로 정리, 코드가 깔끔하고 범용적으로 사용한다\n",
    "# - Early Stopping 객체 사용하여 적용\n",
    "# - AMP torch.float32 사용(메모리 사용 증가, torch.float16 사용시 loss가 너무 작아저 nan 발생)\n",
    "# - 8. 전체 평가 파이프라인\n",
    "# - F1/EM 평가\n",
    "# - 9. 모델 정의 및 최적화 모델 로드\n",
    "# - 10. 멀티 답변 생성\n",
    "# - 11. 문장 추론: Fast API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4539cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3308\n",
      "413\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드 \n",
    "# - AI Hub 대화데이터: 한국어 SNS 멀티턴 대화 데이터\n",
    "# - 데이터 분리: train, validation\n",
    "# - 정상 파일 추출\n",
    "import glob, random, json\n",
    "\n",
    "# 참고: AI Hub에서 제공하는 대화데이터의 JSON 구조가 불안하여 전체 로드시 파싱 에러 발생\n",
    "# - 해결책: JSON 파일 직접 파싱\n",
    "train_files = glob.glob('llm_data/ai_hub_dialogue_session2/train/*.json')\n",
    "valid_files = glob.glob('llm_data/ai_hub_dialogue_session2/validation/*.json')\n",
    "\n",
    "# 10% 샘플링\n",
    "random.seed(42)\n",
    "sample_train = random.sample(train_files, int(len(train_files) * 0.1))\n",
    "sample_valid = random.sample(valid_files, int(len(valid_files) * 0.1))\n",
    "\n",
    "def validate_file(file):\n",
    "    try:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # 최소한 sessionInfo와 dialog가 있어야 정상 파일로 간주\n",
    "        if 'sessionInfo' in data:\n",
    "            for session in data['sessionInfo']:\n",
    "                if 'dialog' in session:\n",
    "                    return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# 정상 파일만 추출\n",
    "valid_train_files = [ f for f in sample_train if validate_file(f) ]\n",
    "valid_valid_files = [ f for f in sample_valid if validate_file(f) ]\n",
    "\n",
    "print(len(valid_train_files))\n",
    "print(len(valid_valid_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2db885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파일 직접 파싱\n",
    "import json\n",
    "\n",
    "def load_aihub(files):\n",
    "    contexts, responses = [], []\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f) # JSON 파일 로드\n",
    "            for session in data.get('sessionInfo', []): # sessionInfo 리스트 로드\n",
    "                dialog = session.get('dialog', []) # dialog 리스트 로드\n",
    "                for i in range(1, len(dialog)): # dialog 리스트 갯수, i는 1부터 시작\n",
    "                    contexts.append(dialog[i-1].get('utterance', '')) # 이전 발화, 0번째 인덱스 데이터\n",
    "                    responses.append(dialog[i].get('utterance', '')) # 현재 발화, 1번째 인덱스 데이터\n",
    "        except Exception as e:\n",
    "            print(f'파일 {file} 처리 실패: {e}')\n",
    "    # return {'context': contexts, 'response': responses}\n",
    "    return contexts, responses # 리스트 반환\n",
    "\n",
    "# 예시: AIHub 데이터 로드\n",
    "train_contexts, train_responses = load_aihub(valid_train_files)\n",
    "valid_contexts, valid_responses = load_aihub(valid_valid_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4620320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pre-tokenized dataset at ./llm_models/21_transformer_dialogue_chatbot/train_data.pt\n",
      "Saved pre-tokenized dataset at ./llm_models/21_transformer_dialogue_chatbot/valid_data.pt\n"
     ]
    }
   ],
   "source": [
    "# 사전 토크나이징 및 저장\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# google/mt5-small 한국어 포함 다국어 처리가 가능한 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\", legacy=False)\n",
    "\n",
    "# 토크나이징 및 저장 함수\n",
    "# - contexts 대화 맥락 리스트, responses 응답 리스트, tokenizer 토크나이저, max_length 최대 토큰 길이 기본 64, save_path 저장 경로\n",
    "def preprocess_and_save(contexts, responses, tokenizer, max_length=64, save_path=\"./llm_models/21_transformer_dialogue_chatbot/train_data.pt\"):\n",
    "    input_ids_list, attention_masks_list, labels_list = [], [], []\n",
    "\n",
    "    for context, response in zip(contexts, responses):\n",
    "        # 토크나이징: context, response\n",
    "        inputs = tokenizer( # 결과 shape: (1, max_length) (1, 64)\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            response,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # lables에서 PAD 토큰을 -100으로 치환\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        input_ids_list.append(inputs[\"input_ids\"].squeeze(0)) # inputs[\"input_ids\"] (1,64) -> inputs[\"input_ids\"].squeeze(0) (64)\n",
    "        attention_masks_list.append(inputs[\"attention_mask\"].squeeze(0)) # inputs[\"attention_mask\"] (1,64) -> inputs[\"attention_mask\"].squeeze(0) (64)\n",
    "        labels_list.append(labels.squeeze(0)) # labels (1,64) -> labels.squeeze(0) (64)\n",
    "\n",
    "    # 학습에 필요한 모든 데이터를 포함한 딕셔너리 형태\n",
    "    dataset_tensors = {\n",
    "        \"input_ids\": torch.stack(input_ids_list), # shape (대화쌍이 1000개라면,64) (1000,64)\n",
    "        \"attention_mask\": torch.stack(attention_masks_list), # shape (대화쌍이 1000개라면,64) (1000,64)\n",
    "        \"labels\": torch.stack(labels_list) # shape (대화쌍이 1000개라면,64) (1000,64)\n",
    "    }\n",
    "    torch.save(dataset_tensors, save_path)\n",
    "    print(f\"Saved pre-tokenized dataset at {save_path}\")\n",
    "\n",
    "# 학습 전에 한 번만 실행\n",
    "preprocess_and_save(train_contexts, train_responses, tokenizer, save_path=\"./llm_models/21_transformer_dialogue_chatbot/train_data.pt\")\n",
    "preprocess_and_save(valid_contexts, valid_responses, tokenizer, save_path=\"./llm_models/21_transformer_dialogue_chatbot/valid_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf0e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 클래스 정의\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    # dataset_tensors 딕셔너리: \"input_ids\", \"attention_mask\", \"labels\" 세 가지 텐서를 포함한 구조\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    # \"input_ids\"의 첫번째 차원(샘플 개수)을 기준으로 길이를 계산\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"input_ids\"]) # (1000,64) -> 1000 길이 리턴\n",
    "    \n",
    "    # idx(정수 인덱스) 값에 맞는 샘플 하나를 반환하는 역할\n",
    "    # - 예시 1000 안에서의 인덱스 순번이다\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.data[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.data[\"attention_mask\"][idx],\n",
    "            \"labels\": self.data[\"labels\"][idx],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2975339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 토큰나이징 파일 불러오기 및 DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 사전 토큰나이징 파일 불러오기\n",
    "train_data = torch.load(\"./llm_models/21_transformer_dialogue_chatbot/train_data.pt\")\n",
    "valid_data = torch.load(\"./llm_models/21_transformer_dialogue_chatbot/valid_data.pt\")\n",
    "\n",
    "# Dataset 객체 생성\n",
    "train_dataset = MyDataset(train_data)\n",
    "valid_dataset = MyDataset(valid_data)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7b67e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320eb4524d4842c29b02e49e98e96850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mMT5ForConditionalGeneration LOAD REPORT\u001b[0m from: google/mt5-small\n",
      "Key                         | Status  | \n",
      "----------------------------+---------+-\n",
      "shared.weight               | MISSING | \n",
      "encoder.embed_tokens.weight | MISSING | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.7.1+cu118, Device: cuda\n",
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MT5ForConditionalGeneration(\n",
      "      (shared): Embedding(250112, 512)\n",
      "      (encoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "# - 모델 본체 동결 처리: Feature Extraction\n",
    "# - LoRA 파인튜닝 적용\n",
    "# - Early Stopping 적용\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "\n",
    "# - 구글의 mT5-small 모델: 다국어 Seq2Seq 언어모델로, 입력(context)을 받아 응답(response)을 생성하는 데 적합하다\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small', tie_word_embeddings=False, use_safetensors=True)\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "# 모델 전체를 GPU/CPU 디바이스 메모리로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 모델 본체 동결 처리: Feature Extraction\n",
    "# - 원래 mT5 모델의 모든 파라미터를 **동결(freeze)** 처리, 즉 기존 모델은 그대로 두고 학습하지 않는다\n",
    "# - Feature Extractor로만 사용되고, 추가 모듈인 LoRA 모듈만 학습하는 구조가 된다\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# LoRA 적용: LoRA 모듈만 학습되도록 설정(경량 파인튜닝)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # 작은 rank(r=8)로 효율적인 파인튜닝 가능\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM' # 대화 응답 생성은 Seq2Seq LM\n",
    ")\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# Automatic Mixed Precision(AMP) 학습을 위한 GradScaler 준비\n",
    "# Automatic Mixed Precision(AMP)은 모델 파라미터는 FP32로 유지하면서 연산(곱셈·덧셈 등)만 FP16으로 자동 전환하여, \n",
    "# 정밀도 손실 없이 메모리와 연산 효율을 극대화하는 기술\n",
    "scaler = GradScaler()\n",
    "num_epochs = 3\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/21_transformer_dialogue_chatbot/best_model.pt'):\n",
    "        self.patience=patience\n",
    "        self.min_delta=min_delta\n",
    "        self.best_loss=None\n",
    "        self.counter=0\n",
    "        self.early_stop=False\n",
    "        self.path=path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        # 최조 손실값에 해당하는 모델 가중치 저장\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss=valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        \n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss=valid_loss\n",
    "            self.counter=0\n",
    "            self.save_checkpoint(model)\n",
    "        \n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop=True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !='' and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        # 모델 가중치 저장\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f' Best model saved at {self.path}')\n",
    "\n",
    "# Early Stopping 객체 생성\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# 모델 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a5035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 학습/검증 루프\n",
    "# - 딕셔너리 형태 학습데이터를 그대로 모델에 전달하는 코드로 정리, 코드가 깔끔하고 범용적으로 사용한다\n",
    "# - Early Stopping 객체 사용하여 적용\n",
    "# - AMP torch.float32 사용(메모리 사용 증가, torch.float16 사용시 loss가 너무 작아저 nan 발생)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Train Loop\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]'):\n",
    "        batch = { k: v.to(device) for k, v in batch.items() } # 딕셔너리 형태로 생성, 학습데이터 GPU 지정\n",
    "        # input_ids, attention_mask, labels = [x.to(device) for x in batch] # 리스트 언패킹\n",
    "\n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        # with autocast(device_type='cuda', dtype=torch.float16):\n",
    "        \n",
    "        # RTX 30 시리즈 GPU는 BF16을 지원하기 때문에, 안정성과 속도를 동시에 확보할 수 있다\n",
    "        with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "            outputs = model(**batch) # 딕셔너리 형태 학습데이터를 그대로 모델에 전달, 코드가 깔끔하고 범용적으로 사용한다\n",
    "            loss = outputs.loss # 손실값\n",
    "        scaler.scale(loss).backward() # 미분 연산\n",
    "        scaler.step(optimizer) # 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "        scaler.update()\n",
    "\n",
    "        # with autocast(device_type='cuda', enabled=False):\n",
    "        #     outputs = model(**batch)\n",
    "        #     # outputs = model(input_ids=input_ids,\n",
    "        #     #         attention_mask=attention_mask,\n",
    "        #     #         labels=labels)\n",
    "\n",
    "        #     loss = outputs.loss\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() # 손실 누적\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval() # 검증/추론 모드 지정\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'Epoch {epoch+1} [Valid]'):\n",
    "            batch = { k: v.to(device) for k, v in batch.items() }\n",
    "            # input_ids, attention_mask, labels = [x.to(device) for x in batch] # 리스트 언패킹\n",
    "\n",
    "            # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "            # with autocast(device_type='cuda', dtype=torch.float16):\n",
    "\n",
    "            # RTX 30 시리즈 GPU는 BF16을 지원하기 때문에, 안정성과 속도를 동시에 확보할 수 있다\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                outputs = model(**batch) # 검증 모델 예측\n",
    "                loss = outputs.loss # 검증 손실값\n",
    "\n",
    "            # with autocast(device_type='cuda', enabled=False):\n",
    "            #     outputs = model(**batch)\n",
    "            #     # outputs = model(input_ids=input_ids,\n",
    "            #     #     attention_mask=attention_mask,\n",
    "            #     #     labels=labels)\n",
    "\n",
    "            #     loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item() # 검증 손실값 누적    \n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    print(f'Epoch {epoch+1}, Valid Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(valid_loss=avg_val_loss, model=model)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping triggered.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평가 파이프라인: F1/EM 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 및 최적화 모델 로드\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "# 같은 구조의 모델 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small', legacy=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small') # MT5 모델 불러온다\n",
    "\n",
    "# 모델 본체 동결 처리: Feature Extraction\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "        \n",
    "# LoRA 적용: LoRA 모듈만 학습되도록 설정(경량 파인튜닝)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # 작은 rank(r=8)로 효율적인 파인튜닝 가능\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    # target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM' # 대화 응답 생성은 Seq2Seq LM\n",
    ")\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 저장된 state_dict 불러오기\n",
    "model.load_state_dict(torch.load('./llm_models/21_transformer_dialogue_chatbot/best_model.pt'))\n",
    "\n",
    "# 추론/검증 모드 적용\n",
    "model.eval()\n",
    "\n",
    "# 모델 전체를 GPU/CPU 디바이스 메모리로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 모델 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f640e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 답변 생성\n",
    "\n",
    "import re\n",
    "\n",
    "# context = \"서울에서 유명한 관광지 알려줄래?\"\n",
    "context = \"서울의 유명 관광지는 경복궁, 남산타워, 명동입니다.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    context,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# Beam search + Sampling 혼합\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=128,\n",
    "    min_length=8, # 최소 길이 강제\n",
    "\n",
    "    num_beams=5, # 5~7 정도가 대화형 모델에서는 가장 균형이 좋다\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.85, # 0.9 → 0.6로 줄여서 불필요한 변형을 줄인다, 변형 단어 출력 감소\n",
    "    temperature=0.8, # 0.7 → 0.5 정도로 낮추면 더 안정적인 답변을 얻을 수 있다\n",
    "    num_return_sequences=3,\n",
    "\n",
    "    repetition_penalty=2.0, # 반복 억제\n",
    "    no_repeat_ngram_size=4, # 4-gram 반복 금지\n",
    "    length_penalty=1.2 # 1.2~2.0 정도로 설정하면, 모델이 불필요하게 길게 반복하는 걸 줄인다\n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # <extra_id_n> 토큰 제거\n",
    "    # decoded = re.sub(r'<extra_id_\\d+>', '', decoded).strip()\n",
    "    decoded = decoded.replace('..', '.').replace('??', '?')\n",
    "    decoded = re.sub(r'북산타워|서산타워|여산타워|한산타워', '남산타워', decoded)\n",
    "    # decoded = re.sub(r'^,?\\s*의 유명 관광지', '서울의 유명 관광지는', decoded)\n",
    "    # decoded = re.sub(r'(국립공원|남산타워)(,\\s*\\1)+', r'\\1', decoded)  # 중복 제거\n",
    "    # decoded = re.sub(r'궁금한 관광지입니다', '관광 명소로 잘 알려져 있습니다', decoded)\n",
    "\n",
    "    print(f'답변 {i+1}: {decoded}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
