{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765acad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face 라이브러리 적용 - 기계 번역 모델 성능 확인\n",
    "# AI HUB 관광지 소개 다국어 번역 데이터셋 적용\n",
    "\n",
    "# Cell 1\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import autocast\n",
    "from torch.cuda.amp import GradScaler\n",
    "import evaluate   # ✅ 최신 BLEU 평가 라이브러리\n",
    "import json, glob\n",
    "import random\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"t5-small\"   # RTX 3060 6GB 환경에 적합한 경량 모델\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# BLEU metric 로드\n",
    "bleu = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d6ac65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "# 폴더명: TL_영어_관광지,TL_영어_레포츠... -> 도메인/카테고리 구분\n",
    "# 파일명:en_2_0_gg_0_(평화누리길김포시첫째길)대명항-문수산성_2.json\n",
    "# 내용: 각 JSON 안에는 annotations 배열이 있고, k_context(한국어) ↔ t_context(영어) 페어\n",
    "\n",
    "ko_lines, en_lines = [], []\n",
    "folders = [ # 폴더 리스트 정의\n",
    "    './llm_data/ai_hub_tourist_accommodation_english/*.json',\n",
    "    './llm_data/ai_hub_tourist_attraction_english/*.json',\n",
    "    './llm_data/ai_hub_tourist_cultural_facility_english/*.json',\n",
    "    './llm_data/ai_hub_tourist_entertainment_english/*.json',\n",
    "    './llm_data/ai_hub_tourist_leisure_sports_english/*.json',\n",
    "    './llm_data/ai_hub_tourist_restaurant_english/*.json',\n",
    "    './llm_data/ai_hub_tourist_shopping_english/*.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aebc9d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentence pairs after cleaning: 2524283 vs 2524283\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 전처리\n",
    "# 폴더명: TL_영어_관광지,TL_영어_레포츠... -> 도메인/카테고리 구분\n",
    "# 파일명:en_2_0_gg_0_(평화누리길김포시첫째길)대명항-문수산성_2.json\n",
    "# 내용: 각 JSON 안에는 annotations 배열이 있고, k_context(한국어) ↔ t_context(영어) 페어\n",
    "\n",
    "# 모든 JSON 읽기\n",
    "for folder in folders:\n",
    "    for path in glob.glob(folder):\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            js = json.load(f)\n",
    "\n",
    "        for ann in js.get('annotations', []):\n",
    "            if ann.get('language') == 'en':\n",
    "                ko = ann.get('k_context')\n",
    "                en = ann.get('t_context')\n",
    "\n",
    "                if ko and en:\n",
    "                    ko_split = [line.strip() for line in ko.splitlines() if line.strip()]\n",
    "                    en_split = [line.strip() for line in en.splitlines() if line.strip()]\n",
    "\n",
    "                    # 줄 수가 다르면 최소 길이에 맞춰 강제 매칭\n",
    "                    min_len = min(len(ko_split), len(en_split))\n",
    "                    ko_lines.extend(ko_split[:min_len])\n",
    "                    en_lines.extend(en_split[:min_len])\n",
    "\n",
    "print(f\"Total sentence pairs after cleaning: {len(ko_lines)} vs {len(en_lines)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e54ac8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료: ./llm_data/ai_hub_tourist_ko_en/origin_train_ko.txt ./llm_data/ai_hub_tourist_ko_en/origin_train_en.txt\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 저장\n",
    "\n",
    "# 최종 저장 단계에서도 안전하게 min_len 맞추기\n",
    "min_len = min(len(ko_lines), len(en_lines))\n",
    "ko_lines = ko_lines[:min_len]\n",
    "en_lines = en_lines[:min_len]\n",
    "\n",
    "# 샘플링 (데이터셋 줄이기)\n",
    "sample_size = 50000   # 원하는 크기 (예: 50k 문장)\n",
    "\n",
    "ko_lines = ko_lines[:sample_size]\n",
    "en_lines = en_lines[:sample_size]\n",
    "\n",
    "out_dir = './llm_data/ai_hub_tourist_ko_en'\n",
    "ko_path = f'{out_dir}/origin_train_ko.txt'\n",
    "en_path = f'{out_dir}/origin_train_en.txt'\n",
    "\n",
    "assert len(ko_lines) == len(en_lines), \"KO/EN line count mismatch!\"\n",
    "\n",
    "with open(ko_path, 'w', encoding='utf-8') as fko, \\\n",
    "     open(en_path, 'w', encoding='utf-8') as fen:\n",
    "    for k, e in zip(ko_lines, en_lines):\n",
    "        fko.write(k.strip() + '\\n')\n",
    "        fen.write(e.strip() + '\\n')\n",
    "print('저장 완료:', ko_path, en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83af15f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts: 50000 50000\n",
      "pairs: 50000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "# - 전체 병렬 데이터 로드: train_ko.txt, train_en.txt를 읽어서 리스트로 만든다\n",
    "# - 랜덤 셔플: 순서를 섞어 데이터 누수를 방지\n",
    "# - 비율 분할: 8:1:1 = train:validation:test\n",
    "# - 각 파일 저장: train_ko.txt, valid_ko.txt, test_ko.txt와 대응하는 영어 파일을 저장\n",
    "\n",
    "# 파일 읽기 (경로 수정)\n",
    "with open('./llm_data/ai_hub_tourist_ko_en/origin_train_ko.txt', encoding='utf-8') as fko, \\\n",
    "     open('./llm_data/ai_hub_tourist_ko_en/origin_train_en.txt', encoding='utf-8') as fen:\n",
    "    ko_lines = [line.strip() for line in fko if line.strip()]\n",
    "    en_lines = [line.strip() for line in fen if line.strip()]\n",
    "\n",
    "# 길이 검증\n",
    "assert len(ko_lines) == len(en_lines), f\"KO/EN mismatch: {len(ko_lines)} vs {len(en_lines)}\"\n",
    "print(\"counts:\", len(ko_lines), len(en_lines))\n",
    "\n",
    "# 병렬 데이터 묶기\n",
    "pairs = list(zip(ko_lines, en_lines))\n",
    "print(\"pairs:\", len(pairs))\n",
    "\n",
    "# 랜덤 셔플\n",
    "random.seed(42) # 재현성 위해 고정\n",
    "random.shuffle(pairs)\n",
    "\n",
    "# 분할 비율\n",
    "n = len(pairs)\n",
    "train_end = int(n * 0.8)\n",
    "valid_end = int(n * 0.9)\n",
    "\n",
    "train_pairs = pairs[:train_end] # 0% ~ 80%\n",
    "valid_pairs = pairs[train_end:valid_end] # 80% ~ 90%\n",
    "test_pairs = pairs[valid_end:] # 90% ~ 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e9f909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 분할 및 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# 분할 데이터 저장\n",
    "\n",
    "# 저장 함수\n",
    "def save_pairs(pairs, ko_path, en_path):\n",
    "    with open(ko_path, 'w', encoding='utf-8') as fko, \\\n",
    "        open(en_path, 'w', encoding='utf-8') as fen:\n",
    "        for k, e in pairs:\n",
    "            fko.write(k + '\\n')\n",
    "            fen.write(e + '\\n')\n",
    "\n",
    "# 저장 실행\n",
    "out_dir = './llm_data/ai_hub_tourist_ko_en'\n",
    "save_pairs(train_pairs, f'{out_dir}/train_ko.txt', f'{out_dir}/train_en.txt')\n",
    "save_pairs(valid_pairs, f'{out_dir}/valid_ko.txt', f'{out_dir}/valid_en.txt')\n",
    "save_pairs(test_pairs, f'{out_dir}/test_ko.txt', f'{out_dir}/test_en.txt')\n",
    "\n",
    "print('데이터셋 분할 및 저장 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e7cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file, tokenizer, max_len=64):\n",
    "        with open(src_file, encoding=\"utf-8\") as fsrc, \\\n",
    "             open(tgt_file, encoding=\"utf-8\") as ftgt:\n",
    "            self.src_lines = [line.strip() for line in fsrc if line.strip()]\n",
    "            self.tgt_lines = [line.strip() for line in ftgt if line.strip()]\n",
    "        assert len(self.src_lines) == len(self.tgt_lines), \"Source/Target mismatch!\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src_lines[idx]\n",
    "        tgt = self.tgt_lines[idx]\n",
    "\n",
    "        src_enc = self.tokenizer(src,\n",
    "                                 max_length=self.max_len,\n",
    "                                 padding=\"max_length\",\n",
    "                                 truncation=True,\n",
    "                                 return_tensors=\"pt\")\n",
    "        tgt_enc = self.tokenizer(tgt,\n",
    "                                 max_length=self.max_len,\n",
    "                                 padding=\"max_length\",\n",
    "                                 truncation=True,\n",
    "                                 return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": src_enc[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": src_enc[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": tgt_enc[\"input_ids\"].squeeze(),\n",
    "            \"target_text\": tgt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e73a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_7216\\743587841.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Cell 3\n",
    "train_dataset = TranslationDataset(\"./llm_data/ai_hub_tourist_ko_en/train_ko.txt\",\n",
    "                                   \"./llm_data/ai_hub_tourist_ko_en/train_en.txt\",\n",
    "                                   tokenizer)\n",
    "valid_dataset = TranslationDataset(\"./llm_data/ai_hub_tourist_ko_en/valid_ko.txt\",\n",
    "                                   \"./llm_data/ai_hub_tourist_ko_en/valid_en.txt\",\n",
    "                                   tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e445d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 0.2209 | Val Loss: 0.1190 | BLEU: 67.52\n",
      "Epoch 2/5 | Train Loss: 0.1286 | Val Loss: 0.1055 | BLEU: 67.28\n",
      "Epoch 3/5 | Train Loss: 0.1148 | Val Loss: 0.0987 | BLEU: 68.69\n",
      "Epoch 4/5 | Train Loss: 0.1064 | Val Loss: 0.0938 | BLEU: 69.23\n",
      "Epoch 5/5 | Train Loss: 0.1002 | Val Loss: 0.0909 | BLEU: 68.63\n"
     ]
    }
   ],
   "source": [
    "# Cell 4\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(\"cuda\"):\n",
    "            outputs = model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # 검증 루프 + BLEU 평가\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    predictions, references = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                labels=labels)\n",
    "                loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            generated_ids = model.generate(input_ids=input_ids,\n",
    "                                           attention_mask=attention_mask,\n",
    "                                           max_length=64)\n",
    "            preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "            refs = [[batch[\"target_text\"][i]] for i in range(len(preds))]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            references.extend(refs)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "          f\"BLEU: {bleu_score['score']:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da7be1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./results_translation_huggingface_t5-small/checkpoints/epoch_5\\\\tokenizer_config.json',\n",
       " './results_translation_huggingface_t5-small/checkpoints/epoch_5\\\\special_tokens_map.json',\n",
       " './results_translation_huggingface_t5-small/checkpoints/epoch_5\\\\spiece.model',\n",
       " './results_translation_huggingface_t5-small/checkpoints/epoch_5\\\\added_tokens.json',\n",
       " './results_translation_huggingface_t5-small/checkpoints/epoch_5\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장\n",
    "model.save_pretrained(f\"./results_translation_huggingface_t5-small/checkpoints/epoch_{epoch+1}\")\n",
    "tokenizer.save_pretrained(f\"./results_translation_huggingface_t5-small/checkpoints/epoch_{epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7bc227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.7.1+cu118, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "model_path = './results_translation_huggingface_t5-small/checkpoints/epoch_5'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88dc4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "번역 결과: Gapyeong-ro, Gapyeong-eup, Gapyeong-gun, Gyeonggi-do\n"
     ]
    }
   ],
   "source": [
    "# 추론(Inference)\n",
    "\n",
    "# 한국어 입력 문장\n",
    "# input_text = \"이 호텔은 서울 중심에 위치해 있습니다.\"\n",
    "# input_text = \"이 식당은 전통 한식을 제공합니다.\"\n",
    "# input_text = '이 호텔은 깨끗합니다'\n",
    "input_text = '이 박물관은 무료 입장이 가능합니다'\n",
    "\n",
    "# 토크나이즈\n",
    "inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "\n",
    "# 번역\n",
    "# generated_ids = model.generate(**inputs, max_length=64, num_beams=5)\n",
    "generated_ids = model.generate(**inputs, max_length=64, do_sample=True, top_k=50, top_p=0.95)\n",
    "\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print('번역 결과:', output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
