{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ec65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer Sentiment Classifier 감정 분류 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터 로드 & 확인: 결측치 제거(None, \"\")\n",
    "# - 2. 토크나이저 적용: Hugging Face DistilBertTokenizer 베이스 모델 사용\n",
    "# - 3. 데이터셋 -> DataLoader 변환: DistilBertTokenizer 베이스 모델 토크나이저에서 DataLoader 로 바로 변환, Custom Dataset 필요 없음\n",
    "# - 4. 모델정의 & GPU설정 & 전이학습 & 본체 동결\n",
    "# - 전이 학습: DistilBertForSequenceClassification 베이스 모델(distilbert-base-uncased), num_labels=2 긍정/부정 2개 클래스\n",
    "# - 본체 동결: model.distilbert.parameters()는 사전학습된 본체(embedding + transformer 블록)의 모든 파라미터를 의미,\n",
    "# - 따라서 학습은 classifier 레이어(pre_classifier, classifier)만 진행된다\n",
    "# - 5. 최적화 설정 & 학습 루프\n",
    "# - 최적화 설정: autocast(속도 향상) GradScaler(안정적 학습) 적용\n",
    "\n",
    "# DistilBert 구조 특징 \n",
    "# - BERT 계열 모델은 Transformer의 Encoder 부분만 사용한다\n",
    "# - 입력 문장을 임베딩 -> 여러층의 Transformer Encoder 블록 -> [CLS]토큰 벡터 추출 -> 분류기(Classifier)\n",
    "\n",
    "# DistilBertForSequenceClassification 내부 흐름\n",
    "# 입력(DataLoader 배치): \n",
    "# - input_ids:(batch_size,seq_len)->(32,128)\n",
    "# - attention_mask: (32,128)\n",
    "# - labels:(32)\n",
    "# DistilBertForSequenceClassification(\n",
    "#   (distilbert): DistilBertModel(\n",
    "#     (embeddings): Embeddings(\n",
    "#       (word_embeddings): Embedding(30522, 768, padding_idx=0) : (32,128) -> (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#       (position_embeddings): Embedding(512, 768) : (32,128) -> (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#       (dropout): Dropout(p=0.1, inplace=False) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#     )\n",
    "#     (transformer): Transformer(\n",
    "#       (layer): ModuleList(\n",
    "#         (0-5): 6 x TransformerBlock( : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#           (attention): DistilBertSdpaAttention(\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#           )\n",
    "#           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#           (ffn): FFN( : (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "#             (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "#             (activation): GELUActivation()\n",
    "#           )\n",
    "#           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#         )\n",
    "#       )\n",
    "#     ) Transformer 출력(output_layer_norm 이후) (32,128,768) (batch_size,seq_len,hidden_dim) shape을 가지며\n",
    "#       내부 forward()에서 hidden_states[:, 0, :] 선택([CSL] 벡터를 추출) -> (32,768) (batch_size,hidden_dim) 변경 한다\n",
    "#   ) \n",
    "#   (pre_classifier): Linear(in_features=768, out_features=768, bias=True) : (32,768) -> (32,768) (batch_size,hidden_dim)\n",
    "#   (classifier): Linear(in_features=768, out_features=2, bias=True) : (32,768)@(768,2) -> (32,2) 입력 벡터와 가중치 행렬의 내적 계산\n",
    "#   (dropout): Dropout(p=0.2, inplace=False) : (32,768) (batch_size,hidden_dim) 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909c366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 & 확인\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face datasets 라이브러리 함수(CSV,JSON.. 데이터 로드)\n",
    "dataset = load_dataset( # DatasetDict 형태로 반환\n",
    "    'csv', # csv 포멧 지정\n",
    "    data_files={ # train/test 데이터 각각 지정\n",
    "        'train': \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
    "        'test': \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    },\n",
    "    delimiter='\\t' # 구분자, 현재 데이터에서는 탭으로 구분\n",
    ")\n",
    "\n",
    "# 결측치 제거\n",
    "all_clean_train = dataset[\"train\"].filter(lambda x: x[\"document\"] is not None and x[\"document\"].strip() != \"\")\n",
    "all_clean_test = dataset[\"test\"].filter(lambda x: x[\"document\"] is not None and x[\"document\"].strip() != \"\")\n",
    "\n",
    "# 데이터 축소\n",
    "clean_train = all_clean_train.select(range(10000))\n",
    "clean_test = all_clean_test.select(range(5000))\n",
    "\n",
    "print(clean_train)\n",
    "print(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959e7d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9976970, 'label': 0, 'input_ids': [101, 1463, 30006, 1457, 30008, 29996, 30019, 30025, 1012, 1012, 100, 100, 1459, 30011, 30020, 29997, 30011, 29994, 30019, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'id': 6270596, 'label': 1, 'input_ids': [101, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 적용\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenizer_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"document\"],   # 문자열 리스트만 들어옴\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128 # 시퀀스 길이, 연산량 절반 이상 감소\n",
    "    )\n",
    "\n",
    "tokenized_train = clean_train.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")\n",
    "\n",
    "tokenized_test = clean_test.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")\n",
    "\n",
    "print(tokenized_train[0])\n",
    "print(tokenized_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7af3042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([32, 128]) torch.Size([32, 128]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 -> DataLoader 변환\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# collate_fn 함수 : Hugging Face Dataset에서 꺼낸 샘플은 파이썬 dict 형태, collate_fn() 각 샘플을 모아 PyTorch 텐서로 변환\n",
    "def collate_fn(batch): # batch 샘플 리스트 : [{'input_ids':[...],'attention_mask':[...],'label':[...]}, {...}, ...]\n",
    "    input_ids = torch.tensor([ item['input_ids'] for item in batch ]) # 토큰화된 문장\n",
    "    attention_mask = torch.tensor([ item['attention_mask'] for item in batch ]) # 패딩 여부\n",
    "    labels = torch.tensor([ item['label'] for item in batch ]) # 감성 분류 라벨(0=부정/1=긍정)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# train: 학습용 데이터, 배치크기 16, epoch 마다 데이터 순서 섞음\n",
    "train_loader = DataLoader(tokenized_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# valid: 검증용 데이터, 학습 데이터에서 10%를 검증용으로 분리 Hugging Face에서 제공하는 train_test_split() 메서드\n",
    "valid_loader = DataLoader(tokenized_train.train_test_split(test_size=0.1)['test'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# test: 테스트 데이터, 성능 최종 평가용\n",
    "test_loader = DataLoader(tokenized_test, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# train_loader 데이터 확인\n",
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07545c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.amp import autocast # 최신 API\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# DistilBertForSequenceClassification 베이스 모델(distilbert-base-uncased), num_labels=2 긍정/부정 2개 클래스\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# DistilBERT 본체 동결(Feature Extraction, Parameter Freezing, Weight Freezing)\n",
    "# - model.distilbert.parameters()는 사전학습된 본체(embedding + transformer 블록)의 모든 파라미터를 의미 한다\n",
    "# - requires_grad = False로 설정하면 역전파 시 이 파라미터들은 업데이트되지 않는다\n",
    "# - 따라서 학습은 classifier 레이어(pre_classifier, classifier)만 진행된다\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 모델에 GPU 설정\n",
    "model.to(device)\n",
    "\n",
    "# 모델 확인\n",
    "print(model)\n",
    "\n",
    "# 최적화 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 반복횟수\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:40<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Tran Loss: 0.6838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 313/313 [00:40<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Tran Loss: 0.6724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 313/313 [00:40<00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Tran Loss: 0.6678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프: autocast(속도 향상) 적용, GradScaler(안정적 학습) 적용\n",
    "# autocast 적용: 연산을 FP16(half precision)과 FP32(full precision)중 적절히 선택해서 실행\n",
    "# - 속도 향상: 대부분의 연산을 FP16으로 처리해 GPU 연산 속도를 높인다\n",
    "# - 안정성 유지: 손실이 큰 연산(예시:소프트맥스,레이어정규화)은 FP32로 자동 변환해 정확도를 보장한다\n",
    "# GradScaler 적용: FP16 학습에서는 작은 값이 underflow(0으로 사라짐)될 위험이 있다\n",
    "# - 안정적 학습 보장: GradScaler는 손실(loss)를 크게 스케일링해서 역전파 시 그래디언트가 사라지지 않도록 한다\n",
    "# - 이후 업데이트 단계에서 다시 원래 크기로 되돌려 안정적인 학습을 보장한다. 즉 FP16 학습에서 발생할 수 있는 수치 불안정 문제를 해결하는 역할\n",
    "from tqdm import tqdm # 시각화(진행바)\n",
    "\n",
    "# DistilBert 구조 특징 \n",
    "# - BERT 계열 모델은 Transformer의 Encoder 부분만 사용한다\n",
    "# - 입력 문장을 임베딩 -> 여러층의 Transformer Encoder 블록 -> [CLS]토큰 벡터 추출 -> 분류기(Classifier)\n",
    "# DistilBertForSequenceClassification 내부 흐름\n",
    "# 1. 입력: (batch_size,seq_len)\n",
    "# 2. Encoder(DistilBERT 본체) \n",
    "# - Embedding: (batch_size,seq_len,hidden_dim) \n",
    "# - Transformer Block(6개): (batch_size,seq_len,hidden_dim)\n",
    "# 3. [CLS] 토큰 추출: (batch_size,hidden_dim)\n",
    "# 4. Classifier 레이어: (batch_size,num_labels)\n",
    "# 5. 출력: logits, loss\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):        \n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "\n",
    "        # 학습데이터 GPU 지정\n",
    "        input_ids = batch['input_ids'].to(device) # [32, 128]\n",
    "        attention_mask = batch['attention_mask'].to(device) # [32, 128]\n",
    "        labels = batch['labels'].to(device) # [32]\n",
    "\n",
    "        # 모델 forward(autocast 영역) 내부 shape 변환\n",
    "        # - 임베딩 레이어: (32,128) -> (32,128,768) \n",
    "        # - Transformer 블록: (32,128,768) -> (32,128,768)\n",
    "        # - CLS 토큰 추출: (32,768) \n",
    "        # - Classifier 레이어: (32,768) -> (32,2) \n",
    "        # - outputs.logits.shape: (32,2) \n",
    "        # - outputs.loss: scalar\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # 손실함수 적재\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch + 1} | Tran Loss: {total_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a155b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 루프\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train() # 학습 모드 지정\n",
    "#     total_loss = 0\n",
    "#     for batch in train_loader:        \n",
    "#         optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "\n",
    "#         # 학습데이터 GPU 지정\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['label'].to(device)\n",
    "\n",
    "#         # 모델 예측\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "#         # 손실함수\n",
    "#         loss = outputs.loss\n",
    "\n",
    "#         loss.backward() # 오차역전파 코드, 미분 연산\n",
    "#         optimizer.step() # 오차역전파 코드, 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "\n",
    "#         # 손실함수 적재\n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     print(f'Epoch {epoch + 1} | Tran Loss: {total_loss / len(train_loader):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
