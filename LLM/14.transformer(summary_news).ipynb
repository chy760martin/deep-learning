{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 요약 모델 & 파인 튜닝\n",
    "# - Hugging Face 라이브러리 적용\n",
    "# - AI HUB 요약문 및 레포트 뉴스(news) 데이터셋 적용\n",
    "# - 입력된 문장을 요약 모델을 통한 문장 요약\n",
    "# 1. 학습 목표\n",
    "# - 구조 최적화 및 파이프라인 단순화\n",
    "# - AI HUB 요약문 및 레포트 뉴스(news) 데이터셋 전처리\n",
    "# - 병렬 문장쌍 데이터셋 변환 전처리\n",
    "# - 토크나이징 및 토크나이징 전처리\n",
    "# - 베이스 모델 로드\n",
    "# - LoRA(Low-Rank Adaptation) 설정, 특정 레이어에 작은 저차원 행렬(랭크 r)을 삽입해서 학습\n",
    "# - LoRA(Low-Rank Adaptation) 모델, 메모리 효율성/빠른 학습/도메인 적용, base 모델에 여러 LoRA 모듈을 붙였다 떼었다 할 수 있음\n",
    "# - 학습 args 설정\n",
    "# - Trainer 정의\n",
    "# - Trainer 실행\n",
    "# - LoRA 적용된 모델 저장, LoRA모델/토크나이저\n",
    "# - LoRA 적용된 모델 불러오기, 베이스모델/LoRA모델/토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob, json, re, os, random, csv\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__, device)\n",
    "\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"빌드 정보:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49046719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 - AI HUB 요약문 및 레포트 뉴스(news) 데이터셋 적용\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# 기본 경로\n",
    "base_dir = './llm_data/ai_hub_summary_news_r'\n",
    "\n",
    "def parse_json_folder(input_dir, summary_type):\n",
    "    rows = []\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if not file_name.endswith('.json'):\n",
    "            continue\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        passage = data['Meta(Refine)']['passage']\n",
    "        doc_type = data['Meta(Acqusition)']['doc_type']\n",
    "\n",
    "        if summary_type == 'short':\n",
    "            for key in ['summary1', 'summary2']:\n",
    "                summary = data['Annotation'].get(key)\n",
    "                if summary:\n",
    "                    rows.append({\n",
    "                        'src': passage,\n",
    "                        'tgt': summary,\n",
    "                        'summary_type': 'short',\n",
    "                        'domain': doc_type\n",
    "                    })\n",
    "        elif summary_type == 'long':\n",
    "            summary3 = data['Annotation'].get('summary3')\n",
    "            if summary3:\n",
    "                rows.append({\n",
    "                    'src': passage,\n",
    "                    'tgt': summary3,\n",
    "                    'summary_type': 'long',\n",
    "                    'domain': doc_type\n",
    "                })\n",
    "    # 판다스 데이터프레이으로 생성 후 리턴\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 학습 데이터 로딩\n",
    "train_short = parse_json_folder(os.path.join(base_dir, 'train/2-3sent'), 'short')\n",
    "train_long = parse_json_folder(os.path.join(base_dir, 'train/20per'), 'long')\n",
    "\n",
    "# 검증 데이터 로딩\n",
    "valid_short = parse_json_folder(os.path.join(base_dir, 'valid/2-3sent'), 'short')\n",
    "valid_long = parse_json_folder(os.path.join(base_dir, 'valid/20per'), 'long')\n",
    "\n",
    "# 확인\n",
    "print('train short:', train_short)\n",
    "print('train long:', train_long)\n",
    "print('valid short:', valid_short)\n",
    "print('valid long:', valid_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c40915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 - AI HUB 요약문 및 레포트 뉴스(news) 데이터셋 적용\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 기본 경로\n",
    "base_dir = './llm_data/ai_hub_summary_news_r'\n",
    "\n",
    "# 텍스트 클리닝 함수\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # URL 제거 (대소문자 구분 없이, http/https 단독 포함)\n",
    "    text = re.sub(r\"https?://[^\\s]+\", \"\", text, flags=re.IGNORECASE)  # 전체 URL 제거\n",
    "    text = re.sub(r\"\\bhttps?\\b\", \"\", text, flags=re.IGNORECASE)       # http/https 단독 제거\n",
    "    text = re.sub(r\"\\bHTTP\\b\", \"\", text, flags=re.IGNORECASE)         # HTTP 단독 제거\n",
    "    text = re.sub(r\"\\bHTTPS\\b\", \"\", text, flags=re.IGNORECASE)        # HTTPS 단독 제거\n",
    "\n",
    "    # 날짜 제거 (YYYY-MM-DD, YYYY년 MM월 DD일, YYYY.MM.DD)\n",
    "    text = re.sub(r\"\\d{4}년 \\d{1,2}월 \\d{1,2}일.*\", \"\", text)\n",
    "    text = re.sub(r\"\\d{4}-\\d{2}-\\d{2}.*\", \"\", text)\n",
    "    text = re.sub(r\"\\d{4}\\.\\d{1,2}\\.\\d{1,2}\", \"\", text)\n",
    "\n",
    "    # 기자명 제거\n",
    "    text = re.sub(r\"\\S*기자\", \"\", text)\n",
    "\n",
    "    # 기사 특수기호 제거\n",
    "    text = re.sub(r\"[▲▶◆■□◇※△○]\", \"\", text)\n",
    "\n",
    "    # 불필요한 공백 제거\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def parse_json_folder(input_dir, summary_type):\n",
    "    rows = []\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if not file_name.endswith('.json'):\n",
    "            continue\n",
    "        file_path = os.path.join(input_dir, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        passage = clean_text(data['Meta(Refine)']['passage'])\n",
    "        doc_type = data['Meta(Acqusition)']['doc_type']\n",
    "\n",
    "        if summary_type == 'short':\n",
    "            for key in ['summary1', 'summary2']:\n",
    "                summary = data['Annotation'].get(key)\n",
    "                if summary:\n",
    "                    rows.append({\n",
    "                        'src': passage,\n",
    "                        'tgt': clean_text(summary),\n",
    "                        'summary_type': 'short',\n",
    "                        'domain': doc_type\n",
    "                    })\n",
    "        elif summary_type == 'long':\n",
    "            summary3 = data['Annotation'].get('summary3')\n",
    "            if summary3:\n",
    "                rows.append({\n",
    "                    'src': passage,\n",
    "                    'tgt': clean_text(summary3),\n",
    "                    'summary_type': 'long',\n",
    "                    'domain': doc_type\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 학습 데이터 로딩\n",
    "train_short = parse_json_folder(os.path.join(base_dir, 'train/2-3sent'), 'short')\n",
    "train_long = parse_json_folder(os.path.join(base_dir, 'train/20per'), 'long')\n",
    "\n",
    "# 검증 데이터 로딩\n",
    "valid_short = parse_json_folder(os.path.join(base_dir, 'valid/2-3sent'), 'short')\n",
    "valid_long = parse_json_folder(os.path.join(base_dir, 'valid/20per'), 'long')\n",
    "\n",
    "# 확인\n",
    "print('train short 샘플:', train_short.head())\n",
    "print('train long 샘플:', train_long.head())\n",
    "print('valid short 샘플:', valid_short.head())\n",
    "print('valid long 샘플:', valid_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae92a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 저장\n",
    "train_short.to_csv(os.path.join(base_dir, 'train/2-3sent/train.csv'), index=False, encoding='utf-8-sig')\n",
    "train_long.to_csv(os.path.join(base_dir, 'train/20per/train.csv'), index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 검증 데이터 저장\n",
    "valid_short.to_csv(os.path.join(base_dir, 'valid/2-3sent/valid.csv'), index=False, encoding='utf-8-sig')\n",
    "valid_long.to_csv(os.path.join(base_dir, 'valid/20per/valid.csv'), index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"CSV 저장 완료: train/valid 데이터셋 분리 저장\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37c86be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 584,170,752 || trainable%: 0.3029\n"
     ]
    }
   ],
   "source": [
    "# 모델 & 토크나이저 로드\n",
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 베이스 모델명\n",
    "model_name = \"google/mt5-base\"\n",
    "# 토크나이저\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# 모델\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,   # mT5는 Seq2Seq 구조\n",
    "    r=16,                              # 랭크 (작을수록 가볍고 빠름)\n",
    "    lora_alpha=32,                     # LoRA scaling factor\n",
    "    lora_dropout=0.1,                  # 드롭아웃\n",
    "    target_modules=[\"q\", \"v\"]          # 주로 attention의 query, value projection에 적용\n",
    ")\n",
    "\n",
    "# 기존 베이스 mT5 모델에 LoRA 적용\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 적용 확인\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 1,769,472 || all params: 584,170,752 || trainable%: 0.3029\n",
    "# 전체 파라미터 수: 약 5억 8천만 개 (mT5-base 전체 크기)\n",
    "# 학습 가능한 파라미터 수: 약 176만 개\n",
    "# 학습 비율: 약 0.3%만 학습 → 나머지는 고정(frozen)\n",
    "# 즉, LoRA 덕분에 전체 모델을 학습시키지 않고도 극히 일부 모듈만 학습해서 GPU 메모리와 시간 절약이 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "804160db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_concat :  400\n",
      "valid_concat :  200\n"
     ]
    }
   ],
   "source": [
    "# CSV 데이터셋 로드\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import os\n",
    "\n",
    "# 기본 경로\n",
    "base_dir = './llm_data/ai_hub_summary_news_r'\n",
    "# 전체 데이터셋 로드(short/long 모두 포함)\n",
    "dataset = load_dataset(\n",
    "    'csv',\n",
    "    data_files={\n",
    "        'train_short': os.path.join(base_dir, 'train/2-3sent/train.csv'),\n",
    "        'train_long': os.path.join(base_dir, 'train/20per/train.csv'),\n",
    "        'valid_short': os.path.join(base_dir, 'valid/2-3sent/valid.csv'),\n",
    "        'valid_long': os.path.join(base_dir, 'valid/20per/valid.csv')\n",
    "    }\n",
    ")\n",
    "\n",
    "# 두 split을 합쳐서 하나의 데이터셋으로 만들기\n",
    "train_concat = concatenate_datasets([\n",
    "    dataset['train_short'].select(range(200)),\n",
    "    dataset['train_long'].select(range(200))\n",
    "])\n",
    "\n",
    "valid_concat = concatenate_datasets([\n",
    "    dataset['valid_short'].select(range(100)),\n",
    "    dataset['valid_long'].select(range(100))\n",
    "])\n",
    "\n",
    "print('train_concat : ', len(train_concat))\n",
    "print('valid_concat : ', len(valid_concat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41684941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src 결측치 개수: 0\n",
      "tgt 결측치 개수: 1\n",
      "summary_type 결측치 개수: 0\n",
      "src 결측치 개수: 0\n",
      "tgt 결측치 개수: 0\n",
      "summary_type 결측치 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# src, tgt 컬럼에 None이나 NaN이 있는지 확인\n",
    "print(\"src 결측치 개수:\", sum(x is None or str(x).lower() == \"nan\" for x in train_concat[\"src\"]))\n",
    "print(\"tgt 결측치 개수:\", sum(x is None or str(x).lower() == \"nan\" for x in train_concat[\"tgt\"]))\n",
    "print(\"summary_type 결측치 개수:\", sum(x is None or str(x).lower() == \"nan\" for x in train_concat[\"summary_type\"]))\n",
    "\n",
    "# 결측치 제거\n",
    "train_concat = train_concat.filter(lambda x: x[\"tgt\"] is not None and str(x[\"tgt\"]).strip() != \"\")\n",
    "valid_concat = valid_concat.filter(lambda x: x[\"tgt\"] is not None and str(x[\"tgt\"]).strip() != \"\")\n",
    "\n",
    "print(\"src 결측치 개수:\", sum(x is None or str(x).lower() == \"nan\" for x in train_concat[\"src\"]))\n",
    "print(\"tgt 결측치 개수:\", sum(x is None or str(x).lower() == \"nan\" for x in train_concat[\"tgt\"]))\n",
    "print(\"summary_type 결측치 개수:\", sum(x is None or str(x).lower() == \"nan\" for x in train_concat[\"summary_type\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648d0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 함수 & 데이터셋 토큰화\n",
    "\n",
    "# 데이터셋 전처리\n",
    "def preprocess(batch):\n",
    "    # summary_type을 prefix로 활용\n",
    "    inputs = [ f'[{stype.upper()}] {src}' for src, stype in zip(batch['src'], batch['summary_type'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "\n",
    "    labels = tokenizer(batch['tgt'], max_length=128, truncation=True, padding='max_length')\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "# 합쳐진 데이터셋에 바로 토큰화 적용\n",
    "tokenized_train = train_concat.map(preprocess, batched=True)\n",
    "tokenized_valid = valid_concat.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35875440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 파이프라인\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# DataCollator 적용\n",
    "# Seq2SeqTrainer에서는 DataCollatorForSeq2Seq를 사용하면 자동으로 길이에 대해서 패딩을 맞춰준다\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# 학습 설정\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./llm_models/results_lora_ai_hub_news_r\",       # 결과 저장 경로\n",
    "    eval_strategy=\"epoch\",                                      # 매 epoch마다 평가\n",
    "    learning_rate=5e-4,                                         # 학습률, # LoRA는 보통 조금 더 큰 lr 사용 가능\n",
    "    per_device_train_batch_size=4,                              # 학습 배치 크기\n",
    "    per_device_eval_batch_size=4,                               # 검증 배치 크기\n",
    "    num_train_epochs=3,                                         # 학습 epoch 수\n",
    "    weight_decay=0.01,                                          # 가중치 감쇠\n",
    "    save_total_limit=2,                                         # 체크포인트 최대 개수\n",
    "    logging_dir=\"./llm_models/results_lora_logs_ai_hub_news_r\", # 로그 저장 경로\n",
    "    logging_steps=50,                                           # 로그 출력 주기\n",
    "    predict_with_generate=True                                  # 평가 시 generate() 사용\n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,    # LoRA 적용 모델\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator     # HuggingFace Trainer는 배치 단위로 묶을 때 모든 시퀀스 길이가 동일하게 맞추기 위함\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360a456a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 42:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.956400</td>\n",
       "      <td>2.207439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.830300</td>\n",
       "      <td>1.937208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.662600</td>\n",
       "      <td>1.744926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=5.5949776713053385, metrics={'train_runtime': 2542.0755, 'train_samples_per_second': 0.471, 'train_steps_per_second': 0.118, 'total_flos': 1441764893786112.0, 'train_loss': 5.5949776713053385, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 진행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af07e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Short 요약: 한국 방위산업은 최근 몇 년간 세계 시장에서 빠르게 성장하고 있다. 특히 K-방산으로 불리는 한국형 무기체계는 중동, 동남아시아, 유럽 등 다양한 지역에서 수출 계약 성사시키며 주목\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "from transformers import pipeline\n",
    "\n",
    "# 학습된 모델과 토크나이저 로드\n",
    "summarizer = pipeline( # Hugging Face pipeline은 모델과 토크나이저를 묶어서 간단히 추론 API\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 테스트 입력 문장\n",
    "test_text = \"\"\"\n",
    "한국 방위산업은 최근 몇 년간 세계 시장에서 빠르게 성장하고 있다. \n",
    "특히 K-방산으로 불리는 한국형 무기체계는 중동, 동남아시아, 유럽 등 다양한 지역에서 수출 계약을 성사시키며 주목받고 있다. \n",
    "대표적으로 K2 전차, K9 자주포, FA-50 경공격기 등이 해외 시장에서 경쟁력을 입증했다. \n",
    "정부는 방산 수출을 국가 전략 산업으로 육성하기 위해 적극적인 외교와 지원 정책을 펼치고 있으며, \n",
    "국내 기업들도 기술 고도화와 현지 맞춤형 생산을 통해 시장을 확대하고 있다. \n",
    "이러한 흐름은 단순한 무기 판매를 넘어, 국방 협력과 외교적 영향력 강화로 이어지고 있다는 평가가 나온다.\n",
    "\"\"\"\n",
    "# short 요약\n",
    "# - 한국 방산은 K2 전차, K9 자주포, FA-50 등으로 세계 시장에서 수출 성과를 내며 성장하고 있다.\n",
    "\n",
    "#Short 요약: <extra_id_0> 한국 방위산업은 최근 몇 년간 세계 시장에서 빠르게 성장하고 있다. \n",
    "# 특히 K-방산으로 불리는 한국형 무기체계는 중동, 동남아시아, 유럽 등 다양한 지역에서 수출 계약 성사시키며 주목\n",
    "\n",
    "#Cleaned Short 요약: 한국 방위산업은 최근 몇 년간 세계 시장에서 빠르게 성장하고 있다.\n",
    "#  특히 K-방산으로 불리는 한국형 무기체계는 중동, 동남아시아, 유럽 등 다양한 지역에서 수출 계약 성사시키며 주목\n",
    "\n",
    "# 요약 생성\n",
    "# Short 요약 생성\n",
    "summary_short = summarizer(\n",
    "    f\"[SHORT] {test_text}\",\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,             # 샘플링 활성화\n",
    "    top_k=50,                   # 다양성 확보\n",
    "    top_p=0.95,                 # nucleus sampling\n",
    "    no_repeat_ngram_size=3,     # 반복 방지\n",
    "    repetition_penalty=2.0,     # 반복 패널티\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# 후처리 함수 (long/short 공통 사용 가능)\n",
    "def clean_summary(text):\n",
    "    text = text.replace(\"<extra_id_0>\", \"\").replace(\"[SHORT]\", \"\").replace(\"[LONG]\", \"\").strip()\n",
    "    if text.startswith(\",\"):\n",
    "        text = text[1:].strip()\n",
    "    sentences = text.split(\". \")\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        # 오타 및 중복 표현 교정\n",
    "        s = s.replace(\"방산로\", \"방산\").replace(\"침산\", \"방산\")\n",
    "        s = s.replace(\"방유산업\", \"방위산업\")\n",
    "        s = s.replace(\"확대를 확대\", \"확대를 추진\")\n",
    "        if s and s not in seen:\n",
    "            cleaned.append(s)\n",
    "            seen.add(s)\n",
    "    return \". \".join(cleaned)\n",
    "\n",
    "result_short = summary_short[0][\"generated_text\"]\n",
    "result_short = clean_summary(result_short)\n",
    "print(\"Cleaned Short 요약:\", result_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8c451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Long 요약: FA-50 경공격기 등이 해외 시장에서 경쟁력을 입증했다. 특히 K-방산으로 불리는 한국형 무기체계는 중동, 동남아시아, 유럽 등 다양한 지역에서 수출 계약 성사시키며 주목받고 있다. 특수적으로 K2 전차, K9 자주포 등이 세계 시장에서 활약하고 있다. 이러한 흐름은 단순한 무기 판매를 넘어, 국방 협력과 외교적 영향을 확대하기 위해 적극적인 외교와 지원 정책을 펼치고 있으며, 국내 기업들도 기술 고도화와 현지 맞춤형 생산을 통해 시장 확대를 추진하고 있다\n"
     ]
    }
   ],
   "source": [
    "# long 요약\n",
    "# - 한국 방위산업은 K-방산 무기체계의 수출 확대와 정부 지원 정책을 바탕으로 세계 시장에서 경쟁력을 강화하고 있으며, \n",
    "# - 이는 단순한 무기 판매를 넘어 국방 협력과 외교적 영향력 확대에도 기여하고 있다.\n",
    "\n",
    "#Cleaned Long 요약: FA-50 경공격기 등이 해외 시장에서 경쟁력을 입증했다. \n",
    "# 특히 K-방산으로 불리는 한국형 무기체계는 중동, 동남아시아, 유럽 등 다양한 지역에서 수출 계약 성사시키며 주목받고 있다. \n",
    "# 특수적으로 K2 전차, K9 자주포 등이 세계 시장에서 활약하고 있다. \n",
    "# 이러한 흐름은 단순한 무기 판매를 넘어, 국방 협력과 외교적 영향을 확대하기 위해 적극적인 외교와 지원 정책을 펼치고 있으며, \n",
    "# 국내 기업들도 기술 고도화와 현지 맞춤형 생산을 통해 시장 확대를 추진하고 있다\n",
    "\n",
    "summary_long = summarizer(\n",
    "    f\"[LONG] {test_text}\",\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    no_repeat_ngram_size=3,\n",
    "    repetition_penalty=2.0,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "\n",
    "# 반복문 제거\n",
    "def clean_summary(text):\n",
    "    text = text.replace(\"<extra_id_0>\", \"\").replace(\"[LONG]\", \"\").strip()\n",
    "    # 불필요한 쉼표 제거\n",
    "    if text.startswith(\",\"):\n",
    "        text = text[1:].strip()\n",
    "    # 문장 단위 분리\n",
    "    sentences = text.split(\". \")\n",
    "    seen = set()\n",
    "    cleaned = []\n",
    "    for s in sentences:\n",
    "        s = s.strip()\n",
    "        # 오타 및 중복 표현 교정\n",
    "        s = s.replace(\"방산로\", \"방산\").replace(\"침산\", \"방산\")\n",
    "        s = s.replace(\"방유산업\", \"방위산업\")\n",
    "        s = s.replace(\"확대를 확대\", \"확대를 추진\")\n",
    "        if s and s not in seen:\n",
    "            cleaned.append(s)\n",
    "            seen.add(s)\n",
    "    return \". \".join(cleaned)\n",
    "\n",
    "result = summary_long[0][\"generated_text\"]\n",
    "result = clean_summary(result)\n",
    "# result = result.replace(\"<extra_id_0>\", \"\").replace(\"[LONG]\", \"\").strip()\n",
    "print(\"Cleaned Long 요약:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 적용된 모델 저장\n",
    "model.save_pretrained(\"./llm_models/summary_model_ai_hub_news_r_lora\")\n",
    "tokenizer.save_pretrained(\"./llm_models/summary_model_ai_hub_news_r_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646becf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# 모델 로드\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\n",
    "    \"./llm_models/summary_model_ai_hub_news_r_lora\"\n",
    ")\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = T5Tokenizer.from_pretrained(\n",
    "    \"./llm_models/summary_model_ai_hub_news_r_lora\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d731c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드 & 추론\n",
    "from transformers import pipeline\n",
    "\n",
    "# 학습된 모델과 토크나이저 로드\n",
    "summarizer = pipeline( # Hugging Face pipeline은 모델과 토크나이저를 묶어서 간단히 추론 API\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 테스트 입력 문장\n",
    "test_text = \"\"\"\n",
    "지난주 열린 국제 AI 컨퍼런스에서는 생성형 AI의 윤리적 활용과\n",
    "기업 내 도입 전략에 대한 다양한 논의가 이루어졌다.\n",
    "특히 데이터 프라이버시와 저작권 문제에 대한 해결책이 주요 의제로 다뤄졌다.\n",
    "\"\"\"\n",
    "\n",
    "# 요약 생성\n",
    "# summary = summarizer( # Greedy Decoding (탐욕적 디코딩)\n",
    "#     f\"[SHORT] {test_text}\", # summary_type prefix 활용, prefix([SHORT], [LONG])\n",
    "#     max_length=128,\n",
    "#     min_length=20,\n",
    "#     do_sample=False         # 샘플링 없이 greedy decoding으로 결정적 결과\n",
    "# )\n",
    "# Beam Search (빔 탐색)\n",
    "summary = summarizer(\n",
    "    f\"[SHORT] {test_text}\",\n",
    "    max_length=128,\n",
    "    num_beams=5,      # 빔 개수\n",
    "    early_stopping=True\n",
    ")\n",
    "# Sampling?(샘플링)\n",
    "# summary = summarizer(\n",
    "#     f\"[SHORT] {test_text}\",\n",
    "#     max_length=128,\n",
    "#     do_sample=True,   # 샘플링 켬\n",
    "#     top_k=50,         # 확률 상위 50개 중 선택\n",
    "#     top_p=0.95,       # 누적 확률 95% 내에서 선택\n",
    "#     num_return_sequences=3\n",
    "# )\n",
    "\n",
    "# summary[0] 리스트의 첫 번째 결과 (딕셔너리), summary[0][\"generated_text\"] 딕셔너리 안에서 모델이 실제로 생성한 요약문 텍스트\n",
    "print('요약 결과:', summary[0][\"generated_text\"])\n",
    "# for i, s in enumerate(summary_2):\n",
    "#     print(f\"요약 {i+1}: {s['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
