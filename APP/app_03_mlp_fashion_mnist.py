# Fashion MNIST MLP ëª¨ë¸ì„ Streamlit ìœ¼ë¡œ ì›¹ì„œë¹„ìŠ¤í™”

import streamlit as st
import torch
from torch import nn
from PIL import Image
import torchvision.transforms as transforms
import torchvision.transforms.functional as TF
import numpy as np
import os

# ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜(MLPDeepLearningModel)
class MLPDeepLearningModel(nn.Module):
    # model ì •ì˜ - ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¤ì–‘í•œ ê³„ì¸µ(layer)ì„ ì •ì˜
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(784, 256)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=0.3)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = self.flatten(x) # ì…ë ¥ì¸µ
        x = self.fc1(x) # ì€ë‹‰ì¸µ
        x = self.relu(x) # í™œì„±í™”í•¨ìˆ˜ ReLU(ë¹„ì„ í˜•í•¨ìˆ˜)
        x = self.dropout(x) # overfitting ë°©ì§€
        x = self.fc2(x) # ì¶œë ¥ì¸µ
        return x

# ë¼ë²¨ ë§µ ì •ì˜
labels_map = {
    0: "T-shirt/top", 1: "Trouser", 2: "Pullover", 3: "Dress", 4: "Coat",
    5: "Sandal", 6: "Shirt", 7: "Sneaker", 8: "Bag", 9: "Ankle boot"
}

# ê²½ë¡œ êµ¬ì„±
base_dir = os.path.dirname(__file__)  # í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ë””ë ‰í† ë¦¬
model_path = os.path.join(base_dir, '..', 'models', 'model_mlp_fashion_mnist.ckpt')

# ëª¨ë¸ ë¡œë”©
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MLPDeepLearningModel().to(DEVICE)
model.load_state_dict(torch.load(model_path))
model.eval()

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Grayscale(), # Fashion MNISTëŠ” í‘ë°±
    transforms.Resize( (28, 28) ), # ì‚¬ì´ì¦ˆ ì¡°ì •
    transforms.ToTensor() # ì´ë¯¸ì§€ 0~255 ê°’ì„ ê°€ì§€ëŠ”ë°, 0~1ì‚¬ì´ ê°’ìœ¼ë¡œ ë³€í™˜
])

# Streamlit UI ìƒì„±
st.title('Fashion MNIST ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°')
uploaded_file = st.file_uploader('ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (28 X 28 í‘ë°±)', type=['png', 'jpg', 'jpeg'])

if uploaded_file is not None:
    image = Image.open(uploaded_file).convert('RGB')
    # st.image(image, caption='ì—…ë¡œë“œëœ ì´ë¯¸ì§€', use_column_width=True)
    st.image(image, caption='ì—…ë¡œë“œëœ ì´ë¯¸ì§€', width='stretch')

    # ì „ì²˜ë¦¬ ë° ì¶”ë¡ 
    img_tensor = transform(image).unsqueeze(0) # ì°¨ì› ì¶”ê°€
    pil_image = TF.to_pil_image(img_tensor.squeeze())
    st.image(pil_image, caption='ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€', width='stretch')
    with torch.no_grad():
        output = model(img_tensor) # ëª¨ë¸ ì¶”ë¡ 
        _, pred = torch.max(output, dim=1) # ëª¨ë¸ ì¶”ë¡ ê°’ ì¶”ì¶œ
        label = labels_map[pred.item()]
    
    st.success(f'ì˜ˆì¸¡ ê²°ê³¼: **{label}**')

    # ì˜ˆì¸¡ í™•ë¥  ì‹œê°í™” ì¶”ê°€ ìœ„ì¹˜
    probs = torch.nn.functional.softmax(output, dim=1)
    st.subheader("ğŸ“Š í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ í™•ë¥ ")
    for i, p in enumerate(probs[0]):
        st.write(f"{labels_map[i]}: {p.item():.2%}")