{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0286209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분류용 Transformer 모델 (Sentiment Analysis, 긍정/부정 분류)\n",
    "# Naver 데이터셋 : 다국어 영화 리뷰(긍정/부정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14887fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 라이브러리 설치 및 임포트\n",
    "import torch\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# from transformers import DistilBertTokenizer, DistilBertForSequenceClassification # 경량 모델\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fbf7889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 데이터셋 로드\n",
    "\n",
    "# CSV 파일 직접 로드 (train/test 분리된 버전)\n",
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files={\n",
    "        \"train\": \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
    "        \"test\": \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    }, \n",
    "    delimiter=\"\\t\"\n",
    ")\n",
    "\n",
    "# 빠른 테스트용: 데이터 크기 줄이기(학습용 2000개, 테스트용 500개만 사용)\n",
    "small_train = dataset['train'].shuffle(seed=42).select(range(10000))\n",
    "small_test = dataset['test'].shuffle(seed=42).select(range(2000))\n",
    "# print(small_train[0])\n",
    "# print(small_test[0])\n",
    "\n",
    "# 최종 학습용: 전체 20만 건 사용\n",
    "full_train = dataset[\"train\"]\n",
    "full_test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13656578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 10000/10000 [00:00<00:00, 63072.05 examples/s]\n",
      "Filter: 100%|██████████| 2000/2000 [00:00<00:00, 59811.40 examples/s]\n",
      "Map: 100%|██████████| 10000/10000 [00:00<00:00, 17505.51 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 16529.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. 토큰화\n",
    "\n",
    "# BERT의 사전학습된 토크나이저 모델 로드, \n",
    "# bert-base-uncased는 영어 BERT 모델로, 모든 단어를 소문자(uncased)롤 변환\n",
    "# - 토크나이저는 문장을 단어 → 토큰 → 숫자 ID로 바꿔주는 역할을 합니다. 예: \"I love movies\" → [101, 1045, 2293, 3185, 102]\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# 경량 모델 DistilBERT 토크나이저 준비\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base') # 다국어 모델 적용\n",
    "\n",
    "def tokenize(batch):\n",
    "    texts = [str(x) for x in batch['document']]\n",
    "    # batch['text'] 데이터셋에서 텍스트 분분을 가져온다\n",
    "    # padding='max_length' 모든 문장을 동일한 길이로 맞추가 위해 패딩(padding)을 추가(짧은 문장은 [PAD] 토큰으로 채움)\n",
    "    # truncation=True 너무 긴 문장은 잘라내기(truncate)를 해서 모델 입력 크기에 맞춘다\n",
    "    # 각 문장이 토큰 ID, attention mask 등으로 변환된 딕셔너리 형태로 반환\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=128) # 경량 모델\n",
    "\n",
    "# 결측치 제거 후 토큰화\n",
    "small_train = small_train.filter(lambda x: x[\"document\"] is not None and x[\"document\"] != \"\")\n",
    "small_test = small_test.filter(lambda x: x[\"document\"] is not None and x[\"document\"] != \"\")\n",
    "full_train = full_train.filter(lambda x: x[\"document\"] is not None and x[\"document\"] != \"\")\n",
    "full_test = full_test.filter(lambda x: x[\"document\"] is not None and x[\"document\"] != \"\")\n",
    "\n",
    "# dataset.map() 데이터셋 모든 샘플에 tokenize 함수를 적용\n",
    "# batched=True 여러 샘플을 한번에 묶어서 처리 -> 속도 향상\n",
    "# 결과적으로 원래 텍스트 데이터셋에 토큰화된 입력값(input_ids, attention_mask)이 추가\n",
    "# dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "small_train = small_train.map(tokenize, batched=True) # 다국어 모델\n",
    "small_test = small_test.map(tokenize, batched=True)\n",
    "full_train = full_train.map(tokenize, batched=True) # 다국어 모델\n",
    "full_test = full_test.map(tokenize, batched=True)\n",
    "\n",
    "# 라벨 컬럼 이름 변경 및 불필요한 컬럼 제거\n",
    "def prepare_dataset(ds):\n",
    "    ds = ds.rename_column(\"label\", \"labels\")\n",
    "    ds = ds.remove_columns([\"id\", \"document\"])\n",
    "    ds.set_format(\"torch\")\n",
    "    return ds\n",
    "\n",
    "small_train = prepare_dataset(small_train)\n",
    "small_test = prepare_dataset(small_test)\n",
    "full_train = prepare_dataset(full_train)\n",
    "full_test = prepare_dataset(full_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58bcb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 4. 모델 불러오기\n",
    "\n",
    "# HuggingFace transformers 라이브러리에서 제공하는 BERT 기반 텍스트 분류 모델 클래스\n",
    "# 기존 BERT 모델 위에 분류용 헤드(classification head)가 추가된 구조\n",
    "# - BERT 본체: 입력 문장을 토큰 단위로 인코딩 -> 문맥적 벡터 표현 생성\n",
    "# - Classification Head: [CLS] 토큰의 벡터를 받아 Dense Layer + Softmax로 분류 결과 출력\n",
    "\n",
    "# from_pretrained(\"bert-base-uncased\") 사전학습된 bert-base-uncased 모델 로드, uncased -> 모든 영어 단어를 소문자로 변환해서 처리\n",
    "# 이미 대규모 코퍼스(위키백과, BookCorpus 등)로 학습된 모델을 가져오기 때문에, 처음부터 학습하지 않고도 좋은 성능을 낼 수 있다\n",
    "# num_labels=2 분류할 클래스 개수를 지정 -> 여기서는 2개 클래스 예시) 긍정(Positive)/부정(Negative), 뉴스 카테고리 분류 num_labels=4(정치,경제,스포츠,기술)\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2) # 다국어 분류 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 학습 파라미터 설정\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='./results_sentiment_analysis_naver_xlm-roberta-base', # 학습된 모델과 체크포인트(중간 저장 파일)를 저장할 디렉토리 경로, 학습 도중과 완료 후 모델 가중치가 이 폴더에 저장\n",
    "    eval_strategy='epoch', # 매 epoch(한번 전체 데이터셋을 학습)마다 평가를 수행\n",
    "    save_strategy='epoch',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=50,    \n",
    "    per_device_train_batch_size=16, # 배치 사이즈 16\n",
    "    num_train_epochs=3, # 학습 반복 3회, 전체 데이터셋을 3회 학습\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs', # 학습 과정의 로그를 저장\n",
    "    fp16=True, # 혼합 정밀도 -> 속도 향상\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c054181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Trainer 구성\n",
    "\n",
    "# Trainer 객체 생성, Hugging Face에서 제공하는 고수준 학습 관리 클래스, 모델 학습/평가/저장 자동 처리\n",
    "# PyTorch의 DataLoader, 학습 루프, 옵티마이저 등을 직접 작성하지 않아도 됨\n",
    "trainer = Trainer(\n",
    "    model=model, # 학습 모델 지정(BertForSequenceClassification), BERT + 분류 헤드 구조가 학습 대상\n",
    "    args=trainer_args, # 학습 파라미터 전달\n",
    "    train_dataset=small_train, # 학습 데이터셋 지정, 토큰화된 입력(input_ids, attention_mask)과 라벨(label)이 포함\n",
    "    eval_dataset=small_test, # 평가 데이터셋 지정, 학습 중간이나 학습 후 성능을 측정할때 사용\n",
    ")\n",
    "\n",
    "# 최종 학습용 학습 (전체 데이터셋)\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=full_train,\n",
    "#     eval_dataset=full_test,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c4af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:49:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.492413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.436799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.424528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.42062521514892576, metrics={'train_runtime': 6544.5615, 'train_samples_per_second': 4.584, 'train_steps_per_second': 0.286, 'total_flos': 1973332915200000.0, 'train_loss': 0.42062521514892576, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f40739ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46613526344299316, 'eval_runtime': 10.6138, 'eval_samples_per_second': 94.217, 'eval_steps_per_second': 11.777, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# 8. 모델 평가\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86a2dca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과: tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n",
      "문장: 이 영화 정말 재미있었어요!\n",
      "예측: 긍정 (확률: 0.98)\n",
      "\n",
      "문장: 스토리가 지루하고 너무 길었어요.\n",
      "예측: 부정 (확률: 0.99)\n",
      "\n",
      "문장: 그럭저럭 볼만했지만 특별한 건 없었어요.\n",
      "예측: 부정 (확률: 0.97)\n",
      "\n",
      "문장: 배우들의 연기가 훌륭해서 몰입할 수 있었습니다.\n",
      "예측: 긍정 (확률: 0.98)\n",
      "\n",
      "문장: 중간에 잠들 정도로 지루했어요.\n",
      "예측: 부정 (확률: 0.99)\n",
      "\n",
      "문장: 전개가 신선하고 흥미로웠습니다.\n",
      "예측: 긍정 (확률: 0.98)\n",
      "\n",
      "문장: 대사가 어색하고 자연스럽지 않았습니다.\n",
      "예측: 부정 (확률: 0.91)\n",
      "\n",
      "문장: 처음부터 끝까지 즐겁게 볼 수 있었어요!\n",
      "예측: 긍정 (확률: 0.99)\n",
      "\n",
      "문장: 특수효과가 싸구려 같아서 실망했어요.\n",
      "예측: 부정 (확률: 0.98)\n",
      "\n",
      "문장: 괜찮았지만 다시 보고 싶지는 않네요.\n",
      "예측: 긍정 (확률: 0.99)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. 추론 테스트\n",
    "# test_sentences = [\n",
    "#     \"I really loved this movie, it was fantastic!\",\n",
    "#     \"The film was boring and too long.\",\n",
    "#     \"An average movie, not too bad but not great either.\",\n",
    "#     \"The acting was brilliant and kept me engaged.\",\n",
    "#     \"I fell asleep halfway through, it was so dull.\",\n",
    "#     \"The storyline was unique and very exciting.\",\n",
    "#     \"The dialogue felt forced and unnatural.\",\n",
    "#     \"I enjoyed every moment, truly a masterpiece!\",\n",
    "#     \"The special effects were cheap and disappointing.\",\n",
    "#     \"It was okay, but I wouldn’t watch it again.\"\n",
    "# ]\n",
    "\n",
    "test_sentences = [\n",
    "    \"이 영화 정말 재미있었어요!\",\n",
    "    \"스토리가 지루하고 너무 길었어요.\",\n",
    "    \"그럭저럭 볼만했지만 특별한 건 없었어요.\",\n",
    "    \"배우들의 연기가 훌륭해서 몰입할 수 있었습니다.\",\n",
    "    \"중간에 잠들 정도로 지루했어요.\",\n",
    "    \"전개가 신선하고 흥미로웠습니다.\",\n",
    "    \"대사가 어색하고 자연스럽지 않았습니다.\",\n",
    "    \"처음부터 끝까지 즐겁게 볼 수 있었어요!\",\n",
    "    \"특수효과가 싸구려 같아서 실망했어요.\",\n",
    "    \"괜찮았지만 다시 보고 싶지는 않네요.\"\n",
    "]\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# 토큰화\n",
    "inputs = tokenizer(\n",
    "    test_sentences, # 추론 테스트 문장\n",
    "    padding=True, # 문장 길이를 맞추기 위해 패딩 추가\n",
    "    truncation=True, # 너무 긴 문장은 잘라냄\n",
    "    max_length=128, # 최대 토큰 길이 제한\n",
    "    return_tensors='pt' # PyTorch 텐서로 반환\n",
    ").to(device) # 입력 텐서를 GPU/CPU로 이동\n",
    "\n",
    "# 모델 예측\n",
    "\n",
    "# **inputs -> 파이썬 언패킹 문법을 사용해 딕셔너리의 키-값을 함수 인자로 전달\n",
    "# 내부적으로 호출 -> model(input_ids=..., attention_mask=...)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# outputs.logits 각 클래스(긍정/부정)에 대한 점수, 예시) [[-1.3135,  1.1777],[ 1.1172, -1.1963],[ 0.2496, -0.2385]]\n",
    "# 가장 높은 점수를 가진 클래스 선택 -> 결과 [1,0,0] (0=부정, 1=긍정), 예시) [1, 0, 0]\n",
    "predictions = torch.argmax(outputs.logits, dim=1)\n",
    "print('예측 결과:', predictions)\n",
    "\n",
    "# 확률값 계산\n",
    "probs = torch.softmax(outputs.logits, dim=1)\n",
    "\n",
    "# test_sentences -> 추론에 테스트 할 문장\n",
    "# predictions -> 모델이 예측한 결과(0 또는 1 값이 들어 있는 텐서)\n",
    "# zip(test_sentences, predictions) -> 두 리스트/텐서를 묶어서 문장과 예측값을 한쌍으로 반복할 수 있게 만든다\n",
    "for i, (sentence, pred) in enumerate(zip(test_sentences, predictions)):\n",
    "    label = '긍정' if pred.item() == 1 else '부정'\n",
    "    confidence = probs[i, pred.item()].item()  # 행(i), 열(pred) 지정\n",
    "    print(f'문장: {sentence}\\n예측: {label} (확률: {confidence:.2f})\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
