{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 다국어 기계 번역 모델 & 파인 튜닝\n",
    "# - Hugging Face 라이브러리 적용\n",
    "# - AI HUB 금융 다국어 번역 데이터셋 적용\n",
    "# - 입력된 문장을 다국어 기계 번역 모델을 통한 영어->한국어, 한국어->번역\n",
    "# 1. 학습 목표\n",
    "# - 구조 최적화 및 파이프라인 단순화\n",
    "# - AI HUB 방송 다국어 번역 데이터셋 전처리\n",
    "# - 병렬 문장쌍 데이터셋 변환 전처리\n",
    "# - 토크나이징 및 토크나이징 전처리\n",
    "# - 베이스 모델 로드\n",
    "# - LoRA(Low-Rank Adaptation) 설정, 특정 레이어에 작은 저차원 행렬(랭크 r)을 삽입해서 학습\n",
    "# - LoRA(Low-Rank Adaptation) 모델, 메모리 효율성/빠른 학습/도메인 적용, base 모델에 여러 LoRA 모듈을 붙였다 떼었다 할 수 있음\n",
    "# - 학습 args 설정\n",
    "# - Trainer 정의\n",
    "# - Trainer 실행\n",
    "# - LoRA 적용된 모델 저장, LoRA모델/토크나이저\n",
    "# - LoRA 적용된 모델 불러오기, 베이스모델/LoRA모델/토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bf0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129 cuda\n",
      "CUDA 사용 가능 여부: True\n",
      "PyTorch CUDA 버전: 12.9\n",
      "빌드 정보: 2.8.0+cu129\n",
      "사용 중인 GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob, json, re, os, random, csv\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__, device)\n",
    "\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"빌드 정보:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49046719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장쌍 개수: 80883, 80883\n",
      "2019년부터 전년말 기준 자산총액 2조 원 이상인 기업에 대해 의무화하였으며, 2020년부터는 전년말 기준 자산총액 5천 억 원 이상 기업까지 의무대상을 확대했다.\n",
      "From 2019, it was mandatory for companies with total assets of KRW 2 trillion or more as of the end of the previous year, and from 2020, the mandatory target was expanded to companies with total assets of KRW 500 billion or more as of the end of the previous year.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 전처리 - AI HUB 방송 다국어 번역 데이터셋\n",
    "ko_lines, en_lines = [], []\n",
    "folders = [ # 폴더 리스트 정의\n",
    "    './llm_data/ai_hub_article/*.json'\n",
    "]\n",
    "\n",
    "# 모든 JSON 읽기\n",
    "for folder in folders:\n",
    "    for path in glob.glob(folder): # 특정 디렉토리에서 지정한 패턴과 일치하는 모든 파일 경로를 리스트로 반환\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f) # 파일 전체 로드(dict 구조)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # sents 리스트에서 문장쌍 추출\n",
    "            for sent in data.get('sents', []):\n",
    "                en = sent.get('mtpe') # 원문(영어)\n",
    "                ko = sent.get('source_cleaned') # 최종번역문(한국어) 추출\n",
    "\n",
    "                if en and ko and ko != 'N/A':\n",
    "                    en_lines.append(en.strip())\n",
    "                    ko_lines.append(ko.strip())\n",
    "\n",
    "# 1. Detokenize 함수 정의\n",
    "def detokenize_sentence(sentence: str) -> str:\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"\\s+([?.!,])\", r\"\\1\", sentence)  # \" ?\" → \"?\"\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)            # 여러 공백 → 하나\n",
    "    return sentence\n",
    "\n",
    "# 2. 데이터셋 전처리\n",
    "en_lines = [detokenize_sentence(s) for s in en_lines]\n",
    "ko_lines = [detokenize_sentence(s) for s in ko_lines]\n",
    "\n",
    "\n",
    "print(f'총 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')\n",
    "print(ko_lines[0])\n",
    "print(en_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "879c0f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 문장쌍 개수: 80883, 80883\n",
      "2019년부터 전년말 기준 자산총액 2조 원 이상인 기업에 대해 의무화하였으며, 2020년부터는 전년말 기준 자산총액 5천 억 원 이상 기업까지 의무대상을 확대했다.\n",
      "From 2019, it was mandatory for companies with total assets of KRW 2 trillion or more as of the end of the previous year, and from 2020, the mandatory target was expanded to companies with total assets of KRW 500 billion or more as of the end of the previous year.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 중복 제거 및 순서 유지\n",
    "# pairs = list(set(zip(en_lines, ko_lines)))\n",
    "# en_lines, ko_lines = zip(*pairs) # 다시 분리\n",
    "seen = set()\n",
    "pairs = []\n",
    "for en, ko in zip(en_lines, ko_lines):\n",
    "    if (en, ko) not in seen:\n",
    "        # 새로운 문장쌍을 집합에 기록, 이후 같은 문장쌍이 나오면 if 조건에서 걸러져 추가되지 않는다\n",
    "        seen.add( (en, ko) )\n",
    "        \n",
    "        # 중복이 아닌 문장쌍을 리스트에 추가, 원래 순서대로 중복 없는 문장쌍 리스트가 만들어 진다\n",
    "        # - pairs는 [(\"Hello\",\"안녕\"), (\"Goodbye\",\"잘가\")] \n",
    "        pairs.append( (en, ko) )\n",
    "\n",
    "# 이를 다시 분리 - 영어 문장들만 모아 (\"Hello\",\"Goodbye\"), 한국어 문장들만 모아 (\"안녕\",\"잘가\")\n",
    "en_lines, ko_lines = zip(*pairs)\n",
    "\n",
    "print(f'중복 제거 후 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')\n",
    "print(ko_lines[0])\n",
    "print(en_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bc2927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링 후 문장쌍 개수: 80883, 80883\n",
      "2019년부터 전년말 기준 자산총액 2조 원 이상인 기업에 대해 의무화하였으며, 2020년부터는 전년말 기준 자산총액 5천 억 원 이상 기업까지 의무대상을 확대했다.\n",
      "From 2019, it was mandatory for companies with total assets of KRW 2 trillion or more as of the end of the previous year, and from 2020, the mandatory target was expanded to companies with total assets of KRW 500 billion or more as of the end of the previous year.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 샘플링 추가\n",
    "\n",
    "# 샘플링 최대 50,000 문장만 사용\n",
    "# sample_size = 50000\n",
    "sample_size = len(en_lines)\n",
    "if len(ko_lines) > sample_size:\n",
    "    indices = random.sample(range(len(ko_lines)), sample_size)\n",
    "    ko_lines = [ ko_lines[i] for i in indices ]\n",
    "    en_lines = [ en_lines[i] for i in indices ]\n",
    "\n",
    "print(f'샘플링 후 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')\n",
    "print(ko_lines[0])\n",
    "print(en_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "756e86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 존재하는 폴더: ./llm_data/ai_hub_article_translation\n",
      "저장 완료 ./llm_data/ai_hub_article_translation/train_ko.txt ./llm_data/ai_hub_article_translation/train_en.txt\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 저장\n",
    "out_dir = './llm_data/ai_hub_article_translation'\n",
    "\n",
    "# 폴더 없을시 생성\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f'폴더 생성 완료: {out_dir}')\n",
    "else:\n",
    "    print(f'이미 존재하는 폴더: {out_dir}')\n",
    "\n",
    "ko_path = f'{out_dir}/train_ko.txt'\n",
    "en_path = f'{out_dir}/train_en.txt'\n",
    "\n",
    "with open(ko_path, 'w', encoding='utf-8') as fko, \\\n",
    "    open(en_path, 'w', encoding='utf-8') as fen:\n",
    "    for k, e in zip(ko_lines, en_lines):\n",
    "        fko.write(k + '\\n')\n",
    "        fen.write(e + '\\n')\n",
    "print('저장 완료', ko_path, en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b55675c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# AI Hub 방송 데이터셋(train_ko.txt, train_en.txt) -> CSV로 변환해 파인 튜닝\n",
    "# 원본 데이터는 train_ko.txt, train_en.txt로 분리 -> 병렬 문장쌍을 만들어야 함\n",
    "# 파인튜닝시 양방향 번역을 지원하려면 같은 문장쌍은 en->ko, ko->en 두방향으로 모두 포함해야 함\n",
    "# CSV 구조 예시\n",
    "# src,tgt,src_lang,tgt_lang\n",
    "# You can buy it from a convenience store try it out.,편의점에서 사실 수 있으니 시도해보시길 바랍니다.,en,ko\n",
    "# 편의점에서 사실 수 있으니 시도해보시길 바랍니다.,You can buy it from a convenience store try it out.,ko,en\n",
    "# She frees him and takes him as her navigator.,그녀는 그를 풀어주고 조종자로써 그를 데려간다.,en,ko\n",
    "# 그녀는 그를 풀어주고 조종자로써 그를 데려간다.,She frees him and takes him as her navigator.,ko,en\n",
    "# Iyengar's belief in Gandhi's philosophy is so deep that he can't even open his laptop without remorse.,간디의 철학에 대한 아이옌가르의 믿음은 너무 깊어서, 그는 심지어 그의 노트북도 양심의 가책 없이 열 수 없다.,en,ko\n",
    "# 간디의 철학에 대한 아이옌가르의 믿음은 너무 깊어서, 그는 심지어 그의 노트북도 양심의 가책 없이 열 수 없다.,Iyengar's belief in Gandhi's philosophy is so deep that he can't even open his laptop without remorse.,ko,en\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 전체 데이터 로드 - AI Hub 방송 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_article_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_article_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines = f_en.read().splitlines()\n",
    "    ko_lines = f_ko.read().splitlines()\n",
    "\n",
    "# 데이터 개수 제한 (예: 100개)\n",
    "limit = 1000\n",
    "en_lines = en_lines[:limit]\n",
    "ko_lines = ko_lines[:limit]\n",
    "\n",
    "# train/valid split(90 : 10)\n",
    "split_idx = int(len(en_lines) * 0.9)\n",
    "train_en, valid_en = en_lines[:split_idx], en_lines[split_idx:]\n",
    "train_ko, valid_ko = ko_lines[:split_idx], ko_lines[split_idx:]\n",
    "\n",
    "print(len(train_en))\n",
    "print(len(valid_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efef8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2019, it was mandatory for companies with total assets of KRW 2 trillion or more as of the end of the previous year, and from 2020, the mandatory target was expanded to companies with total assets of KRW 500 billion or more as of the end of the previous year.\n",
      "2019년부터 전년말 기준 자산총액 2조 원 이상인 기업에 대해 의무화하였으며, 2020년부터는 전년말 기준 자산총액 5천 억 원 이상 기업까지 의무대상을 확대했다.\n"
     ]
    }
   ],
   "source": [
    "# 병렬 데이터 생성 - 편향되지 않은 데이터셋 생성\n",
    "# 영어->한국어, 한국어->영어\n",
    "\n",
    "# train.csv 생성\n",
    "with open('./llm_data/ai_hub_article_translation/train.csv', 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang'])\n",
    "\n",
    "    print(train_en[0])\n",
    "    print(train_ko[0])\n",
    "    for en, ko in zip(train_en, train_ko):\n",
    "        en, ko = en.strip(), ko.strip()\n",
    "        if not en or not ko:\n",
    "            continue\n",
    "\n",
    "        # 영어 -> 한국어\n",
    "        writer.writerow([en, ko, 'en', 'ko'])\n",
    "        # 한국어 -> 영어\n",
    "        writer.writerow([ko, en, 'ko', 'en'])\n",
    "\n",
    "# valid.csv 생성\n",
    "with open('./llm_data/ai_hub_article_translation/valid.csv', 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang'])\n",
    "\n",
    "    for en, ko in zip(valid_en, valid_ko):\n",
    "        en, ko = en.strip(), ko.strip()\n",
    "        if not en or not ko:\n",
    "            continue\n",
    "        \n",
    "        # 영어 -> 한국어\n",
    "        writer.writerow([en, ko, 'en', 'ko'])\n",
    "        # 한국어 -> 영어\n",
    "        writer.writerow([ko, en, 'ko', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "420188a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 전처리\n",
    "from datasets import load_dataset # Hugging Face의 데이터셋 관리 라이브러리(학습용 데이터 로딩에 사용)\n",
    "# M2M100 모델과 토크나이저, 학습 관련 유틸리티 제공\n",
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model # LoRA 설정을 위한 라이브러리(모델 파라미터 효율적 파인튜닝)\n",
    "\n",
    "# 1. tokenizer 로드, Hugging Face M2M100-418M 모델의 토크나이저 로드\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# 2. 전처리 함수 (동적 tgt_lang 설정)\n",
    "def preprocess_function(examples):\n",
    "    # 데이터셋에서 src(원문), tgt(번역문), tgt_lang(목표 언어 코드) 가져옴\n",
    "    inputs = examples[\"src\"]\n",
    "    targets = examples[\"tgt\"]\n",
    "    tgt_langs = examples[\"tgt_lang\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, # src(원문)을 토큰화\n",
    "        max_length=128,  # 최대 길이 128\n",
    "        truncation=True, # 최대 길이 초과시 잘라냄\n",
    "        padding=\"max_length\" # 길이가 부족하면 padding 처리\n",
    "    )\n",
    "\n",
    "    labels_list = []\n",
    "    for text, lang in zip(targets, tgt_langs):\n",
    "        # 각 문장마디 목표 언어(tgt_lang)\n",
    "        if lang in tokenizer.lang_code_to_id:\n",
    "            tokenizer.tgt_lang = lang\n",
    "        else:\n",
    "            tokenizer.tgt_lang = \"en\"  # 기본값\n",
    "        \n",
    "        # 타겟 문장 토큰화\n",
    "        with tokenizer.as_target_tokenizer(): # 번역 대상 문장을 토큰화할때 사용하는 모드\n",
    "            labels = tokenizer(\n",
    "                text, # tgt(번역문)\n",
    "                max_length=128, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        # labels[\"input_ids\"]를 추출해서 학습용 정답(label)로 저장\n",
    "        labels_list.append(labels[\"input_ids\"])\n",
    "\n",
    "    # 입력(src)와 정답(tgt)을 모두 포함한 딕셔너리 반환, Trainer가 이 반환값을 받아서 학습에 사용\n",
    "    # {\n",
    "    #     \"input_ids\": [...],        # 원문 토큰 시퀀스\n",
    "    #     \"attention_mask\": [...],   # 패딩 여부 표시\n",
    "    #     \"labels\": [...]            # 번역문 토큰 시퀀스\n",
    "    # }\n",
    "    model_inputs[\"labels\"] = labels_list\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fd609d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afee5aabaf649f38012b1d1ff87283c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47ca7e77a1046baba4f4db88302c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d8bd969465473f908012b105419c8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fd2ee38f8844c283a8ea71bea8449d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 불러오기\n",
    "dataset = load_dataset( # Hugging Face datasets 라이브러리를 사용해 CSV 파일을 로드\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"./llm_data/ai_hub_article_translation/train.csv\",\n",
    "        \"validation\": \"./llm_data/ai_hub_article_translation/valid.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 토크나이즈 적용\n",
    "# - 결과: 각 샘플이 {\"input_ids\": ..., \"attention_mask\": ..., \"labels\": ...} \n",
    "tokenized_dataset = dataset.map( # map() 함수로 앞서 정의한 preprocess_function을 데이터셋 전체에 적용\n",
    "    preprocess_function, \n",
    "    batched=True # 여러 샘플을 한번에 처리하여 속도 향상\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "817f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "\n",
    "# pretrained model M2M100_418M 로드, 베이스 모델로 사용\n",
    "base_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,              # 랭크 크기(저차원 행렬 크기), 작을수록 가볍고 빠르지만 표현력이 줄어듬\n",
    "    lora_alpha=32,    # 스케일링 계수, 학습된 LoRA 행렬을 원래 모델에 얼마나 반영할지 결정하는 스케일\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Attention 모듈에 적용, Query/Value 저차원 행렬만 학습\n",
    ")\n",
    "\n",
    "# LoRA 모델 생성, PEFT(Param-Efficient Fine-Tuning) 모델 생성\n",
    "model = get_peft_model(base_model, lora_config) # 전체 모델 파라미터는 그대로 두고 LoRA 모듈만 학습 대상이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4780be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_models/results_lora\",    # 학습 결과(모델, 체크포인트) 저장 경로\n",
    "    eval_strategy=\"epoch\",          # 구버전, 매 epoch마다 평가\n",
    "    learning_rate=5e-5,             # 학습률\n",
    "    per_device_train_batch_size=16, # 학습 배치 크기\n",
    "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
    "    num_train_epochs=3,             # 학습 epoch 수\n",
    "    weight_decay=0.01,              # 가중치 감쇠(정규화)\n",
    "    save_total_limit=2,             # 체크포인트 최대 저장 개수\n",
    "    logging_dir=\"./results_lora_logs\",           # 로그 저장 경로\n",
    "    logging_steps=100,              # 100 step마다 로그 기록\n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,        # LoRA 적용된 모델\n",
    "    args=training_args, # 학습 설정\n",
    "    train_dataset=tokenized_dataset[\"train\"],       # 학습 데이터셋\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],   # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ca31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 43/339 04:58 < 35:52, 0.14 it/s, Epoch 0.37/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3218a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO: It is literally the basis of life.\n",
      "KO→EN: 말 그대로 삶의 기초이다.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)   # 입력도 GPU로 이동\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)   # 입력도 GPU로 이동\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40cb8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llm_models/lora_translation_finance_model\\\\tokenizer_config.json',\n",
       " './llm_models/lora_translation_finance_model\\\\special_tokens_map.json',\n",
       " 'llm_models\\\\lora_translation_finance_model\\\\vocab.json',\n",
       " 'llm_models\\\\lora_translation_finance_model\\\\sentencepiece.bpe.model',\n",
       " './llm_models/lora_translation_finance_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 적용된 모델 저장\n",
    "model.save_pretrained(\"./llm_models/lora_translation_finance_model\")\n",
    "tokenizer.save_pretrained(\"./llm_models/lora_translation_finance_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98cc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 원본 M2M100 모델 로드\n",
    "base_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# LoRA 어댑터 붙여서 불러오기\n",
    "model = PeftModel.from_pretrained(base_model, \"./llm_models/lora_translation_finance_model\")\n",
    "\n",
    "# 토크나이저도 불러오기\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"./llm_models/lora_translation_finance_model\")\n",
    "\n",
    "# 디바이스 맞추기\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07bb1f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO:\n",
      "The weather is very good today. → The weather is very good today.\n",
      "I am studying machine learning. → I am studying machine learning.\n",
      "Artificial intelligence is changing the world. → Artificial Intelligence is changing the world.\n",
      "Deep learning is a subset of machine learning. → Deep learning is a subset of machine learning.\n",
      "Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day. → Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day.\n",
      "When women replaced men, they earned much less money than them, which was a real problem. → When women replaced men, they earned much less money than themselves, which was a real problem.\n",
      "It is literally the basis of life. → It is literally the basis of life.\n",
      "It's hard to believe, but the People's Artist of Russia, singer and TV presenter Nadezhda Babkina is 70 years old! → It’s hard to believe, but the People’s Artist of Russia, singer and TV presentator Nadezhda Babkina is 70 years old!\n",
      "Arriving in the USA, Nuno is assigned the teacher Jane Dayle Haddon as a &quot;tutor&quot;. → Arriving in the United States, Nuno is assigned to the teacher Jane Dayle Haddon as a &quot;tutor&quot;.\n",
      "This mediated dialogue allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness. → This mediated dialog allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness.\n",
      "The findings of this analysis, as demonstrated during the recent global financial crisis, indicate that the impact of volatility in overseas financial markets on the domestic financial market has undergone long-term and continuous changes, reflecting the increased interconnectivity between domestic and foreign financial markets since the foreign exchange crisis. → The findings of this analysis, as demonstrated during the recent global financial crisis, indicate that the impact of volatility in foreign financial markets on the domestic financial market has undergone long-term and continuous changes, reflecting the increased interconnectivity between domestic and foreign financial markets since the foreign exchange crisis.\n",
      "\n",
      "KO→EN:\n",
      "오늘 날씨는 매우 좋습니다. → 오늘 날씨는 매우 좋다.\n",
      "저는 머신러닝을 공부하고 있습니다. → 나는 기계학을 공부하고 있다.\n",
      "인공 지능은 세계를 변화시킵니다. → 인공지능은 세계를 변화시킨다.\n",
      "딥러닝은 머신 러닝의 하위 세트입니다. → 깊은 학습은 기계 학습의 하위 세트이다.\n",
      "지엠 판매원인 로베르토는 그가 매일 편지를 쓰는 오르넬라 무티를 열렬히 사랑한다. → 지임 판매원인 로베르토는 매일 편지를 쓰는 오넬라 무티를 열정적으로 사랑한다.\n",
      "여성이 남성을 대체했을 때, 그들은 남성보다 훨씬 적은 돈을 벌게 되었고 이것이 진짜 문제였습니다. → 여성이 남자를 대체했을 때 남성보다 훨씬 적은 돈을 벌 수 있었고 이것이 진정한 문제였다.\n",
      "말 그대로 삶의 기초입니다. → 말 그대로 삶의 기초이다.\n",
      "믿기 어렵겠지만, 러시아의 인민 예술가이자 가수이자 텔레비전 진행자인 나데즈다 밥키나는 70세입니다! → 믿기 힘들지만 러시아의 민간 예술가이자 노래가자 텔레비전 선구자 나데즈다 보빈은 70세이다.\n",
      "미국에 도착한 누노는 제인 데일 해든 선생님으로 임명되었다. → 미국에 도착한 누노는 제인 데일 하든 교수로 임명되었다.\n",
      "이 중재된 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반성하고 게임이 공개되면 더 큰 인식으로 게임을 재개할 수 있습니다. → 이 중재 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반영하고 게임이 공개되면 게임을 더 큰 인식으로 재개할 수 있다.\n",
      "이러한 분석 결과는 최근의 글로벌 금융위기 기간에 잘 드러났듯이 국내외 금융 시장 간 연계성이 외환위기 이후 커진 가운데 해외 금융시장의 변동성이 국내 금융 시장에 미치는 파급효과도 보다 장기적이고 지속적으로 변화된 것을 의미한다. → 이러한 분석 결과는 최근의 글로벌 금융위기 기간에 밝혀진 바와 같이 외환위기 이후 국내외 금융시장 간의 연관성이 증가함에 따라 해외 금융시장의 변동이 국내 금융시장에 미치는 파도 효과가 더 장기적이고 지속적으로 변화하고 있음을 의미한다.\n"
     ]
    }
   ],
   "source": [
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "texts = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day.\",\n",
    "    \"When women replaced men, they earned much less money than them, which was a real problem.\",\n",
    "    \"It is literally the basis of life.\",\n",
    "    \"It's hard to believe, but the People's Artist of Russia, singer and TV presenter Nadezhda Babkina is 70 years old!\",\n",
    "    \"Arriving in the USA, Nuno is assigned the teacher Jane Dayle Haddon as a &quot;tutor&quot;.\",\n",
    "    \"This mediated dialogue allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness.\",\n",
    "    \"The findings of this analysis, as demonstrated during the recent global financial crisis, indicate that the impact of volatility in overseas financial markets on the domestic financial market has undergone long-term and continuous changes, reflecting the increased interconnectivity between domestic and foreign financial markets since the foreign exchange crisis.\"\n",
    "\n",
    "]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"EN→KO:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "texts_ko = [\n",
    "    \"오늘 날씨는 매우 좋습니다.\",\n",
    "    \"저는 머신러닝을 공부하고 있습니다.\",\n",
    "    \"인공 지능은 세계를 변화시킵니다.\",\n",
    "    \"딥러닝은 머신 러닝의 하위 세트입니다.\",\n",
    "    \"지엠 판매원인 로베르토는 그가 매일 편지를 쓰는 오르넬라 무티를 열렬히 사랑한다.\",\n",
    "    \"여성이 남성을 대체했을 때, 그들은 남성보다 훨씬 적은 돈을 벌게 되었고 이것이 진짜 문제였습니다.\",\n",
    "    \"말 그대로 삶의 기초입니다.\",\n",
    "    \"믿기 어렵겠지만, 러시아의 인민 예술가이자 가수이자 텔레비전 진행자인 나데즈다 밥키나는 70세입니다!\",\n",
    "    \"미국에 도착한 누노는 제인 데일 해든 선생님으로 임명되었다.\",\n",
    "    \"이 중재된 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반성하고 게임이 공개되면 더 큰 인식으로 게임을 재개할 수 있습니다.\",\n",
    "    \"이러한 분석 결과는 최근의 글로벌 금융위기 기간에 잘 드러났듯이 국내외 금융 시장 간 연계성이 외환위기 이후 커진 가운데 해외 금융시장의 변동성이 국내 금융 시장에 미치는 파급효과도 보다 장기적이고 지속적으로 변화된 것을 의미한다.\"\n",
    "\n",
    "]\n",
    "inputs = tokenizer(texts_ko, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\\nKO→EN:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts_ko[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO:\n",
      "The weather is very good today. → 오늘날 날씨는 매우 좋습니다.\n",
      "I am studying machine learning. → 저는 기계학을 공부하고 있습니다.\n",
      "Artificial intelligence is changing the world. → 인공지능은 세상을 변화시킨다.\n",
      "Deep learning is a subset of machine learning. → 깊은 학습은 기계 학습의 하위 세트입니다.\n",
      "Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day. → GM 판매자 인 Roberto는 매일 글을 쓰는 Ornella Muti와 미친 듯 사랑에 빠져 있습니다.\n",
      "When women replaced men, they earned much less money than them, which was a real problem. → 여성들이 남자를 대신했을 때, 그들은 그들보다 훨씬 적은 돈을 벌었고, 이는 실제 문제였습니다.\n",
      "It is literally the basis of life. → 그것은 문자 그대로 삶의 기초입니다.\n",
      "It's hard to believe, but the People's Artist of Russia, singer and TV presenter Nadezhda Babkina is 70 years old! → 그것은 믿기 어렵지만, 러시아의 민간 아티스트, 노래와 TV 선생 Nadezhda Babkina는 70 년입니다!\n",
      "Arriving in the USA, Nuno is assigned the teacher Jane Dayle Haddon as a &quot;tutor&quot;. → 미국에 도착하면서, Nuno는 교사 Jane Dayle Haddon에게 &quot;교사&quot;로 임명됩니다.\n",
      "This mediated dialogue allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness. → 이 중재된 대화는 스테파니아와 안드레아가 그들의 관계를 반영하고, 게임이 공개되면, 그것을 더 많은 인식으로 재현할 수 있게 해줍니다.\n",
      "The findings of this analysis, as demonstrated during the recent global financial crisis, indicate that the impact of volatility in overseas financial markets on the domestic financial market has undergone long-term and continuous changes, reflecting the increased interconnectivity between domestic and foreign financial markets since the foreign exchange crisis. → 최근의 글로벌 금융 위기에서 보여진 이 분석의 결과에 따르면 국내 금융 시장에 대한 해외 금융 시장의 변동성의 영향은 장기적이고 지속적인 변화를 겪었으며, 이는 외환 위기 이후 국내 금융 시장과 외국 금융 시장 간의 상호 연결이 증가한 것을 반영한다.\n",
      "\n",
      "KO→EN:\n",
      "오늘 날씨는 매우 좋습니다. → The weather is very good today.\n",
      "저는 머신러닝을 공부하고 있습니다. → I am studying machine learning.\n",
      "인공 지능은 세계를 변화시킵니다. → Artificial intelligence changes the world.\n",
      "딥러닝은 머신 러닝의 하위 세트입니다. → Deep learning is a subset of machine learning.\n",
      "지엠 판매원인 로베르토는 그가 매일 편지를 쓰는 오르넬라 무티를 열렬히 사랑한다. → Roberto, a ZIM seller, loves Ornella Muti, whom he writes every day.\n",
      "여성이 남성을 대체했을 때, 그들은 남성보다 훨씬 적은 돈을 벌게 되었고 이것이 진짜 문제였습니다. → When women replaced men, they earned much less money than men, and this was a real problem.\n",
      "말 그대로 삶의 기초입니다. → It is literally the basis of life.\n",
      "믿기 어렵겠지만, 러시아의 인민 예술가이자 가수이자 텔레비전 진행자인 나데즈다 밥키나는 70세입니다! → It’s hard to believe, but Russian folk artist, singer and television player Nadezda Bobkina is 70 years old!\n",
      "미국에 도착한 누노는 제인 데일 해든 선생님으로 임명되었다. → The sister arriving in the United States was appointed as Professor Jane Dale Harden.\n",
      "이 중재된 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반성하고 게임이 공개되면 더 큰 인식으로 게임을 재개할 수 있습니다. → Through this intermitted conversation, Stephania and Andrea reflect on their relationship and when the game is opened, they can reopen the game with greater awareness.\n",
      "이러한 분석 결과는 최근의 글로벌 금융위기 기간에 잘 드러났듯이 국내외 금융 시장 간 연계성이 외환위기 이후 커진 가운데 해외 금융시장의 변동성이 국내 금융 시장에 미치는 파급효과도 보다 장기적이고 지속적으로 변화된 것을 의미한다. → These analysis results, as well revealed in the recent period of global financial crisis, mean that the linkability between the domestic and foreign financial markets has increased since the foreign currency crisis and that the volatility of the foreign financial markets on the domestic financial markets has also changed more long-term and continuously.\n"
     ]
    }
   ],
   "source": [
    "# 번역 :      이러한 분석 결과는 최근의 글로벌 금융위기 기간에 잘 드러났듯이 국내외 금융 시장 간 연계성이 외환위기 이후 커진 가운데 해외 금융시장의 변동성이 국내 금융 시장에 미치는 파급효과도 보다 장기적이고 지속적으로 변화된 것을 의미한다.\n",
    "# 파인튜닝 :   최근의 글로벌 금융 위기에서 증명된 이 분석의 결과에 따르면 해외 금융 시장의 변동성의 국내 금융 시장에 미치는 영향은 외환 위기 이후 국내 금융 시장과 외국 금융 시장 간의 상호 연결성 증가를 반영하는 장기적 및 지속적인 변화를 경험했습니다.\n",
    "# 베이스모델 : 최근의 글로벌 금융 위기에서 보여진 이 분석의 결과에 따르면 국내 금융 시장에 대한 해외 금융 시장의 변동성의 영향은 장기적이고 지속적인 변화를 겪었으며, 이는 외환 위기 이후 국내 금융 시장과 외국 금융 시장 간의 상호 연결이 증가한 것을 반영한다.\n",
    "\n",
    "# 베이스 모델 테스트\n",
    "import torch\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "texts = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day.\",\n",
    "    \"When women replaced men, they earned much less money than them, which was a real problem.\",\n",
    "    \"It is literally the basis of life.\",\n",
    "    \"It's hard to believe, but the People's Artist of Russia, singer and TV presenter Nadezhda Babkina is 70 years old!\",\n",
    "    \"Arriving in the USA, Nuno is assigned the teacher Jane Dayle Haddon as a &quot;tutor&quot;.\",\n",
    "    \"This mediated dialogue allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness.\",\n",
    "    \"The findings of this analysis, as demonstrated during the recent global financial crisis, indicate that the impact of volatility in overseas financial markets on the domestic financial market has undergone long-term and continuous changes, reflecting the increased interconnectivity between domestic and foreign financial markets since the foreign exchange crisis.\"\n",
    "\n",
    "]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"EN→KO:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "texts_ko = [\n",
    "    \"오늘 날씨는 매우 좋습니다.\",\n",
    "    \"저는 머신러닝을 공부하고 있습니다.\",\n",
    "    \"인공 지능은 세계를 변화시킵니다.\",\n",
    "    \"딥러닝은 머신 러닝의 하위 세트입니다.\",\n",
    "    \"지엠 판매원인 로베르토는 그가 매일 편지를 쓰는 오르넬라 무티를 열렬히 사랑한다.\",\n",
    "    \"여성이 남성을 대체했을 때, 그들은 남성보다 훨씬 적은 돈을 벌게 되었고 이것이 진짜 문제였습니다.\",\n",
    "    \"말 그대로 삶의 기초입니다.\",\n",
    "    \"믿기 어렵겠지만, 러시아의 인민 예술가이자 가수이자 텔레비전 진행자인 나데즈다 밥키나는 70세입니다!\",\n",
    "    \"미국에 도착한 누노는 제인 데일 해든 선생님으로 임명되었다.\",\n",
    "    \"이 중재된 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반성하고 게임이 공개되면 더 큰 인식으로 게임을 재개할 수 있습니다.\",\n",
    "    \"이러한 분석 결과는 최근의 글로벌 금융위기 기간에 잘 드러났듯이 국내외 금융 시장 간 연계성이 외환위기 이후 커진 가운데 해외 금융시장의 변동성이 국내 금융 시장에 미치는 파급효과도 보다 장기적이고 지속적으로 변화된 것을 의미한다.\"\n",
    "\n",
    "]\n",
    "inputs = tokenizer(texts_ko, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\\nKO→EN:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts_ko[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b185fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO chrF Score:\n",
      "chrF2 = 31.88\n",
      "\n",
      "KO→EN chrF Score:\n",
      "chrF2 = 77.53\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 영어 → 한국어 평가\n",
    "references_ko = [\n",
    "    \"오늘 날씨는 매우 좋습니다.\",\n",
    "    \"저는 머신러닝을 공부하고 있습니다.\",\n",
    "    \"인공지능은 세상을 변화시키고 있습니다.\",\n",
    "    \"딥러닝은 머신러닝의 하위 집합입니다.\",\n",
    "    \"지엠 판매원 로베르토는 오르넬라 무티를 열렬히 사랑하며 매일 편지를 씁니다.\"\n",
    "]\n",
    "\n",
    "hypotheses_ko = [\n",
    "    \"오늘날 날씨는 매우 좋습니다.\",\n",
    "    \"저는 기계 학습을 공부하고 있습니다.\",\n",
    "    \"인공 지능이 세상을 바꾸고 있다.\",\n",
    "    \"깊은 학습은 기계 학습의 하위 세트입니다.\",\n",
    "    \"GM 판매자 인 로베로는 매일 글을 쓰는 오넬라 무티와 미친 사랑에 빠져 있습니다.\"\n",
    "]\n",
    "\n",
    "print(\"EN→KO chrF Score:\")\n",
    "chrf_ko = sacrebleu.corpus_chrf(hypotheses_ko, [references_ko])\n",
    "print(chrf_ko)\n",
    "\n",
    "\n",
    "# 한국어 → 영어 평가\n",
    "references_en = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day.\"\n",
    "]\n",
    "\n",
    "hypotheses_en = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence changes the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a Jim-seller, loves Ornella Muti, whose letters he writes every day.\"\n",
    "]\n",
    "\n",
    "print(\"\\nKO→EN chrF Score:\")\n",
    "chrf_en = sacrebleu.corpus_chrf(hypotheses_en, [references_en])\n",
    "print(chrf_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
