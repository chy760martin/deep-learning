{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ec65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer Sentiment Classifier 감정 분류 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터 로드 & 확인: 결측치 제거(None, \"\")\n",
    "# - 2. 토크나이저 적용: Hugging Face DistilBertTokenizer 베이스 모델 사용\n",
    "# - 3. 데이터셋 -> DataLoader 변환: DistilBertTokenizer 베이스 모델 토크나이저에서 DataLoader 로 바로 변환, Custom Dataset 필요 없음\n",
    "# - 4. 모델정의 & GPU설정 & 전이학습 & 본체 동결(Feature Extraction) + LoRA Fine-tuning 조합 & EarlyStopping 클래스 정의\n",
    "# - 전이 학습: DistilBertForSequenceClassification 베이스 모델(distilbert-base-uncased), num_labels=2 긍정/부정 2개 클래스\n",
    "# - 본체 동결: model.distilbert.parameters()는 사전학습된 본체(embedding + transformer 블록)의 모든 파라미터를 의미, 따라서 학습은 classifier 레이어(pre_classifier, classifier)만 진행된다\n",
    "# - LoRA: Attention의 q_lin, v_lin 레이어에서 LoRA가 768차원 → 8차원 축소 → 768차원 복원 과정을 거쳐 업데이트를 추가하는 구조\n",
    "# - EarlyStopping: - 과적합 방지 + 최적 모델 확보 + 자원 절약 + EarlyStopping 발동 시점에서 최적 모델 가중치를 자동 저장\n",
    "# - 5. 최적화 설정 & 학습 루프 & 검증 루프 EarlyStopping 클래스 적용\n",
    "# - 최적화 설정: autocast(속도 향상) GradScaler(안정적 학습) 적용\n",
    "# - 6. 최적 모델 로드: GPU 설정, 검증/추론 모드 적용\n",
    "# - 7. 테스트 데이터 평가: 사이킷런 평가 지표 적용\n",
    "# - classification_report 정확도/정밀도/재현율/F1-socre 확인\n",
    "# - confusion_matrix 오분류 패턴 분석\n",
    "# - 8. Confusion Matrix Heatmap: Confusion Matrix를 Heatmap 그래프로 시각화 적용, 테스트 데이터 평가 데이터를 활용\n",
    "# - 9. 추론 테스트\n",
    "# - 10. FastAPI 추론 서비스\n",
    "# - /llm_app/transformer_classifier_sentiment_18_app.py\n",
    "# - FastAPI 구동: 터미널에서 구동, uvicorn transformer_classifier_sentiment_18_app:app --reload\n",
    "# - 윈도우 파워쉘: Invoke-RestMethod -Uri \"http://127.0.0.1:8000/predict\" -Method Post -ContentType \"application/json\" -Body '{\"text\":\"I really love this movie, it was fantastic!\"}'\n",
    "# - Postman app\n",
    "# - API 코드로 테스트: Python, Java...\n",
    "\n",
    "# DistilBert 구조 특징 \n",
    "# - BERT 계열 모델은 Transformer의 Encoder 부분만 사용한다\n",
    "# - 입력 문장을 임베딩 -> 여러층의 Transformer Encoder 블록 -> [CLS]토큰 벡터 추출 -> 분류기(Classifier)\n",
    "\n",
    "# DistilBertForSequenceClassification 내부 흐름\n",
    "# 입력(DataLoader 배치): \n",
    "# - input_ids:(batch_size,seq_len)->(32,128)\n",
    "# - attention_mask: (32,128)\n",
    "# - labels:(32)\n",
    "# DistilBertForSequenceClassification(\n",
    "#   (distilbert): DistilBertModel(\n",
    "#     (embeddings): Embeddings(\n",
    "#       (word_embeddings): Embedding(30522, 768, padding_idx=0) : (32,128) -> (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#       (position_embeddings): Embedding(512, 768) : (32,128) -> (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#       (dropout): Dropout(p=0.1, inplace=False) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#     )\n",
    "#     (transformer): Transformer(\n",
    "#       (layer): ModuleList(\n",
    "#         (0-5): 6 x TransformerBlock( : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#           (attention): DistilBertSdpaAttention(\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#             (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#           )\n",
    "#           (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#           (ffn): FFN( : (32,128,768) (batch_size,seq_len,hidden_dim)\n",
    "#             (dropout): Dropout(p=0.1, inplace=False)\n",
    "#             (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "#             (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "#             (activation): GELUActivation()\n",
    "#           )\n",
    "#           (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) : (32,128,768) (batch_size,seq_len,hidden_dim) 유지\n",
    "#         )\n",
    "#       )\n",
    "#     ) : Transformer 출력(output_layer_norm 이후) (32,128,768) (batch_size,seq_len,hidden_dim) shape을 가지며\n",
    "#         내부 forward()에서 hidden_states[:, 0, :] 선택([CSL] 벡터를 추출) -> (32,768) (batch_size,hidden_dim) 변경 한다\n",
    "#   ) \n",
    "#   (pre_classifier): Linear(in_features=768, out_features=768, bias=True) : (32,768) -> (32,768) (batch_size,hidden_dim)\n",
    "#   (classifier): Linear(in_features=768, out_features=2, bias=True) : (32,768)@(768,2) -> (32,2) 입력 벡터와 가중치 행렬의 내적 계산\n",
    "#   (dropout): Dropout(p=0.2, inplace=False) : (32,768) (batch_size,hidden_dim) 유지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c229bac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본체 동결(Feature Extraction) + LoRA Fine-tuning 조합\n",
    "# - 원래 q_lin / v_lin은 Linear(768 → 768) 구조: LoRA는 이 weight를 그대로 두고, 추가로 저차원 랭크 행렬을 학습\n",
    "# - Attention의 q_lin, v_lin 레이어에서 LoRA가 768차원 → 8차원 축소 → 768차원 복원 과정을 거쳐 업데이트를 추가하는 구조\n",
    "\n",
    "# trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n",
    "# PeftModelForSequenceClassification(\n",
    "#   (base_model): LoraModel(\n",
    "#     (model): DistilBertForSequenceClassification(\n",
    "#       (distilbert): DistilBertModel(\n",
    "#         (embeddings): Embeddings(\n",
    "#           (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "#           (position_embeddings): Embedding(512, 768)\n",
    "#           (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#           (dropout): Dropout(p=0.1, inplace=False)\n",
    "#         )\n",
    "#         (transformer): Transformer(\n",
    "#           (layer): ModuleList(\n",
    "#             (0-5): 6 x TransformerBlock(\n",
    "#               (attention): DistilBertSdpaAttention(\n",
    "#                 (dropout): Dropout(p=0.1, inplace=False)\n",
    "#                 (q_lin): lora.Linear(\n",
    "#                   (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
    "#                   (lora_dropout): ModuleDict(\n",
    "#                     (default): Dropout(p=0.1, inplace=False)\n",
    "#                   )\n",
    "#                   (lora_A): ModuleDict(\n",
    "#                     (default): Linear(in_features=768, out_features=8, bias=False)\n",
    "#                   )\n",
    "#                   (lora_B): ModuleDict(\n",
    "#                     (default): Linear(in_features=8, out_features=768, bias=False)\n",
    "#                   )\n",
    "#                   (lora_embedding_A): ParameterDict()\n",
    "#                   (lora_embedding_B): ParameterDict()\n",
    "#                   (lora_magnitude_vector): ModuleDict()\n",
    "#                 )\n",
    "#                 (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#                 (v_lin): lora.Linear(\n",
    "#                   (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
    "#                   (lora_dropout): ModuleDict(\n",
    "#                     (default): Dropout(p=0.1, inplace=False)\n",
    "#                   )\n",
    "#                   (lora_A): ModuleDict(\n",
    "#                     (default): Linear(in_features=768, out_features=8, bias=False)\n",
    "#                   )\n",
    "#                   (lora_B): ModuleDict(\n",
    "#                     (default): Linear(in_features=8, out_features=768, bias=False)\n",
    "#                   )\n",
    "#                   (lora_embedding_A): ParameterDict()\n",
    "#                   (lora_embedding_B): ParameterDict()\n",
    "#                   (lora_magnitude_vector): ModuleDict()\n",
    "#                 )\n",
    "#                 (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
    "#               )\n",
    "#               (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#               (ffn): FFN(\n",
    "#                 (dropout): Dropout(p=0.1, inplace=False)\n",
    "#                 (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "#                 (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "#                 (activation): GELUActivation()\n",
    "#               )\n",
    "#               (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "#             )\n",
    "#           )\n",
    "#         )\n",
    "#       )\n",
    "#       (pre_classifier): ModulesToSaveWrapper(\n",
    "#         (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
    "#         (modules_to_save): ModuleDict(\n",
    "#           (default): Linear(in_features=768, out_features=768, bias=True)\n",
    "#         )\n",
    "#       )\n",
    "#       (classifier): ModulesToSaveWrapper(\n",
    "#         (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
    "#         (modules_to_save): ModuleDict(\n",
    "#           (default): Linear(in_features=768, out_features=2, bias=True)\n",
    "#         )\n",
    "#       )\n",
    "#       (dropout): Dropout(p=0.2, inplace=False)\n",
    "#     )\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "909c366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'document', 'label'],\n",
      "    num_rows: 5000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드 & 확인\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Hugging Face datasets 라이브러리 함수(CSV,JSON.. 데이터 로드)\n",
    "dataset = load_dataset( # DatasetDict 형태로 반환\n",
    "    'csv', # csv 포멧 지정\n",
    "    data_files={ # train/test 데이터 각각 지정\n",
    "        'train': \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\",\n",
    "        'test': \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    },\n",
    "    delimiter='\\t' # 구분자, 현재 데이터에서는 탭으로 구분\n",
    ")\n",
    "\n",
    "# 결측치 제거\n",
    "all_clean_train = dataset[\"train\"].filter(lambda x: x[\"document\"] is not None and x[\"document\"].strip() != \"\")\n",
    "all_clean_test = dataset[\"test\"].filter(lambda x: x[\"document\"] is not None and x[\"document\"].strip() != \"\")\n",
    "\n",
    "# 데이터 축소\n",
    "clean_train = all_clean_train.select(range(10000))\n",
    "clean_test = all_clean_test.select(range(5000))\n",
    "\n",
    "print(clean_train)\n",
    "print(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959e7d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 9976970, 'label': 0, 'input_ids': [101, 1463, 30006, 1457, 30008, 29996, 30019, 30025, 1012, 1012, 100, 100, 1459, 30011, 30020, 29997, 30011, 29994, 30019, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "{'id': 6270596, 'label': 1, 'input_ids': [101, 100, 100, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 적용\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenizer_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"document\"],   # 문자열 리스트만 들어옴\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128 # 시퀀스 길이, 연산량 절반 이상 감소\n",
    "    )\n",
    "\n",
    "tokenized_train = clean_train.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")\n",
    "\n",
    "tokenized_test = clean_test.map(\n",
    "    tokenizer_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\"]\n",
    ")\n",
    "\n",
    "print(tokenized_train[0])\n",
    "print(tokenized_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7af3042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "torch.Size([32, 128]) torch.Size([32, 128]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 -> DataLoader 변환\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# collate_fn 함수 : Hugging Face Dataset에서 꺼낸 샘플은 파이썬 dict 형태, collate_fn() 각 샘플을 모아 PyTorch 텐서로 변환\n",
    "def collate_fn(batch): # batch 샘플 리스트 : [{'input_ids':[...],'attention_mask':[...],'label':[...]}, {...}, ...]\n",
    "    input_ids = torch.tensor([ item['input_ids'] for item in batch ]) # 토큰화된 문장\n",
    "    attention_mask = torch.tensor([ item['attention_mask'] for item in batch ]) # 패딩 여부\n",
    "    labels = torch.tensor([ item['label'] for item in batch ]) # 감성 분류 라벨(0=부정/1=긍정)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# train: 학습용 데이터, 배치크기 16, epoch 마다 데이터 순서 섞음\n",
    "train_loader = DataLoader(tokenized_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# valid: 검증용 데이터, 학습 데이터에서 10%를 검증용으로 분리 Hugging Face에서 제공하는 train_test_split() 메서드\n",
    "valid_loader = DataLoader(tokenized_train.train_test_split(test_size=0.1)['test'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "# test: 테스트 데이터, 성능 최종 평가용\n",
    "test_loader = DataLoader(tokenized_test, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# train_loader 데이터 확인\n",
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['labels'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07545c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n",
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DistilBertForSequenceClassification(\n",
      "      (distilbert): DistilBertModel(\n",
      "        (embeddings): Embeddings(\n",
      "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "          (position_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (transformer): Transformer(\n",
      "          (layer): ModuleList(\n",
      "            (0-5): 6 x TransformerBlock(\n",
      "              (attention): DistilBertSdpaAttention(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (q_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_lin): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (ffn): FFN(\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (activation): GELUActivation()\n",
      "              )\n",
      "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pre_classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (classifier): ModulesToSaveWrapper(\n",
      "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
      "        (modules_to_save): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.amp import autocast # 최신 API\n",
    "from torch.amp import GradScaler\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# DistilBertForSequenceClassification 베이스 모델(distilbert-base-uncased), num_labels=2 긍정/부정 2개 클래스\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n",
    "\n",
    "# DistilBERT 본체 동결(Feature Extraction, Parameter Freezing, Weight Freezing)\n",
    "# - model.distilbert.parameters()는 사전학습된 본체(embedding + transformer 블록)의 모든 파라미터를 의미 한다\n",
    "# - requires_grad = False로 설정하면 역전파 시 이 파라미터들은 업데이트되지 않는다\n",
    "# - 따라서 학습은 classifier 레이어(pre_classifier, classifier)만 진행된다\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# LoRA config & warp\n",
    "# - trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925, 학습 가능한 파라미터 수: 739,586(전체의 1.0925%)\n",
    "# - Attention의 q_lin, v_lin 레이어에서 LoRA가 768차원 → 8차원 축소 → 768차원 복원 과정을 거쳐 업데이트를 추가하는 구조\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_lin', 'v_lin'], # DistilBERT attention\n",
    "    bias='none',\n",
    "    task_type='SEQ_CLS'\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 모델 확인\n",
    "model.print_trainable_parameters() # LoRA 적용 확인용\n",
    "print(model)\n",
    "\n",
    "# 최적화 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 반복횟수\n",
    "num_epochs = 3\n",
    "\n",
    "# EarlyStopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/18_transformer_classifier_sentiment/best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = valid_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !=\"\" and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "            \n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'Best model saved at {self.path}')\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7d7c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 313/313 [02:02<00:00,  2.55it/s]\n",
      "1 [Valid]: 100%|██████████| 32/32 [00:04<00:00,  6.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ./llm_models/18_transformer_classifier_sentiment/best_model.pt\n",
      "Epoch 1 | Tran Loss: 0.6772 | Train Acc: 0.5605 | Valid Loss: 0.6633 | Valid Acc: 0.5870 | EarlyStopping: CONTINUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 313/313 [02:11<00:00,  2.38it/s]\n",
      "2 [Valid]: 100%|██████████| 32/32 [00:05<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ./llm_models/18_transformer_classifier_sentiment/best_model.pt\n",
      "Epoch 2 | Tran Loss: 0.6584 | Train Acc: 0.5911 | Valid Loss: 0.6540 | Valid Acc: 0.5800 | EarlyStopping: CONTINUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 313/313 [02:12<00:00,  2.37it/s]\n",
      "3 [Valid]: 100%|██████████| 32/32 [00:05<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ./llm_models/18_transformer_classifier_sentiment/best_model.pt\n",
      "Epoch 3 | Tran Loss: 0.6530 | Train Acc: 0.6002 | Valid Loss: 0.6421 | Valid Acc: 0.6200 | EarlyStopping: CONTINUE\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프: autocast(속도 향상) 적용, GradScaler(안정적 학습) 적용\n",
    "# autocast 적용: 연산을 FP16(half precision)과 FP32(full precision)중 적절히 선택해서 실행\n",
    "# - 속도 향상: 대부분의 연산을 FP16으로 처리해 GPU 연산 속도를 높인다\n",
    "# - 안정성 유지: 손실이 큰 연산(예시:소프트맥스,레이어정규화)은 FP32로 자동 변환해 정확도를 보장한다\n",
    "# GradScaler 적용: FP16 학습에서는 작은 값이 underflow(0으로 사라짐)될 위험이 있다\n",
    "# - 안정적 학습 보장: GradScaler는 손실(loss)를 크게 스케일링해서 역전파 시 그래디언트가 사라지지 않도록 한다\n",
    "# - 이후 업데이트 단계에서 다시 원래 크기로 되돌려 안정적인 학습을 보장한다. 즉 FP16 학습에서 발생할 수 있는 수치 불안정 문제를 해결하는 역할\n",
    "from tqdm import tqdm # 시각화(진행바)\n",
    "\n",
    "# DistilBert 구조 특징 \n",
    "# - BERT 계열 모델은 Transformer의 Encoder 부분만 사용한다\n",
    "# - 입력 문장을 임베딩 -> 여러층의 Transformer Encoder 블록 -> [CLS]토큰 벡터 추출 -> 분류기(Classifier)\n",
    "# DistilBertForSequenceClassification 내부 흐름\n",
    "# 1. 입력: (batch_size,seq_len)\n",
    "# 2. Encoder(DistilBERT 본체) \n",
    "# - Embedding: (batch_size,seq_len,hidden_dim) \n",
    "# - Transformer Block(6개): (batch_size,seq_len,hidden_dim)\n",
    "# 3. [CLS] 토큰 추출: (batch_size,hidden_dim)\n",
    "# 4. Classifier 레이어: (batch_size,num_labels)\n",
    "# 5. 출력: logits, loss\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # train\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1} [Train]'):        \n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "        # 학습데이터 GPU 지정\n",
    "        input_ids = batch['input_ids'].to(device) # [32, 128]\n",
    "        attention_mask = batch['attention_mask'].to(device) # [32, 128]\n",
    "        labels = batch['labels'].to(device) # [32]\n",
    "\n",
    "        # 모델 forward(autocast 영역) 내부 shape 변환\n",
    "        # - 임베딩 레이어: (32,128) -> (32,128,768) \n",
    "        # - Transformer 블록: (32,128,768) -> (32,128,768)\n",
    "        # - CLS 토큰 추출: (32,768) \n",
    "        # - Classifier 레이어: (32,768) -> (32,2) \n",
    "        # - outputs.logits.shape: (32,2) \n",
    "        # - outputs.loss: scalar\n",
    "        \n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        # with autocast('cuda'):\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): # 최신 버전 GPU에서 FP16 연산\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits # (32,2)        \n",
    "        # 오차역전차\n",
    "        scaler.scale(loss).backward() # 미분 연산\n",
    "        scaler.step(optimizer) # 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "        scaler.update()        \n",
    "        # 손실 누적\n",
    "        total_loss += loss.item()\n",
    "        # Accuracy 계산\n",
    "        preds = logits.argmax(dim=-1) # 예측 클래스 (32,)\n",
    "        correct += (preds == labels).sum().item() # 맞춘 개수\n",
    "        total += labels.size(0)    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # validation\n",
    "    model.eval() # 검증/추론 모드\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # 검증/추론 모드에서는 미분 연산 처리 하지 않음\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'{epoch + 1} [Valid]'):\n",
    "            # 검증 데이터 GPU에 할당\n",
    "            input_ids = batch['input_ids'].to(device) # (32,128) (batch_size,seq_len), 토큰화된 문장 ID(각 문장을 토큰 단위로 숫자로 변환한 것)\n",
    "            attention_mask = batch['attention_mask'].to(device) # (32,128) (batch_size,seq_len), 패딩 토큰을 무시하기 위한 마스크 1=실제 토큰 0=패딩\n",
    "            labels = batch['labels'].to(device) # (32,) (batch_size,), 각 문장의 정답 라벨(긍정=1,부정=0 클래스)\n",
    "\n",
    "            # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "            # with autocast('cuda'):\n",
    "            with autocast(device_type='cuda', dtype=torch.float16): # 최신 버전 GPU에서 FP16 연산\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits            \n",
    "            # 손실 누적\n",
    "            total_loss += loss.item()\n",
    "            # Accuracy 계산\n",
    "            # logits (32,2) (batch_size,num_labels), logits.argmax(dim=-1) (32,) 32개 각 샘플중 2개의 점수중 더 큰쪽의 인덱스(0또는1) 선택\n",
    "            preds = logits.argmax(dim=-1) # (32,)\n",
    "            # (preds == labels) 예측/정답을 비교해 True/False 벡터 생성\n",
    "            # ->.sum() True=1/False=0 계산해 맞춘 개수 합산(PyTorch 텐서)\n",
    "            # ->.item() 파이썬 숫자로 변환(파이썬 숫자 int/float)\n",
    "            correct += (preds == labels).sum().item() # (맞춘 개수 누적)\n",
    "            total += labels.size(0) # (32,)\n",
    "        # 손실 계산\n",
    "        # - 예시) 전체 검증데이터가 320개, 1배치가 32 -> 10번 배치 반복을 한다\n",
    "        # - 계산) total_loss 손실 누적 합산값 / 10(10번 배치), 즉 손실을 배치 개수로 나누어 배치 평균 손실을 구한다\n",
    "        valid_loss = total_loss / len(valid_loader)\n",
    "        valid_acc = correct / total # 정확도 계산\n",
    "\n",
    "        # Early Stopping 객체 호출\n",
    "        early_stopping(valid_loss, model)\n",
    "        status = 'STOP' if early_stopping.early_stop else 'CONTINUE' # # Early Stopping status 상태값\n",
    "\n",
    "        print(f'Epoch {epoch + 1} | Tran Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}'\n",
    "              f' | Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.4f} | EarlyStopping: {status}')\n",
    "        \n",
    "        # Early Stopping 체크\n",
    "        if early_stopping.early_stop: # early_stop=True 학습 종료\n",
    "            print('Early stopping triggered')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e98212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): DistilBertForSequenceClassification(\n",
       "      (distilbert): DistilBertModel(\n",
       "        (embeddings): Embeddings(\n",
       "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (transformer): Transformer(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x TransformerBlock(\n",
       "              (attention): DistilBertSdpaAttention(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (q_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_lin): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (ffn): FFN(\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (activation): GELUActivation()\n",
       "              )\n",
       "              (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pre_classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적 모델 로드\n",
    "\n",
    "# torch.load() 파일에서 파라미터(가중치) 딕셔너리를 불러옴\n",
    "# model.load_state_dict() 불러온 파리미터를 모델 구조에 맞게 적용\n",
    "model.load_state_dict(torch.load('./llm_models/18_transformer_classifier_sentiment/best_model.pt'))\n",
    "\n",
    "# 모델을 실행할 디바이스(GPU or CPU)에 올린다\n",
    "model.to(device)\n",
    "\n",
    "# 검증/추론 모드 전환 \n",
    "# - model.eval() 검증/추촌 모드 에서는 Dropout 등이 비활성화되어 일관된 추론 결과를 보장한다, \n",
    "# - model.train() 학습 모드 에서는 Dropout, BatchNorm 등이 활성화되어 파라미터 업데이트를 준비 한다\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb7681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 [Test]: 100%|██████████| 157/157 [00:28<00:00,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.61      0.57      0.59      2478\n",
      "    positive       0.60      0.64      0.62      2522\n",
      "\n",
      "    accuracy                           0.61      5000\n",
      "   macro avg       0.61      0.61      0.61      5000\n",
      "weighted avg       0.61      0.61      0.61      5000\n",
      "\n",
      "[[1415 1063]\n",
      " [ 898 1624]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "\n",
    "# 사이킷런 평가 지표 함수 라이브러리 호출\n",
    "# - classification_report 정확도/정밀도/재현율/F1-socre 확인\n",
    "# - confusion_matrix 오분류 패턴 분석\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# 전체 예측값, 실제 라벨을 저장 리스트 초기화\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "# 검증/추론시에는 역전파(gradient 계산)가 필요 없으므로 메모리/연산 절약\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=f'{epoch + 1} [Test]'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        # with autocast('cuda'):\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): # 최신 버전 GPU에서 FP16 연산\n",
    "            # 모델 예측\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # 각 샘플에서 가장 큰 점수의 인덱스(0=Negative, 1=Positive) 선택\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy() # PyTorch Tensor -> numpy 변환\n",
    "        labels = labels.cpu().numpy() # PyTorch Tensor -> numpy 변환\n",
    "\n",
    "        all_preds.extend(preds) \n",
    "        all_labels.extend(labels)\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=['Negative', 'positive']))\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6a4883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion Matrix를 Heatmap 그래프로 시각화, 테스트 데이터 평가 데이터를 활용\n",
    "import matplotlib.pyplot as plt # 그래프 그리기 기본 라이브러리\n",
    "import seaborn as sns # 시각화 스타일을 더 깔끔하게 만들어주는 라이브러리\n",
    "import numpy as np # Confunsion Matrix 데이터를 배열 형태로 관리\n",
    "\n",
    "# Confusion Matrix 데이터, 2x2 행렬 구조 실제 라벨과 예측 라벨의 매칭 결과\n",
    "# cm = np.array([ [1562, 916],\n",
    "#                [973, 1549] ])\n",
    "cm = np.array(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# 클래스 이름\n",
    "labels = ['Negative', 'Positive']\n",
    "\n",
    "# Heatmap 시각화\n",
    "plt.figure(figsize=(6, 5))\n",
    "# cm 데이터, annot=True 각 셀에 숫자 표시, fmt='d' 정수 형태로 출력, cmap='Blues' 파란색 계열 색상 맵 선택, xticklabels/yticklabels 축 라벨에 클래스 이름 푝시\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a48b8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I really love this movie, it was fantastic!\n",
      "Prediction: Positive\n",
      "\n",
      "Text: This product is terrible and I will never buy it again.\n",
      "Prediction: Positive\n",
      "\n",
      "Text: The service was okay, not too bad but not great either.\n",
      "Prediction: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추론 함수\n",
    "\n",
    "def predict(text, tokenizer, model, device):\n",
    "    model.eval() # 검증/추론 모드 전환\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    # 검증/추론시 미분 연산 하지 않음\n",
    "    with torch.no_grad():\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        with autocast(device_type='cuda', dtype=torch.float16): # GPU에서 FP16 연산으로 추론\n",
    "            outputs = model(**inputs) # 모델 예측\n",
    "            pred = outputs.logits.argmax(dim=-1).item()\n",
    "    return 'Positive' if pred == 1 else 'Negative'\n",
    "\n",
    "# 추론 테스트\n",
    "test_texts = [\n",
    "    \"I really love this movie, it was fantastic!\",\n",
    "    \"This product is terrible and I will never buy it again.\",\n",
    "    \"The service was okay, not too bad but not great either.\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    result = predict(text, tokenizer, model, device)\n",
    "    print(f'Text: {text}\\nPrediction: {result}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'Positive'}\n"
     ]
    }
   ],
   "source": [
    "# FastAPI 추론 서비스\n",
    "# - /llm_app/transformer_classifier_sentiment_18_app.py\n",
    "# - FastAPI 구동: 터미널에서 구동, uvicorn transformer_classifier_sentiment_18_app:app --reload\n",
    "# - 윈도우 파워쉘: Invoke-RestMethod -Uri \"http://127.0.0.1:8000/predict\" -Method Post -ContentType \"application/json\" -Body '{\"text\":\"I really love this movie, it was fantastic!\"}'\n",
    "# - Postman app\n",
    "# - API 코드로 테스트: Python, Java...\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/predict\"\n",
    "data = {\"text\": \"I really love this movie, it was fantastic!\"}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
