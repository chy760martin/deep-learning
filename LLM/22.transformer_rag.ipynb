{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer RAG(Retriever Augmentation Generation) 구성 및 모델 파이프라인 구축\n",
    "# - Retrieval 단계: 검색엔진이나 벡터DB(예: FAISS, Milvus, Weaviate)를 통해 관련 문서를 빠르게 찾아냄\n",
    "# - Augmentation 단계: 검색된 문서를 기반으로 LLM 입력 프롬프트를 강화 → 맥락 있는 답변 생성\n",
    "# - Generation 단계: LLM이 최종 답변을 생성 → 단순 생성보다 정확성과 신뢰성이 높아짐\n",
    "\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f363a074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b857f1fbec024e8eb8ec0f687d1c0edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mElectraForQuestionAnswering LOAD REPORT\u001b[0m from: monologg/koelectra-base-v3-finetuned-korquad\n",
      "Key                             | Status     |  | \n",
      "--------------------------------+------------+--+-\n",
      "electra.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a33054890d44ae875ac29ed732ac82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2d2963d50a480e96123436f5197bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RAG 구성도 및 모델 파이프라인 검토\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "\n",
    "# QA 모델( KoELECTRA KorQuAD)\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"monologg/koelectra-base-v3-finetuned-korquad\")\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=qa_model,\n",
    "    tokenizer=qa_tokenizer\n",
    ")\n",
    "\n",
    "# 요약 모델(KoBART)\n",
    "kobart_tokenizer = BartTokenizer.from_pretrained(\"gogamza/kobart-summarization\")\n",
    "kobart_model = BartForConditionalGeneration.from_pretrained(\"gogamza/kobart-summarization\")\n",
    "\n",
    "# LLM 모델(GPT 계열 API 호출 또는 파인튜닝된 모델)\n",
    "# - 선택 필요\n",
    "\n",
    "\n",
    "# Hugging Face pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# 테스트 성능 미달\n",
    "# llm_tokenizer = AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "# 리소스 부족\n",
    "# llm_tokenizer = AutoTokenizer.from_pretrained(\"beomi/KoAlpaca-Polyglot-12.8B\")\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained(\"beomi/KoAlpaca-Polyglot-12.8B\")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/polyglot-ko-1.3b\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/polyglot-ko-1.3b\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",   # GPU 메모리에 맞게 자동 배치\n",
    "    offload_folder=\"offload\"  # 일부 레이어를 CPU로 오프로딩\n",
    ")\n",
    "\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=llm_model,\n",
    "    tokenizer=llm_tokenizer,\n",
    "    return_full_text=False # 프롬프트 제외\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed8e23da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fc885f5f984135b9a404ff24654f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search_result: AI는 의료, 금융 등 다양한 분야에서 활용되고 있다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=150) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA 결과: 의료, 금융\n",
      "문서 요약: AI는 의료, 금융 등 다양한 분야에서 활용되고 있다...\n",
      "최종 답변: AI는 의료, 금융 등 다양한 분야에서 활용되고 있다.​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​​\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인 실행\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩 모델 로드\n",
    "# - 여기서는 paraphrase-multilingual-MiniLM-L12-v2 모델을 사용했는데, 다국어(한국어 포함) 문장 의미를 잘 반영하는 임베딩을 생성한다\n",
    "# - 문장을 입력하면 의미 공간에서 가까운 벡터로 변환\n",
    "embedder = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# 문서 벡터화 및 DB 구축\n",
    "documents = [ # 검색 대상이 되는 문서 리스트\n",
    "    \"최근 AI 분야에서는 RAG 시스템이 각광받고 있으며...\",\n",
    "    \"AI는 의료, 금융 등 다양한 분야에서 활용되고 있다...\"\n",
    "]\n",
    "# 각 문서를 벡터로 변환: 예시) \"AI는 의료, 금융...\" → [0.12, -0.34, ...]\n",
    "doc_embeddings = embedder.encode(documents)\n",
    "\n",
    "# Faiss 인덱스 구축\n",
    "# - faiss.IndexFlatL2() L2 거리(유클리드 거리) 기반 최근접 검색 인덱스, doc_embeddings.shape[1] 벡터 차원 수를 지정\n",
    "# - faiss 인덱스를 만들때는 벡터의 차원수를 지정해야 한다, doc_embeddings.shape (문서개수, 임베딩 차원수)\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1]) # doc_embeddings.shape[1] (임베딩 차원수) (예시 384)\n",
    "# faiss index.add() 는 (N 벡터개수, d 벡터 차원수) 형태의 float32 넘파이 배열을 기대한다 \n",
    "index.add(np.array(doc_embeddings))\n",
    "\n",
    "# 검색 단계\n",
    "query = \"AI 최신 동향은 무엇인가요?\"\n",
    "query_embedding = embedder.encode([query]) # (1, 384) 형태의 벡터, 질의 벡터\n",
    "\n",
    "# index.search()는 질의 벡터와 인덱스에 저장된 문서 벡터들 사이의 L2 거리(유클리드 거리)를 계산\n",
    "# - D: 거리 값들(작을수록 더 가까움), I: 가장 가까운 벡터의 인덱스 번호\n",
    "D, I = index.search(np.array(query_embedding), k=1)\n",
    "# 예시) documents = [\"문서0\", \"문서1\"]\n",
    "search_result = documents[I[0][0]] # I는 가장 가까운 벡터의 인덱스 번호에 맞는 documents[I[0][0]] 문서 인덱스 와 문서\n",
    "print(\"search_result:\", search_result)\n",
    "\n",
    "# QA 단계\n",
    "qa_result = qa_pipeline(\n",
    "    question=query,\n",
    "    context=search_result\n",
    ")\n",
    "answer_candidate = qa_result[\"answer\"]\n",
    "print(\"QA 결과:\", answer_candidate)\n",
    "\n",
    "# 요약 단계(조건부)\n",
    "if len(search_result.split()) > 50:\n",
    "    inputs = kobart_tokenizer(\n",
    "        search_result,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "    summary_ids = kobart_model.generate(\n",
    "        inputs=inputs[\"input_ids\"],\n",
    "        max_length=128,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary_text = kobart_tokenizer.decode(\n",
    "        summary_ids[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "else:\n",
    "    summary_text = search_result\n",
    "print(\"문서 요약:\", summary_text)\n",
    "\n",
    "# LLM 최종 생성\n",
    "# llm_input = f\"질문: {query}\\n문서요약: {summary_text}\\nQA 모델 정답 후보: {answer_candidate}\\n위 정보를 바탕으로 사용자의 질문에 대해 간결하고 정확한 답변을 작성하세요.\"\n",
    "llm_input = f\"\"\"\n",
    "{query}\n",
    "{summary_text}\n",
    "\n",
    "위 정보를 참고해 한두 문장으로 답변하세요.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "raw_output = llm_pipeline(\n",
    "    llm_input,\n",
    "    max_new_tokens=150,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=False\n",
    ")[0][\"generated_text\"]\n",
    "\n",
    "# 불필요한 키워드 제거\n",
    "for kw in [\"질문:\", \"문서 요약:\", \"요약:\"]:\n",
    "    raw_output = raw_output.replace(kw, \"\")\n",
    "\n",
    "final_answer = raw_output.strip()\n",
    "print(\"최종 답변:\", final_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
