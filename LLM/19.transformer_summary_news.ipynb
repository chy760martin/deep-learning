{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae39f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer News Summary 요약 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터 로드 & 확인: 결측치 제거(None, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ca1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 및 json 파일 추출\n",
    "import os\n",
    "import json\n",
    "\n",
    "data_dir = './llm_data/ai_hub_news_summary' # 데이터셋 경로\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for file_name in os.listdir(data_dir): # 파일명 추출\n",
    "    if file_name.endswith('.json'): # .json 파일 추출\n",
    "        with open(os.path.join(data_dir, file_name), 'r', encoding='utf-8') as f: # 파일 일기\n",
    "            data = json.load(f) # json 파일 로드\n",
    "            all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f384e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 파싱: 본문과 요약 추출\n",
    "dataset = [] # 최종적으로 본문과 요약문을 담는 리스트\n",
    "\n",
    "for item in all_data: # JSON 데이터셋    \n",
    "    passage = item['Meta(Refine)']['passage'] # 본문\n",
    "\n",
    "    summaries = [] # 요약문\n",
    "    if item['Annotation'].get('summary1'):\n",
    "        summaries.append(item['Annotation'].get('summary1'))\n",
    "    if item['Annotation'].get('summary2'):        \n",
    "        summaries.append(item['Annotation'].get('summary2'))\n",
    "\n",
    "    # 요약이 없는 경우 제외\n",
    "    if passage and summaries:\n",
    "        dataset.append({'text': passage, 'summary': summaries})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6658353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10800 {'text': '보수진영 사분오열 속 ‘국민통합연대’ 띄운 비박계 크리스마스를 앞둔 지난 23일 오전 서울 프레스센터 국제회의실.   보수분열 극복을 내건 ‘국민통합연대’가 창립대회를 열었다.   햇살 없이 착 가라앉은 날씨에 동지 바람이 매서웠지만 행사장 안은 달아올랐다.   문재인 정권을 향한 맹폭격이 이어졌고 ‘무능, 기만의 오만방자한 정권에 사망을 선고한다’는 창립선언문이 나왔다.   홍준표 전 자유한국당 대표 등 전현직 의원 20여명을 포함해 500여명이 자리를 빼곡하게 메웠다.   총선이 불과 석달 남짓이다.   야권 인사들이 정권을 두들겨 패는 거야 이상한 일이 아니다.   눈 여겨 볼 대목은 모인 사람이 대부분 친이·비박계(친이명박·비박근혜) 인사들이란 점이다.   박관용 전 국회의장, 이문열 작가와 함께 보수쪽 명망가 여럿이 이름을 올리고 더러 참석했다.   전광훈 목사는 축사를 했다.   그래도 이명박 정권서 요직을 맡았던 사람들이 주축이다.   이재오 중앙집행위원장과 홍준표 전 대표가 한가운데 있다.   두 사람은 ‘친박 그룹’에 둘러싸인 황교안 대표와 한국당에 불편한 기색을 감추지 않는 중이다.   홍 전 대표는 이튿날 “무기력한 야당만 믿고 따르기엔 너무 답답하고 앞날이 보이지 않아 창립한 게 국민통합연대”란 글을 올렸다.   31일엔 “한국당 지도부는 총사퇴하고 비상대책위를 꾸려야 한다”고 황 대표 사퇴를 요구했다.   이재오 위원장은 지난 10월3일 광화문의 조국 규탄집회장에서 “자유한국당은 집회에서 빠지라”고 외쳤다.   가뜩이나 뿔뿔이 흩어진 각자도생의 보수세력이다.   한국당과 우리공화당에다 새로운 보수당, 이언주 신당, 이정현 신당이 나올 판이다.   게다가 개정선거법의 준연동형 비례대표제는 군소정당에 유리한 분열요인이다.   중앙선관위에 등록된 정당이 34개인데 창당준비위원회를 설립한 예비정당만 16개에 달한다.   야권 빅텐트를 외칠만한 상황이긴 하다.   그런데 통합을 내건 이재오 위원장은 “어느 한 정당이나 단체 중 힘 있는 정당, 단체를 중심으로 뭘 하자는 식의 통합은 어렵다”고 주장했다.   황교안 대표와 한국당 중심의 보수 통합론을 가로막고 나선 셈이다.   그렇다고 ‘힘 있는’ 한국당이 ‘힘 없는’ 국민통합연대의 주문에 따를 리는 없다.   결국 친이계가 떨어져 나가는 ‘보수 4분열’이란 해석이 나왔다.   정말 그럴까.   국민통합연대는 조만간 친이 비박 신당으로 탈바꿈할 것인가.', 'summary': ['국민통합연대가 연 창립대회에 자유한국당 홍 전 대표 등 의원 20여 명을 포함한 500여 명이 자리를 빼곡하게 매웠으며 축사는 전 목사가 했다.', '보수분열 극복을 내건 ‘국민통합연대’가 창립대회를 열었다. 홍준표 전 자유한국당 대표 등 전현직 의원 20여명을 포함해 500여명이 자리를 빼곡하게 메웠다. 전광훈 목사는 축사를 했다.']}\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "def clean_text(text):\n",
    "    # 줄바꿈 -> 공백으로 바꾼다, 문장열 앞뒤 불필요한 공백 제거\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    return text\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['text'] = clean_text(sample['text']) # 문자열\n",
    "    sample['summary'] = [ clean_text(s) for s in sample['summary'] ] # 리스트\n",
    "\n",
    "# 데이터 확인\n",
    "print(len(dataset), dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8501602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10800, 1024]) torch.Size([10800, 128])\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 적용 데이터셋 -> DataLoader 생성, collate_fn 적용\n",
    "from transformers import BartTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# BART 토크라이저 로드, Bart vocab size: 50265\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "# print('Bart vocab size:', tokenizer.vocab_size)\n",
    "\n",
    "# collate_fn 정의\n",
    "def collate_fn(batch):\n",
    "    texts = [ item['text'] for item in batch ]\n",
    "    summaries = [ ' '.join(item['summary']) for item in batch ]\n",
    "\n",
    "    # 입력과 요약을 토큰화\n",
    "    inputs = tokenizer(\n",
    "        [ sample['text'] for sample in dataset ], # 본문 리스트\n",
    "        max_length=1024, # BART 입력 최대 길이\n",
    "        truncation=True,\n",
    "        padding='max_length', # padding='max_length' max_length까지 모든 문장을 강제 패딩, padding=True 배치 내에서 가장 긴 문장 길이에 맞추어 패딩\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    labels = tokenizer(\n",
    "        [ ' '.join(sample['summary']) for sample in dataset ], # 요약 리스트를 하나의 문자열로 합치기\n",
    "        max_length=128, # 요약 최대 길이\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': labels['input_ids']        \n",
    "    }\n",
    "\n",
    "# DataLoader 생성(dataset은 리스트)\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 배치 확인\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape, batch['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 408,649,728 || trainable%: 0.5773\n",
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BartForConditionalGeneration(\n",
      "      (model): BartModel(\n",
      "        (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "        (encoder): BartEncoder(\n",
      "          (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "          (layers): ModuleList(\n",
      "            (0-11): 12 x BartEncoderLayer(\n",
      "              (self_attn): BartAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (activation_fn): GELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (decoder): BartDecoder(\n",
      "          (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "          (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "          (layers): ModuleList(\n",
      "            (0-11): 12 x BartDecoderLayer(\n",
      "              (self_attn): BartAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (activation_fn): GELUActivation()\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): BartAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (q_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=1024, out_features=16, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.amp import autocast, GradScaler\n",
    "import os\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# BartForConditionalGeneration 베이스 모델\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Feature Extraction\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name: # LoRA 모듈이 아닌 경우\n",
    "        param.requires_grad = False # 모델 본체 동결\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 모델 확인, trainable params: 2,359,296 || all params: 408,649,728 || trainable%: 0.5773\n",
    "model.print_trainable_parameters() # LoRA 적용 확인\n",
    "print(model)\n",
    "\n",
    "# 최적화 설정\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 반복횟수\n",
    "num_epochs = 3\n",
    "\n",
    "# Early Stopping 클래스 정의\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/19_transformer_summary_news/best_model.pt'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = valid_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !='' and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        \n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f'Best model saved at {self.path}')\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
