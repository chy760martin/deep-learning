{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094ec65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 생성형 모델 & 파인 튜닝\n",
    "# - Hugging Face 라이브러리 적용\n",
    "# - AI HUB 금융 분야 다국어 병렬 말뭉치 데이터셋 적용\n",
    "# - 입력된 한국어 문장을 생성형 모델을 한국어 문장 생성\n",
    "# 1. 학습 목표\n",
    "# - 구조 최적화 및 파이프라인 단순화\n",
    "# - AI HUB 금융 분야 다국어 병렬 말뭉치 데이터셋 전처리\n",
    "# - 금융 학술논문 데이터셋 변환 전처리\n",
    "# - 토크나이징 및 토크나이징 전처리\n",
    "# - 베이스 모델 로드\n",
    "# - LoRA(Low-Rank Adaptation) 설정, 특정 레이어에 작은 저차원 행렬(랭크 r)을 삽입해서 학습\n",
    "# - LoRA(Low-Rank Adaptation) 모델, 메모리 효율성/빠른 학습/도메인 적용, base 모델에 여러 LoRA 모듈을 붙였다 떼었다 할 수 있음\n",
    "# - 학습 args 설정\n",
    "# - Trainer 정의\n",
    "# - Trainer 실행\n",
    "# - LoRA 적용된 모델 저장, LoRA모델/토크나이저\n",
    "# - LoRA 적용된 모델 불러오기, 베이스모델/LoRA모델/토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b9054a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장 개수: 80883\n",
      "2019년부터 전년말 기준 자산총액 2조 원 이상인 기업에 대해 의무화하였으며, 2020년부터는 전년말 기준 자산총액 5천 억 원 이상 기업까지 의무대상을 확대했다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 전처리 - AI HUB 금융 분야 다국어 병렬 말뭉치 데이터셋\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "ko_lines = []\n",
    "folders = [\n",
    "    './llm_data/ai_hub_article/*.json'\n",
    "]\n",
    "\n",
    "# 모든 JSON 읽기\n",
    "for folder in folders:\n",
    "    for path in glob.glob(folder): # 특정 디렉토리에서 지정한 패턴과 일치하는 모든 파일 경로를 리스트로 반환\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f) # 파일 전체 로드(dict 구조)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        # sents 리스트에서 문장 추출\n",
    "        for sent in data.get('sents', []):\n",
    "            sentences = sent.get('source_cleaned') # 최종번역문(한국어) 추출\n",
    "            if sentences != 'N/A':\n",
    "                ko_lines.append(sentences.strip())\n",
    "\n",
    "# 특수문자/공백 전처리\n",
    "def detokenize_sentence(sentence: str) -> str:\n",
    "    sentence = sentence.strip() # 양쪽 공백 처리\n",
    "    sentence = re.sub(r\"\\s+([?.!,])\", r\"\\1\", sentence)  # \" ?\" → \"?\"\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)            # 여러 공백 → 하나\n",
    "    return sentence\n",
    "\n",
    "# 데이터셋 전처리\n",
    "ko_lines = [ detokenize_sentence(s) for s in ko_lines ]\n",
    "\n",
    "print(f'총 문장 개수: {len(ko_lines)}')\n",
    "print(ko_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acba1292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 존재하는 폴더: ./llm_data/ai_hub_article_llm\n",
      "총 문장 개수: 80868\n",
      "데이터셋 저장 완료\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 중복 문장 제거(set()), 너무 짧은 문장(3~4자) 제거, 너무 긴 문장 잘라내기(gpt-2는 max_length=1024 제한), 학습/검증/테스트 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "out_dir = './llm_data/ai_hub_article_llm'\n",
    "# 폴더 없을시 생성\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f'폴더 생성 완료: {out_dir}')\n",
    "else:\n",
    "    print(f'이미 존재하는 폴더: {out_dir}')\n",
    "\n",
    "# 중복 제거\n",
    "ko_lines = list(set(ko_lines))\n",
    "\n",
    "# 최소 길이 필터링(10자 이상만 사용)\n",
    "ko_lines = [ s for s in ko_lines if len(s) > 10 ]\n",
    "\n",
    "# train/validation/test 분리\n",
    "train, test = train_test_split(ko_lines, test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "# 저장\n",
    "with open(f'{out_dir}/train.txt', 'w', encoding='utf-8') as f: # train\n",
    "    for line in train:\n",
    "        f.write(line + '\\n')\n",
    "with open(f'{out_dir}/val.txt', 'w', encoding='utf-8') as f: # validation\n",
    "    for line in val:\n",
    "        f.write(line + '\\n')\n",
    "with open(f'{out_dir}/test.txt', 'w', encoding='utf-8') as f: # test\n",
    "    for line in test:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f'총 문장 개수: {len(ko_lines)}')        \n",
    "print('데이터셋 저장 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c5d9984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '본 연구는 이 모형을 통해 적합한 자산배분을 제시하려는 것이 아니라, 다양한 자본시장 환경을 감안하여 현재 재정추계에서 가정하고 있는 요구수익률이 합리적인지를 점검하는 데 일차적 목적을 두었기 때문이다.'}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 생성\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'text',\n",
    "    data_files={\n",
    "        'train': './llm_data/ai_hub_article_llm/train.txt',\n",
    "        'validation': './llm_data/ai_hub_article_llm/val.txt',\n",
    "        'test': './llm_data/ai_hub_article_llm/test.txt'\n",
    "    }\n",
    ")\n",
    "\n",
    "# train/validation/test 각각에서 100개만 추출\n",
    "small_train = dataset['train'].select(range(20000))\n",
    "small_val = dataset['validation'].select(range(2000))\n",
    "small_test = dataset['test'].select(range(2000))\n",
    "\n",
    "small_dataset = DatasetDict({\n",
    "    'train': small_train,\n",
    "    'validation': small_val,\n",
    "    'test': small_test\n",
    "})\n",
    "\n",
    "print(small_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f1fedbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817610c62f8741ae9780d92c777827b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c4fddf1b2f4435bbd601ab0f3c372d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67f9077a42c40a68b04dc7326da7645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '본 연구는 이 모형을 통해 적합한 자산배분을 제시하려는 것이 아니라, 다양한 자본시장 환경을 감안하여 현재 재정추계에서 가정하고 있는 요구수익률이 합리적인지를 점검하는 데 일차적 목적을 두었기 때문이다.'}\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 및 데이터셋 생성\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "# 별도의 [PAD] 토큰을 추가, 모델의 임베딩을 확장해야 하므로, 학습 전에 반드시 model.resize_token_embeddings(len(tokenizer)) 호출 필요\n",
    "tokenizer.add_special_tokens( {'pad_token': '[PAD]'} )\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "    return tokens\n",
    "\n",
    "# tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "# print(tokenized_dataset['train'][0])\n",
    "\n",
    "tokenized_dataset = small_dataset.map(tokenize_function, batched=True)\n",
    "print(small_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11a9a70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드, 임베딩 확장, LoRA 설정\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 모델 로드 및 임베딩 확장\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['c_attn'], # GPT-2 Attention 모듈\n",
    "\n",
    "    lora_dropout=0.1,\n",
    "    bias='none',\n",
    "    task_type='CAUSAL_LM'\n",
    ")\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94e32936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터 정의\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_models/results_lora_llm_gpt2\",    # 학습 결과(모델, 체크포인트) 저장 경로\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./llm_models/results_lora_logs_llm_gpt2\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    fp16=True,  # GPU 메모리 절약\n",
    "    learning_rate=5e-5,   # ← 학습률 추가\n",
    "    load_best_model_at_end=True                     # 최적 모델 자동 로드\n",
    "\n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # 개선 없으면 조기 종료\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f8809786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 1:14:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.728300</td>\n",
       "      <td>2.653009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.645700</td>\n",
       "      <td>2.600876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.686600</td>\n",
       "      <td>2.587525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=2.7541090494791667, metrics={'train_runtime': 4456.6574, 'train_samples_per_second': 13.463, 'train_steps_per_second': 0.841, 'total_flos': 3932970024960000.0, 'train_loss': 2.7541090494791667, 'epoch': 3.0})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c1d1776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./llm_models/llm_gpt2_lora\\\\tokenizer_config.json',\n",
       " './llm_models/llm_gpt2_lora\\\\special_tokens_map.json',\n",
       " './llm_models/llm_gpt2_lora\\\\vocab.json',\n",
       " './llm_models/llm_gpt2_lora\\\\merges.txt',\n",
       " './llm_models/llm_gpt2_lora\\\\added_tokens.json',\n",
       " './llm_models/llm_gpt2_lora\\\\tokenizer.json')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 모델 저장\n",
    "model.save_pretrained(\"./llm_models/llm_gpt2_lora\", safe_serialization=False)\n",
    "\n",
    "# 토크나이저 저장\n",
    "tokenizer.save_pretrained(\"./llm_models/llm_gpt2_lora\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# 1) 토크나이저: 학습 때 저장한 것을 불러옴 (vocab=50258)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./llm_models/llm_gpt2_lora\")\n",
    "\n",
    "# 2) base GPT-2 불러오고, 임베딩을 50258로 확장\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))  # 50258로 확장\n",
    "\n",
    "# 3) LoRA 어댑터를 base_model 위에 로드\n",
    "model = PeftModel.from_pretrained(base_model, \"./llm_models/llm_gpt2_lora\")\n",
    "\n",
    "# 4) 파이프라인으로 생성 테스트\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "prompt = \"2025년 금융 규제 변화에 따라\"\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_new_tokens=120,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,   # 반복 억제\n",
    "    no_repeat_ngram_size=3    # 3-gram 반복 방지\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
