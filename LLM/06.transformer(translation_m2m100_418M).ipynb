{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face 라이브러리 적용 - 기계 번역 모델\n",
    "# AI HUB 방송 다국어 번역 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bf0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129 cuda\n",
      "CUDA 사용 가능 여부: True\n",
      "PyTorch CUDA 버전: 12.9\n",
      "빌드 정보: 2.8.0+cu129\n",
      "사용 중인 GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob, json, re, os, random, csv\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__, device)\n",
    "\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"빌드 정보:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49046719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문장쌍 개수: 121124, 121124\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 전처리\n",
    "ko_lines, en_lines = [], []\n",
    "folders = [ # 폴더 리스트 정의\n",
    "    './llm_data/aihub_broadcast_translation/aihub_documentary_esko_translation/*.json',\n",
    "    './llm_data/aihub_broadcast_translation/aihub_etc_esko_translation/*.json',\n",
    "    './llm_data/aihub_broadcast_translation/aihub_movie_esko_translation/*.json'\n",
    "]\n",
    "\n",
    "# 모든 JSON 읽기\n",
    "for folder in folders:\n",
    "    for path in glob.glob(folder): # 특정 디렉토리에서 지정한 패턴과 일치하는 모든 파일 경로를 리스트로 반환\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f) # 파일 전체 로드(dict 구조)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # 원문(영어), 최종번역문(한국어) 추출\n",
    "            en = data.get('원문')\n",
    "            ko = data.get('최종번역문')\n",
    "\n",
    "            if en and ko and ko != 'N/A':\n",
    "                en_lines.append(en.strip())\n",
    "                ko_lines.append(ko.strip())\n",
    "\n",
    "# 1. Detokenize 함수 정의\n",
    "def detokenize_sentence(sentence: str) -> str:\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"\\s+([?.!,])\", r\"\\1\", sentence)  # \" ?\" → \"?\"\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)            # 여러 공백 → 하나\n",
    "    return sentence\n",
    "\n",
    "# 2. 데이터셋 전처리\n",
    "en_lines = [detokenize_sentence(s) for s in en_lines]\n",
    "ko_lines = [detokenize_sentence(s) for s in ko_lines]\n",
    "\n",
    "\n",
    "print(f'총 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "879c0f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 후 문장쌍 개수: 121115, 121115\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 중복 제거 및 순서 유지\n",
    "# pairs = list(set(zip(en_lines, ko_lines)))\n",
    "# en_lines, ko_lines = zip(*pairs) # 다시 분리\n",
    "seen = set()\n",
    "pairs = []\n",
    "for en, ko in zip(en_lines, ko_lines):\n",
    "    if (en, ko) not in seen:\n",
    "        # 새로운 문장쌍을 집합에 기록, 이후 같은 문장쌍이 나오면 if 조건에서 걸러져 추가되지 않는다\n",
    "        seen.add( (en, ko) )\n",
    "        \n",
    "        # 중복이 아닌 문장쌍을 리스트에 추가, 원래 순서대로 중복 없는 문장쌍 리스트가 만들어 진다\n",
    "        # - pairs는 [(\"Hello\",\"안녕\"), (\"Goodbye\",\"잘가\")] \n",
    "        pairs.append( (en, ko) )\n",
    "\n",
    "# 이를 다시 분리 - 영어 문장들만 모아 (\"Hello\",\"Goodbye\"), 한국어 문장들만 모아 (\"안녕\",\"잘가\")\n",
    "en_lines, ko_lines = zip(*pairs)\n",
    "\n",
    "print(f'중복 제거 후 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc2927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플링 후 문장쌍 개수: 100000, 100000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 샘플링 추가\n",
    "\n",
    "# 샘플링 최대 50,000 문장만 사용\n",
    "# sample_size = 50000\n",
    "sample_size = 100000\n",
    "if len(ko_lines) > sample_size:\n",
    "    indices = random.sample(range(len(ko_lines)), sample_size)\n",
    "    ko_lines = [ ko_lines[i] for i in indices ]\n",
    "    en_lines = [ en_lines[i] for i in indices ]\n",
    "\n",
    "print(f'샘플링 후 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "756e86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 존재하는 폴더: ./llm_data/aihub_broadcast_translation\n",
      "저장 완료 ./llm_data/aihub_broadcast_translation/train_ko.txt ./llm_data/aihub_broadcast_translation/train_en.txt\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - 저장\n",
    "out_dir = './llm_data/aihub_broadcast_translation'\n",
    "\n",
    "# 폴더 없을시 생성\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f'폴더 생성 완료: {out_dir}')\n",
    "else:\n",
    "    print(f'이미 존재하는 폴더: {out_dir}')\n",
    "\n",
    "ko_path = f'{out_dir}/train_ko.txt'\n",
    "en_path = f'{out_dir}/train_en.txt'\n",
    "\n",
    "with open(ko_path, 'w', encoding='utf-8') as fko, \\\n",
    "    open(en_path, 'w', encoding='utf-8') as fen:\n",
    "    for k, e in zip(ko_lines, en_lines):\n",
    "        fko.write(k + '\\n')\n",
    "        fen.write(e + '\\n')\n",
    "print('저장 완료', ko_path, en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b55675c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# AI Hub 방송 데이터셋(train_ko.txt, train_en.txt) -> CSV로 변환해 파인 튜닝\n",
    "# 원본 데이터는 train_ko.txt, train_en.txt로 분리 -> 병렬 문장쌍을 만들어야 함\n",
    "# 파인튜닝시 양방향 번역을 지원하려면 같은 문장쌍은 en->ko, ko->en 두방향으로 모두 포함해야 함\n",
    "# CSV 구조 예시\n",
    "# src,tgt,src_lang,tgt_lang\n",
    "# You can buy it from a convenience store try it out.,편의점에서 사실 수 있으니 시도해보시길 바랍니다.,en,ko\n",
    "# 편의점에서 사실 수 있으니 시도해보시길 바랍니다.,You can buy it from a convenience store try it out.,ko,en\n",
    "# She frees him and takes him as her navigator.,그녀는 그를 풀어주고 조종자로써 그를 데려간다.,en,ko\n",
    "# 그녀는 그를 풀어주고 조종자로써 그를 데려간다.,She frees him and takes him as her navigator.,ko,en\n",
    "# Iyengar's belief in Gandhi's philosophy is so deep that he can't even open his laptop without remorse.,간디의 철학에 대한 아이옌가르의 믿음은 너무 깊어서, 그는 심지어 그의 노트북도 양심의 가책 없이 열 수 없다.,en,ko\n",
    "# 간디의 철학에 대한 아이옌가르의 믿음은 너무 깊어서, 그는 심지어 그의 노트북도 양심의 가책 없이 열 수 없다.,Iyengar's belief in Gandhi's philosophy is so deep that he can't even open his laptop without remorse.,ko,en\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 전체 데이터 로드 - AI Hub 방송 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/aihub_broadcast_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/aihub_broadcast_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines = f_en.read().splitlines()\n",
    "    ko_lines = f_ko.read().splitlines()\n",
    "\n",
    "# 데이터 개수 제한 (예: 100개)\n",
    "limit = 1000\n",
    "en_lines = en_lines[:limit]\n",
    "ko_lines = ko_lines[:limit]\n",
    "\n",
    "# train/valid split(90 : 10)\n",
    "split_idx = int(len(en_lines) * 0.9)\n",
    "train_en, valid_en = en_lines[:split_idx], en_lines[split_idx:]\n",
    "train_ko, valid_ko = ko_lines[:split_idx], ko_lines[split_idx:]\n",
    "\n",
    "print(len(train_en))\n",
    "print(len(valid_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "efef8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬 데이터 생성\n",
    "\n",
    "# train.csv 생성\n",
    "with open('./llm_data/aihub_broadcast_translation/train.csv', 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang'])\n",
    "\n",
    "    for en, ko in zip(train_en, train_ko):\n",
    "        en, ko = en.strip(), ko.strip()\n",
    "        if not en or not ko:\n",
    "            continue\n",
    "\n",
    "        # 영어 -> 한국어\n",
    "        writer.writerow([en, ko, 'en', 'ko'])\n",
    "        # 한국어 -> 영어\n",
    "        writer.writerow([ko, en, 'ko', 'en'])\n",
    "\n",
    "# valid.csv 생성\n",
    "with open('./llm_data/aihub_broadcast_translation/valid.csv', 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang'])\n",
    "\n",
    "    for en, ko in zip(valid_en, valid_ko):\n",
    "        en, ko = en.strip(), ko.strip()\n",
    "        if not en or not ko:\n",
    "            continue\n",
    "        \n",
    "        # 영어 -> 한국어\n",
    "        writer.writerow([en, ko, 'en', 'ko'])\n",
    "        # 한국어 -> 영어\n",
    "        writer.writerow([ko, en, 'ko', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "420188a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 전처리\n",
    "from datasets import load_dataset\n",
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 1. tokenizer 로드\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# 2. 전처리 함수 (동적 tgt_lang 설정)\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"src\"]\n",
    "    targets = examples[\"tgt\"]\n",
    "    tgt_langs = examples[\"tgt_lang\"]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    labels_list = []\n",
    "    for text, lang in zip(targets, tgt_langs):\n",
    "        if lang in tokenizer.lang_code_to_id:\n",
    "            tokenizer.tgt_lang = lang\n",
    "        else:\n",
    "            tokenizer.tgt_lang = \"en\"  # 기본값\n",
    "\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(text, max_length=128, truncation=True, padding=\"max_length\")\n",
    "        labels_list.append(labels[\"input_ids\"])\n",
    "\n",
    "    model_inputs[\"labels\"] = labels_list\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fd609d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9440380d0947496b98ecf8323337fec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a87542b5ce942c693bbda18fafeaec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c60c23f4b6c44687bed864f6a15f5fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea630b6285d421b9a6639f0dae4a98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 불러오기\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"./llm_data/aihub_broadcast_translation/train.csv\",\n",
    "        \"validation\": \"./llm_data/aihub_broadcast_translation/valid.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 토크나이즈 적용\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "817f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "base_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,              # 랭크 크기\n",
    "    lora_alpha=32,    # 스케일링 계수\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Attention 모듈에 적용\n",
    ")\n",
    "\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(base_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4780be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    eval_strategy=\"epoch\", # 구버전\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8ca31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [339/339 1:38:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.022100</td>\n",
       "      <td>6.771487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.731700</td>\n",
       "      <td>6.245128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.424600</td>\n",
       "      <td>6.196653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=339, training_loss=7.278763582572824, metrics={'train_runtime': 5912.2558, 'train_samples_per_second': 0.913, 'train_steps_per_second': 0.057, 'total_flos': 1467687842611200.0, 'train_loss': 7.278763582572824, 'epoch': 3.0})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3218a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO: 그것은 문자 그대로 삶의 기초입니다.\n",
      "KO→EN: It is literally the basis of life.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)   # 입력도 GPU로 이동\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)   # 입력도 GPU로 이동\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40cb8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llm_models/lora_translation_model\\\\tokenizer_config.json',\n",
       " './llm_models/lora_translation_model\\\\special_tokens_map.json',\n",
       " 'llm_models\\\\lora_translation_model\\\\vocab.json',\n",
       " 'llm_models\\\\lora_translation_model\\\\sentencepiece.bpe.model',\n",
       " './llm_models/lora_translation_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 적용된 모델 저장\n",
    "model.save_pretrained(\"./llm_models/lora_translation_model\")\n",
    "tokenizer.save_pretrained(\"./llm_models/lora_translation_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f98cc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 원본 M2M100 모델 로드\n",
    "base_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# LoRA 어댑터 붙여서 불러오기\n",
    "model = PeftModel.from_pretrained(base_model, \"./llm_models/lora_translation_model\")\n",
    "\n",
    "# 토크나이저도 불러오기\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"./llm_models/lora_translation_model\")\n",
    "\n",
    "# 디바이스 맞추기\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "07bb1f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO:\n",
      "The weather is very good today. → 오늘날 날씨는 매우 좋습니다.\n",
      "I am studying machine learning. → 저는 기계 학습을 공부하고 있습니다.\n",
      "Artificial intelligence is changing the world. → 인공 지능이 세상을 바꾸고 있다.\n",
      "Deep learning is a subset of machine learning. → 깊은 학습은 기계 학습의 하위 세트입니다.\n",
      "Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day. → GM 판매자 인 로베로 (Roberto)는 매일 글을 쓰는 오넬라 무티 (Ornella Muti)와 미친 사랑에 빠져 있습니다.\n",
      "When women replaced men, they earned much less money than them, which was a real problem. → 여성들이 남자를 대체했을 때, 그들은 그들보다 훨씬 적은 돈을 벌었습니다.\n",
      "It is literally the basis of life. → 그것은 문자 그대로 삶의 기초입니다.\n",
      "It's hard to believe, but the People's Artist of Russia, singer and TV presenter Nadezhda Babkina is 70 years old! → 믿기 어려운 일이지만, 러시아의 민간 아티스트, 노래가자 TV 선물자 나데지다 바비키나가 70세입니다!\n",
      "Arriving in the USA, Nuno is assigned the teacher Jane Dayle Haddon as a &quot;tutor&quot;. → 미국에 도착했을 때, Nuno는 교사 Jane Dayle Haddon에게 &quot;교사&quot;로 임명됩니다.\n",
      "This mediated dialogue allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness. → 이 중재된 대화는 스테파니아와 안드레아가 그들의 관계를 반영하고 게임이 공개되면 더 많은 인식으로 다시 시작할 수 있게 해줍니다.\n",
      "\n",
      "KO→EN:\n",
      "오늘 날씨는 매우 좋습니다. → The weather is very good today.\n",
      "저는 머신러닝을 공부하고 있습니다. → I am studying machine learning.\n",
      "인공 지능은 세계를 변화시킵니다. → Artificial intelligence changes the world.\n",
      "딥러닝은 머신 러닝의 하위 세트입니다. → Deep learning is a subset of machine learning.\n",
      "지엠 판매원인 로베르토는 그가 매일 편지를 쓰는 오르넬라 무티를 열렬히 사랑한다. → Roberto, a Jim-seller, loves Ornella Muti, whose letters he writes every day.\n",
      "여성이 남성을 대체했을 때, 그들은 남성보다 훨씬 적은 돈을 벌게 되었고 이것이 진짜 문제였습니다. → When women replaced men, they earned much less money than men, and this was a real problem.\n",
      "말 그대로 삶의 기초입니다. → It is literally the basis of life.\n",
      "믿기 어렵겠지만, 러시아의 인민 예술가이자 가수이자 텔레비전 진행자인 나데즈다 밥키나는 70세입니다! → It’s hard to believe, but Russian folk artist, singer and television producer Nadezda Bobkinna is 70 years old!\n",
      "미국에 도착한 누노는 제인 데일 해든 선생님으로 임명되었다. → The sister arriving in the United States was appointed as Professor Jane Dale Haden.\n",
      "이 중재된 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반성하고 게임이 공개되면 더 큰 인식으로 게임을 재개할 수 있습니다. → Through this intermittent conversation, Stephania and Andrea can reflect on their relationship and reopen the game with greater awareness when the game is opened.\n"
     ]
    }
   ],
   "source": [
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "texts = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day.\",\n",
    "    \"When women replaced men, they earned much less money than them, which was a real problem.\",\n",
    "    \"It is literally the basis of life.\",\n",
    "    \"It's hard to believe, but the People's Artist of Russia, singer and TV presenter Nadezhda Babkina is 70 years old!\",\n",
    "    \"Arriving in the USA, Nuno is assigned the teacher Jane Dayle Haddon as a &quot;tutor&quot;.\",\n",
    "    \"This mediated dialogue allows Stefania and Andrea to reflect on their relationship and, when the game is revealed, to resume it with greater awareness.\"\n",
    "\n",
    "]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"EN→KO:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "texts_ko = [\n",
    "    \"오늘 날씨는 매우 좋습니다.\",\n",
    "    \"저는 머신러닝을 공부하고 있습니다.\",\n",
    "    \"인공 지능은 세계를 변화시킵니다.\",\n",
    "    \"딥러닝은 머신 러닝의 하위 세트입니다.\",\n",
    "    \"지엠 판매원인 로베르토는 그가 매일 편지를 쓰는 오르넬라 무티를 열렬히 사랑한다.\",\n",
    "    \"여성이 남성을 대체했을 때, 그들은 남성보다 훨씬 적은 돈을 벌게 되었고 이것이 진짜 문제였습니다.\",\n",
    "    \"말 그대로 삶의 기초입니다.\",\n",
    "    \"믿기 어렵겠지만, 러시아의 인민 예술가이자 가수이자 텔레비전 진행자인 나데즈다 밥키나는 70세입니다!\",\n",
    "    \"미국에 도착한 누노는 제인 데일 해든 선생님으로 임명되었다.\",\n",
    "    \"이 중재된 대화를 통해 스테파니아와 안드레아는 그들의 관계에 대해 반성하고 게임이 공개되면 더 큰 인식으로 게임을 재개할 수 있습니다.\"\n",
    "\n",
    "]\n",
    "inputs = tokenizer(texts_ko, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\\nKO→EN:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts_ko[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b185fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO chrF Score:\n",
      "chrF2 = 31.88\n",
      "\n",
      "KO→EN chrF Score:\n",
      "chrF2 = 77.53\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# 영어 → 한국어 평가\n",
    "references_ko = [\n",
    "    \"오늘 날씨는 매우 좋습니다.\",\n",
    "    \"저는 머신러닝을 공부하고 있습니다.\",\n",
    "    \"인공지능은 세상을 변화시키고 있습니다.\",\n",
    "    \"딥러닝은 머신러닝의 하위 집합입니다.\",\n",
    "    \"지엠 판매원 로베르토는 오르넬라 무티를 열렬히 사랑하며 매일 편지를 씁니다.\"\n",
    "]\n",
    "\n",
    "hypotheses_ko = [\n",
    "    \"오늘날 날씨는 매우 좋습니다.\",\n",
    "    \"저는 기계 학습을 공부하고 있습니다.\",\n",
    "    \"인공 지능이 세상을 바꾸고 있다.\",\n",
    "    \"깊은 학습은 기계 학습의 하위 세트입니다.\",\n",
    "    \"GM 판매자 인 로베로는 매일 글을 쓰는 오넬라 무티와 미친 사랑에 빠져 있습니다.\"\n",
    "]\n",
    "\n",
    "print(\"EN→KO chrF Score:\")\n",
    "chrf_ko = sacrebleu.corpus_chrf(hypotheses_ko, [references_ko])\n",
    "print(chrf_ko)\n",
    "\n",
    "\n",
    "# 한국어 → 영어 평가\n",
    "references_en = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence is changing the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a GM salesman, is madly in love with Ornella Muti, to whom he writes every day.\"\n",
    "]\n",
    "\n",
    "hypotheses_en = [\n",
    "    \"The weather is very good today.\",\n",
    "    \"I am studying machine learning.\",\n",
    "    \"Artificial intelligence changes the world.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Roberto, a Jim-seller, loves Ornella Muti, whose letters he writes every day.\"\n",
    "]\n",
    "\n",
    "print(\"\\nKO→EN chrF Score:\")\n",
    "chrf_en = sacrebleu.corpus_chrf(hypotheses_en, [references_en])\n",
    "print(chrf_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
