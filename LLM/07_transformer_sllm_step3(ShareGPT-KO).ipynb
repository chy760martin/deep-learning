{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6f2905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Language Model(SLLM) \n",
    "\n",
    "# Step 03 - íŠ¸ëœìŠ¤í¬ë¨¸ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹(ShareGPT-KO)ì„ ì ìš©í•˜ì—¬ Small Language Model(SLLM) êµ¬ì¶• ë° í…ìŠ¤íŠ¸ ìƒì„±\n",
    "# - Hugging Faceì—ì„œ ì œê³µí•˜ëŠ” FreedomIntelligence/sharegpt-korean ë°ì´í„°ì…‹ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ Transformer ì–¸ì–´ ëª¨ë¸ êµ¬ì¡°ì— ë§ì¶° êµ¬ì„±\n",
    "# - ShareGPT-KOëŠ” GPTì™€ ì‚¬ìš©ì ê°„ì˜ ëŒ€í™” ë°ì´í„°ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•œ ê³ í’ˆì§ˆ ëŒ€í™”í˜• ë°ì´í„°ì…‹ìœ¼ë¡œ, ë¬¸ì¥ ìƒì„±, ëŒ€í™” ëª¨ë¸ í•™ìŠµ, ì‘ë‹µ ìƒì„± ë“±ì— ë§¤ìš° ì í•©í•¨\n",
    "\n",
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ GPU ì„¤ì • -> ë°ì´í„°ì…‹ -> ë°ì´í„°ë¡œë” -> í† í¬ë‚˜ì´ì €/í† í°í™” -> SLLM ëª¨ë¸ ì •ì˜ -> í•™ìŠµ -> í…ìŠ¤íŠ¸ ìƒì„± -> í…ŒìŠ¤íŠ¸\n",
    "\n",
    "# í•µì‹¬ ëª©ì  - í•œêµ­ì–´ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ ìƒì„± ë° ë‹¨ì¼ë¬¸ì¥/ëŒ€í™” íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡\n",
    "# - ê¸°ì¡´ WikiText-2 ê¸°ë°˜ Transformer ì–¸ì–´ ëª¨ë¸ â†’ ShareGPT-KO ëŒ€í™” ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€ê²½\n",
    "# - ë¬¸ì¥ ìƒì„± ë° ë¬¸ë§¥ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ í•™ìŠµ\n",
    "# - ë‹¨ì¼ ë¬¸ì¥ ë˜ëŠ” ëŒ€í™” íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6575fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version: 2.2.2, Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1) ë¼ì´ë¸ŒëŸ¬ë¦¬ GPU ì„¤ì •\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Pytorch Version: {torch.__version__}, Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe74e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) ShareGPT-KO ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "\n",
    "# Hugging Faceì—ì„œ ShareGPT-KO ë°ì´í„°ì…‹ ë¡œë”©\n",
    "dataset = load_dataset('FreedomIntelligence/sharegpt-korean', split='train') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811995e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'lang', 'conversations'],\n",
      "    num_rows: 6014\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 3) ë°ì´í„°ì…‹ ë‚´ì˜ ëŒ€í™”ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "print(dataset)\n",
    "\n",
    "texts = []\n",
    "for item in dataset:\n",
    "    for turn in item['conversations']:\n",
    "        texts.append(turn['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8bb29c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0f0d1ab4a647db8ed384d6d17506b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b3d5cb9bcb44308213be05174da65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c1fbfd7bd34c8ba9b3b89e5f672ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) í† í¬ë‚˜ì´ì € ëª¨ë¸ ë¡œë“œ\n",
    "# tokenizer_model = 'monologg/kobert'\n",
    "tokenizer_model = 'beomi/KcBERT-base'  # ì•ˆì •ì ì¸ í•œêµ­ì–´ í† í¬ë‚˜ì´ì €\n",
    "auto_tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "834915dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (395 > 300). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# 5) í† í¬ë‚˜ì´ì €ë¥¼ ì´ìš©í•˜ì—¬ í† í°í™”\n",
    "\n",
    "# í† í°í™” í•¨ìˆ˜\n",
    "def tokenize(text):\n",
    "    return auto_tokenizer.tokenize(text)\n",
    "\n",
    "tokens = []\n",
    "for text in texts: # í…ìŠ¤íŠ¸ -> í† í°í™”\n",
    "    tokens.extend(tokenize(text))\n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "vocab = sorted(set(tokens))\n",
    "word2idx = { word:idx for idx, word in enumerate(vocab) } # ë‹¨ì–´ -> ì¸ë±ìŠ¤\n",
    "idx2word = { idx:word for word, idx in word2idx.items() } # ì¸ë±ìŠ¤ -> ë‹¨ì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3b3ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16538\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97e2d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) ì‹œí€€ìŠ¤ ìƒì„± - ì…ë ¥ ì‹œí€€ìŠ¤, ì¶œë ¥ ì‹œí€€ìŠ¤ ìƒì„±\n",
    "sequence_length = 32\n",
    "data = []\n",
    "\n",
    "# ì…ë ¥ ì¶œë ¥ - 32, 32\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    input_seq = tokens[i:i+sequence_length] # ì…ë ¥ ì‹œí€€ìŠ¤\n",
    "    target_seq = tokens[i+1:i+sequence_length+1] # ì¶œë ¥ ì‹œí€€ìŠ¤\n",
    "    # ì…ë ¥, ì¶œë ¥ - ë‹¨ì–´ -> ì¸ë±ìŠ¤ -> í…ì„œ ë³€í™˜\n",
    "    data.append(\n",
    "        (\n",
    "            # torch.tensor( [ word2idx[word] for word in input_seq ] ),\n",
    "            # torch.tensor( [ word2idx[word] for word in target_seq ] )\n",
    "            torch.tensor( [ word2idx.get(word, word2idx.get('[PAD]', 0)) for word in input_seq ] ),\n",
    "            torch.tensor( [ word2idx.get(word, word2idx.get('[PAD]', 0)) for word in target_seq ] )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18b56c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Dataset ë° DataLoader ìƒì„±\n",
    "\n",
    "# Dataset ì„¤ì •\n",
    "class ShareGPTDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "# DataLoader ì„¤ì •\n",
    "dataloader = DataLoader(ShareGPTDataset(data=data[:50000]), batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c71aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Transformer ëª¨ë¸ ì •ì˜\n",
    "class TransformerSLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len):\n",
    "        super().__init__()\n",
    "        # ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ê³ ì •ëœ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì •ì˜ vocab_size(ë‹¨ì–´ ì‚¬ì „ì˜ í¬ê¸°), embed_dim(ê° ë‹¨ì–´ë¥¼ í‘œí˜„í•  ë²¡í„°ì˜ ì°¨ì›, ì˜ˆì‹œ 128ì°¨ì›)\n",
    "        # ì¦‰ ë‹¨ì–´ ì¸ë±ìŠ¤ëŠ” ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆì§€ ì•ŠìŒ ê·¸ë˜ì„œ ì„ë² ë”© ë²¡í„°ëŠ” ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ í•™ìŠµ, ì´ ë ˆì´ì–´ëŠ” vocab_size x embed_dim í¬ê¸°ì˜ ì„ë² ë”© í–‰ë ¬ì„ í•™ìŠµí•œë‹¤\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim) # 8086, 128\n",
    "\n",
    "        # ìœ„ì§€ ì •ë³´(positional encoding) ì…ë ¥ ì‹œì¿¼ìŠ¤ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ë”°ë¡œ ì œê³µí•´ì•¼ í•˜ë©°, ê° ìœ„ì¹˜ì— ëŒ€í•´ í•™ìŠµ ê°€ëŠ¥í•œ ìœ„ì¹˜ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ì„ë² ë”© ë ˆì´ì–´\n",
    "        # max_len(32), embed_dim(128) ìœ„ì¹˜ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•´ì•¼ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆìŒ\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim) # 32, 128\n",
    "\n",
    "        # d_model(128 ì…ë ¥ ë° ì¶œë ¥ ì„ë² ë”© ì°¨ì›), nhead(4 ì–´í…ì…” í—¤ë“œì˜ ìˆ˜), dim_feedforward(256 ë‚´ë¶€ì˜ ì€ë‹‰ì¸µ í¬ê¸°)\n",
    "        # ì¦‰ ì´ ë ˆì´ì–´ëŠ” ì…ë ¥ í…ì„œì˜ shapeì´ [seq_len, batch_size, embed_dim]ì¼ë•Œ ë™ì¼í•œ shapeì˜ ì¶œë ¥ì„ ë°˜í™˜í•˜ë©´ì„œ ë¬¸ë§¥ ì •ë³´ë¥¼ ê°•í™”í•œë‹¤\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Transformer ëª¨ë¸ì˜ ì¸ì½”ë” ë¸”ë¡ ì „ì²´ë¥¼ êµ¬ì„±í•˜ëŠ” í•µì‹¬ ì½”ë“œ ì•ì„œ ì •ì˜í•œ encoder_layerë¥¼ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•´ì„œ ê¹Šì´ ìˆëŠ” ë¬¸ë§¥ ì´í•´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” êµ¬ì¡°\n",
    "        # nn.TransformerEncoder ëŠ” í•˜ë‚˜ì˜ encoder_layer ë¥¼ num_layers ë§Œí¼ ë°˜ë³µí•˜ì—¬ ì „ì²´ ì¸ì½”ë” ìŠ¤íƒì„ êµ¬ì„±í•œë‹¤\n",
    "        # Input â†’ EncoderLayer 1 â†’ EncoderLayer 2 â†’ ... â†’ EncoderLayer N â†’ Output\n",
    "        # ê° ë ˆì´ì–´ëŠ” ì…ë ¥ ì‹œí€€ì‹œë¥¼ ë°›ì•„ ë¬¸ë§¥ ì •ë³´ë¥¼ ê°•í™”í•˜ê³ , ë‹¤ìŒ ë ˆì´ì–´ë¡œ ì „ë‹¬í•œë‹¤(ë°˜ë³µ íšŸìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ë³µì¡í•œ ë¬¸ë§¥ê³¼ ì˜ë¯¸ ê´€ê³„ë¥¼ ë” ì˜ í•™ìŠµí•  ìˆ˜ ìˆìŒ)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers) # encoder_layer, 2\n",
    "\n",
    "        # ëª¨ë¸ì˜ ì¶œë ¥ì¸µ ì •ì˜í•˜ëŠ” ë¶€ë¶„ìœ¼ë¡œ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì„ë² ë”©ì„ ë‹¨ì–´ ì‚¬ì „ì˜ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• , Transformerì˜ ë§ˆì§€ë§‰ ì¶œë ¥ì€ [batch_size, seq_len, embed_dim] í˜•íƒœì˜ ë¬¸ë§¥ ë²¡í„°ì´ë‹¤\n",
    "        # ì´ ë²¡í„°ë¥¼ vocab_size ì°¨ì›ì˜ ë¡œì§“(logits)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬, ê° ìœ„ì¹˜ì—ì„œ ì–´ë–¤ ë‹¨ì–´ê°€ ì˜¬ì§€ ì—ì¸¡í•œë‹¤, Linear ë ˆì´ì–´ëŠ” ì„ë² ë”© ê³µê°„ -> ë‹¨ì–´ ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë¶„ë¥˜ê¸° ì—­í• ì„ í•œë‹¤\n",
    "        # embed_dim(Transformerì˜ ì¶œë ¥ ì°¨ì› ì˜ˆì‹œ 128), vocab_size(ì „ì²´ ë‹¨ì–´ ì‚¬ì „ í¬ê¸° ì—ì‹œ 10000)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size) # 128, 8086\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # xëŠ” [batch_size, sequence_length] í˜•íƒœì˜ í…ì„œ, ì˜ˆë¥¼ ë“¤ì–´, x.shape = [8, 32]ë¼ë©´:\n",
    "        # x.size(0) â†’ ë°°ì¹˜ í¬ê¸° (8)\n",
    "        # x.size(1) â†’ ì‹œí€€ìŠ¤ ê¸¸ì´ (32)\n",
    "        # ì¦‰, seq_len = 32ê°€ ëœë‹¤\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # seq_len = 32ì´ë©´ [0, 1, 2, ..., 31] í˜•íƒœì˜ ìœ„ì¹˜ ì¸ë±ìŠ¤ ë²¡í„°ë¥¼ ìƒì„±\n",
    "        # í…ì„œì˜ shapeì„ [1, seq_len]ìœ¼ë¡œ ë³€í™˜ â†’ ì´ë ‡ê²Œ í•˜ë©´ ë°°ì¹˜ ì°¨ì›ê³¼ ë§ì¶°ì„œ broadcastingì´ ê°€ëŠ¥\n",
    "        position = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "\n",
    "        # ë‹¨ì–´ ì„ë² ë”©ê³¼ ìœ„ì¹˜ ì„ë² ë”©ì„ ë”í•´ì„œ ìµœì¢… ì…ë ¥ ë²¡í„°ë¥¼ ë§Œë“¤ê³ , ì´ í•©ì‚°ì€ Transformerê°€ ë‹¨ì–´ ì˜ë¯¸ + ìœ„ì¹˜ ì •ë³´ë¥¼ ë™ì‹œì— ì¸ì‹í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤\n",
    "        x = self.token_embedding(x) + self.pos_embedding(position)\n",
    "\n",
    "        # Transformer ì¸ì½”ë”ì— ì…ë ¥ì„ ì „ë‹¬í•˜ì—¬ ë¬¸ë§¥ ì •ë³´ë¥¼ ê°•í™”\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Transformerì˜ ì¶œë ¥ì€ [batch_size, seq_len, embed_dim] í˜•íƒœì˜ ë¬¸ë§¥ ë²¡í„°ì´ë©°, fc_out ë ˆì´ì–´ë¥¼ í†µí•´ ê° ìœ„ì¹˜ì—ì„œ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ì˜ í™•ë¥  ë¶„í¬ë¥¼ ì˜ˆì¸¡\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "129102bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 20215.6324\n",
      "Epoch 2, Loss: 17601.9132\n",
      "Epoch 3, Loss: 17304.4040\n",
      "Epoch 4, Loss: 17121.7981\n",
      "Epoch 5, Loss: 16988.9319\n",
      "Epoch 6, Loss: 16869.8428\n",
      "Epoch 7, Loss: 16778.3830\n",
      "Epoch 8, Loss: 16692.0304\n",
      "Epoch 9, Loss: 16619.1102\n",
      "Epoch 10, Loss: 16565.8797\n"
     ]
    }
   ],
   "source": [
    "# 9) í•™ìŠµ\n",
    "model = TransformerSLLM(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    max_len=sequence_length\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # ì†ì‹¤í•¨ìˆ˜ ê°‘\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # ì˜µí‹°ë§ˆì´ì € Adam\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs = inputs.to(device) # ì…ë ¥ ê°’\n",
    "        targets = targets.to(device) # ì •ë‹µ ê°’\n",
    "\n",
    "        outputs = model(inputs) # ëª¨ë¸ ì˜ˆì¸¡ ê°’\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1)) # ì†ì‹¤í•¨ìˆ˜ ê³„ì‚° ê°’\n",
    "\n",
    "        # ì˜¤ì°¨ì—­ì „íŒŒ\n",
    "        optimizer.zero_grad() # ê°€ì¤‘ì¹˜, ë°”ì´ì–´ìŠ¤ íŒŒë¦¬ë¯¸í„° ì´ˆê¸°í™”\n",
    "        loss.backward() # ë¯¸ë¶„ ì—°ì‚°\n",
    "        optimizer.step() # ë¯¸ë¶„ ì—°ì‚° í›„ ê°€ì¤‘ì¹˜,ë°”ì´ì–´ìŠ¤ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4021077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) í…ìŠ¤íŠ¸ ìƒì„± ë° ì €ì¥\n",
    "\n",
    "# torch.topk()ë¡œ ìƒìœ„ top_kê°œì˜ ë¡œì§“ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°, ì œê±°ëœ ë¡œì§“ì€ -infë¡œ ì„¤ì •ë˜ì–´ softmaxì—ì„œ í™•ë¥ ì´ 0ì´ ëœë‹¤.\n",
    "def top_k_filtering(logits, top_k=50):\n",
    "    values, _ = torch.topk(logits, top_k)\n",
    "    min_value = values[:, -1].unsqueeze(1)\n",
    "    return torch.where(logits < min_value, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "# softmax í›„ ëˆ„ì  í™•ë¥ ì´ top_pë¥¼ ë„˜ëŠ” í† í°ì„ ì œê±°, top_p=0.9ì´ë©´ ëˆ„ì  í™•ë¥ ì´ 90%ë¥¼ ë„˜ëŠ” ì´í›„ì˜ í† í°ì€ ì„ íƒë˜ì§€ ì•ŠìŒ, ë” ìì—°ìŠ¤ëŸ½ê³  ë‹¤ì–‘ì„± ìˆëŠ” ë¬¸ì¥ ìƒì„±ì— íš¨ê³¼ì ì´ë‹¤.\n",
    "def top_p_filtering(logits, top_p=0.9):\n",
    "    # logits: [batch_size, vocab_size]\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "    # top_p ì´ˆê³¼í•˜ëŠ” ë¶€ë¶„ ì œê±°\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0 # ê°€ì¥ ë†’ì€ í™•ë¥ ì€ ìœ ì§€\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[0, indices_to_remove] = float('-inf')\n",
    "    return logits\n",
    "\n",
    "def generate_text(model, start_text, word2idx, idx2word, max_length=50, temperature=1.0, top_k=40, top_p=0.85):\n",
    "    model.eval()\n",
    "    tokens = auto_tokenizer.tokenize(start_text)\n",
    "    # input_ids = [ word2idx.get(word, 0) for word in tokens ]\n",
    "    input_ids = [ word2idx.get(token, word2idx.get('[PAD]', 0)) for token in tokens ]\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            next_token_logits = output[:, -1, :] / temperature\n",
    "            logits = top_k_filtering(next_token_logits, top_k)\n",
    "            logits = top_p_filtering(logits, top_p)\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1).item()\n",
    "            input_ids.append(next_token_id)\n",
    "            input_ids = input_ids[-sequence_length:]\n",
    "            input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_words = [idx2word.get(int(idx), \"[UNK]\") for idx in input_ids]\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), 'sharegpt_korean_transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "767221ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerSLLM(\n",
       "  (token_embedding): Embedding(8086, 128)\n",
       "  (pos_embedding): Embedding(32, 128)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=128, out_features=8086, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11) ëª¨ë¸ ì¬ìƒì„± í›„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = TransformerSLLM(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    max_len=sequence_length\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"sharegpt_korean_transformer.pt\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9073f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¢ ìƒì„±ëœ ë¬¸ì¥:\n",
      "ë´„ ë‚  ì„œìš¸ ì—ì„œ ê°€ì¥ ì•„ë¦„ë‹¤ìš´ ì¥ì†Œ ëŠ” ì–´ë”” ì¸ ê°€ ìš” ? O p ic k a ch e x i th on ì´ ì ‘ê·¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ . t ra y p en d ë°©ë²• ì€ ì¶”ê°€ í•˜ëŠ” í´ ë˜ ìŠ¤ë¥¼ ì‚¬ìš© í•˜ì—¬ P ri c 2 . ì´ëŸ¬í•œ ë°©ë²• ì˜ í•©ë‹ˆë‹¤ . ì´ëŠ” ë¸”ë¡œê·¸ ê¸€ , 9\n"
     ]
    }
   ],
   "source": [
    "# 12) í…ŒìŠ¤íŠ¸\n",
    "import re\n",
    "\n",
    "# ì¶œë ¥ ì „ í›„ì²˜ë¦¬ (SentencePiece í† í° ì œê±° ë“±)\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"â–\", \"\")  # SentencePiece ê²½ê³„ ì œê±°\n",
    "    text = text.replace(\"[UNK]\", \"\")  # ì•Œ ìˆ˜ ì—†ëŠ” í† í° ì œê±°\n",
    "    text = re.sub(r\"[^ê°€-í£a-zA-Z0-9.,!? ]\", \"\", text)  # íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
    "    text = re.sub(r\"(.)\\s+\\1+\", r\"\\1\", text)\n",
    "    return ' '.join(text.split()).strip()\n",
    "\n",
    "# ì‹œì‘ ë¬¸ì¥ ì„¤ì •\n",
    "# start_text = \"ì„œìš¸ì˜ ë´„ë‚ , ë²šê½ƒì´ ë§Œê°œí•œ ê±°ë¦¬ë¥¼ ê±¸ìœ¼ë©°\"\n",
    "start_text = \"ë´„ë‚  ì„œìš¸ì—ì„œ ê°€ì¥ ì•„ë¦„ë‹¤ìš´ ì¥ì†ŒëŠ” ì–´ë””ì¸ê°€ìš”?\"\n",
    "# start_text = \"ë²šê½ƒì´ ë§Œê°œí•œ ê±°ë¦¬ë¥¼ ê±¸ìœ¼ë©° ëŠë‚€ ê°ì •ì„ ë§í•´ì¤˜.\"\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ìƒì„± ì‹¤í–‰\n",
    "generated = generate_text(model, start_text, word2idx, idx2word, max_length=50)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ğŸ“¢ ìƒì„±ëœ ë¬¸ì¥:\")\n",
    "print(clean_text(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
