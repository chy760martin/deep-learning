{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer 질의응답(QA) 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22922b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Pre-trained 모델 테스트\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "base_model = 'monologg/koelectra-base-v3-finetuned-korquad' # 한국어 KorQuAD 데이터셋으로 파인 튜닝된 KoELECTRA QA 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model) # 토크나이저 로드\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(base_model) # QA 태스크용 헤드가 포함된 모델 로드\n",
    "\n",
    "question = '서울은 어디에 있나요?'\n",
    "context = '서울은 대한민국의 수도이며, 한반도의 중서부에 위치해 있습니다.'\n",
    "\n",
    "# QA 모델 구조: BERT 기반 QS 모델은 입력을 [CLS]질문[SEP]문맥[SEP] 형태로 받는다\n",
    "# 질문과 문맥을 하나의 입력으로 합쳐서 토큰화 결과를 모델에 전달해야 답변을 얻을 수 있는 구조\n",
    "# [CLS] 서울 은 어디 에 있 나요 ? [SEP] 서울 은 대한민국 의 수도 이며 , 한반도 의 중서부 에 위치 해 있습니다 . [SEP]\n",
    "inputs = tokenizer(question, context, return_tensors='pt') # (batch_size,seq_len) (1,seq_len)\n",
    "\n",
    "# 모델에 입력을 전달하여 start_logits, end_logits 출력\n",
    "# - start_logits: 답변 시작 위치에 대한 확률 분포\n",
    "# - end_logits: 답변 끝 위치에 대한 확률 분포\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# torch.argmax() 가장 확률이 높은 인덱스를 선택\n",
    "# - answer_start: 답변 시작 토큰 위치, answer_end: 답변 끝 토큰 위치\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits) + 1 # 마지막 인덱스 포함\n",
    "\n",
    "# 토큰을 문자열로 변환\n",
    "# - tokenizer.convert_ids_to_tokens(): 토큰 ID->토큰 문자열\n",
    "# - tokenizer.convert_tokens_to_string(): 토큰 문자열->사람이 읽을 수 있는 문장\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    ")\n",
    "\n",
    "# Debug 확인\n",
    "print('답변 시작 인덱스:', answer_start.item())\n",
    "print('답변 끝 인덱스:', answer_end.item())\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]) # 전체 토큰 확인\n",
    "print('전체 토큰 시퀀스:', tokens)\n",
    "\n",
    "answer_tokens = tokens[answer_start:answer_end] # 답변 토큰 범위 확인\n",
    "print('답변 토큰:', answer_tokens)\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens) # 최종 답변\n",
    "print('최종 답변:', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드 및 전처리\n",
    "# - KorQuAD 1.0/2.0: 위키 문서 기반, 질문-답변 쌍 포함, AI Hub QA 데이터(한국어 QA 태스크용)\n",
    "# - SQuAD 1.1/2.0: 가장 널리 쓰이는 QA 데이터셋\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 한국어 KorQuAD 데이터셋: train(60,407), validation(5,774)\n",
    "# - 데이터 구조: context(문서 본문), question(질문), answers(정답 스팬, 텍스트 + 시작 위치)\n",
    "dataset = load_dataset('squad_kor_v1')\n",
    "\n",
    "# validation 데이터셋을 반으로 나누어 test 데이터셋 생성\n",
    "split_dataset = dataset['validation'].train_test_split(test_size=0.5, seed=42)\n",
    "dataset['validation'] = split_dataset['train']\n",
    "dataset['test'] = split_dataset['test']\n",
    "print(dataset['train'].shape, dataset['validation'].shape, dataset['test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a55c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저\n",
    "model_name = 'monologg/koelectra-base-v3-finetuned-korquad'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess(example):\n",
    "    return tokenizer(\n",
    "        example['question'],\n",
    "        example['context'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512\n",
    "    )\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d9e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn 정의 및 DataLoader 생성\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# collate_fn 정의: 텐서 변환\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(x['input_ids']) for x in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(x['attention_mask']) for x in batch])\n",
    "    start_positions = torch.stack([torch.tensor(x['start_positions']) for x in batch])\n",
    "    end_positions = torch.stack([torch.tensor(x['end_positions']) for x in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'start_positions': start_positions,\n",
    "        'end_positions': end_positions\n",
    "    }\n",
    "\n",
    "# DataLoader 구성: train, validation, test\n",
    "train_loader = DataLoader(\n",
    "    tokenized_dataset['train'], \n",
    "    batch_size=16, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    tokenized_dataset['validation'], \n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    tokenized_dataset['test'], \n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
