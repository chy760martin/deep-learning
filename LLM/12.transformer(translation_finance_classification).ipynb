{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 다국어 기계 번역 모델 분류기 \n",
    "# - Hugging Face 라이브러리 적용\n",
    "# - AI HUB 금융 학술논문/공시정보/뉴스/규정/보고서 다국어 번역 데이터셋 적용\n",
    "# - 입력된 문장을 다국어 기계 번역 모델 분류기을 통한 학술논문(0)/공시정보(1)/뉴스(2)/규정(3)/보고서(4) 분류\n",
    "# 1. 학습 목표\n",
    "# - 구조 최적화 및 파이프라인 단순화\n",
    "# - AI HUB 금융 학술논문/공시정보/뉴스/규정/보고서 다국어 번역 데이터셋 전처리\n",
    "# - 병렬 문장쌍 데이터셋 변환 전처리\n",
    "# - 토크나이징 및 토크나이징 전처리\n",
    "# - 베이스 모델 로드\n",
    "# - LoRA(Low-Rank Adaptation) 설정, 특정 레이어에 작은 저차원 행렬(랭크 r)을 삽입해서 학습\n",
    "# - LoRA(Low-Rank Adaptation) 모델, 메모리 효율성/빠른 학습/도메인 적용, base 모델에 여러 LoRA 모듈을 붙였다 떼었다 할 수 있음\n",
    "# - 학습 args 설정\n",
    "# - Trainer 정의\n",
    "# - Trainer 실행\n",
    "# - LoRA 적용된 모델 저장, LoRA모델/토크나이저\n",
    "# - LoRA 적용된 모델 불러오기, 베이스모델/LoRA모델/토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bf0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129 cuda\n",
      "CUDA 사용 가능 여부: True\n",
      "PyTorch CUDA 버전: 12.9\n",
      "빌드 정보: 2.8.0+cu129\n",
      "사용 중인 GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import glob, json, re, os, random, csv, zipfile\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__, device)\n",
    "\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"빌드 정보:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b55675c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "금융 학술 논문 :  900 100\n",
      "금융 공시 정보 :  900 100\n",
      "금융 뉴스 :  900 100\n",
      "금융 규정 :  900 100\n",
      "금융 보고서 :  900 100\n"
     ]
    }
   ],
   "source": [
    "# AI Hub 금융 학술 논문 데이터셋(train_ko.txt, train_en.txt) -> CSV로 변환해 파인 튜닝\n",
    "# 원본 데이터는 train_ko.txt, train_en.txt로 분리 -> 병렬 문장쌍을 만들어야 함\n",
    "# 파인튜닝시 양방향 번역을 지원하려면 같은 문장쌍은 en->ko, ko->en 두방향으로 모두 포함해야 함\n",
    "# CSV 구조 예시\n",
    "# - src,tgt,src_lang,tgt_lang,label\n",
    "# - You can buy it from a convenience store try it out.,편의점에서 사실 수 있으니 시도해보시길 바랍니다.,en,ko\n",
    "# - 편의점에서 사실 수 있으니 시도해보시길 바랍니다.,You can buy it from a convenience store try it out.,ko,en\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# AI Hub 금융 학술 논문 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_article_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_article_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines_article = f_en.read().splitlines()\n",
    "    ko_lines_article = f_ko.read().splitlines()\n",
    "\n",
    "limit = 1000 # 데이터 개수 제한 (예: 100개)\n",
    "en_lines_article = en_lines_article[:limit]\n",
    "ko_lines_article = ko_lines_article[:limit]\n",
    "\n",
    "split_idx = int(len(en_lines_article) * 0.9) # train/valid split(90 : 10)\n",
    "train_en_article, valid_en_article = en_lines_article[:split_idx], en_lines_article[split_idx:]\n",
    "train_ko_article, valid_ko_article = ko_lines_article[:split_idx], ko_lines_article[split_idx:]\n",
    "print('금융 학술 논문 : ', len(train_en_article), len(valid_en_article))\n",
    "\n",
    "# AI Hub 금융 공시 정보 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_disclosure_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_disclosure_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines_disclosure = f_en.read().splitlines()\n",
    "    ko_lines_disclosure = f_ko.read().splitlines()\n",
    "\n",
    "limit = 1000 # 데이터 개수 제한 (예: 100개)\n",
    "en_lines_disclosure = en_lines_disclosure[:limit]\n",
    "ko_lines_disclosure = ko_lines_disclosure[:limit]\n",
    "\n",
    "split_idx = int(len(en_lines_article) * 0.9) # train/valid split(90 : 10)\n",
    "train_en_disclosure, valid_en_disclosure = en_lines_disclosure[:split_idx], en_lines_disclosure[split_idx:]\n",
    "train_ko_disclosure, valid_ko_disclosure = ko_lines_disclosure[:split_idx], ko_lines_disclosure[split_idx:]\n",
    "print('금융 공시 정보 : ', len(train_en_disclosure), len(valid_en_disclosure))\n",
    "\n",
    "# AI Hub 금융 뉴스 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_news_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_news_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines_news = f_en.read().splitlines()\n",
    "    ko_lines_news = f_ko.read().splitlines()\n",
    "\n",
    "limit = 1000 # 데이터 개수 제한 (예: 100개)\n",
    "en_lines_news = en_lines_news[:limit]\n",
    "ko_lines_news = ko_lines_news[:limit]\n",
    "\n",
    "split_idx = int(len(en_lines_article) * 0.9) # train/valid split(90 : 10)\n",
    "train_en_news, valid_en_news = en_lines_news[:split_idx], en_lines_news[split_idx:]\n",
    "train_ko_news, valid_ko_news = ko_lines_news[:split_idx], ko_lines_news[split_idx:]\n",
    "print('금융 뉴스 : ', len(train_en_news), len(valid_en_news))\n",
    "\n",
    "# AI Hub 금융 규정 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_regulation_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_regulation_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines_regulation = f_en.read().splitlines()\n",
    "    ko_lines_regulation = f_ko.read().splitlines()\n",
    "\n",
    "limit = 1000 # 데이터 개수 제한 (예: 100개)\n",
    "en_lines_regulation = en_lines_regulation[:limit]\n",
    "ko_lines_regulation = ko_lines_regulation[:limit]\n",
    "\n",
    "split_idx = int(len(en_lines_article) * 0.9) # train/valid split(90 : 10)\n",
    "train_en_regulation, valid_en_regulation = en_lines_regulation[:split_idx], en_lines_regulation[split_idx:]\n",
    "train_ko_regulation, valid_ko_regulation = ko_lines_regulation[:split_idx], ko_lines_regulation[split_idx:]\n",
    "print('금융 규정 : ', len(train_en_regulation), len(valid_en_regulation))\n",
    "\n",
    "# AI Hub 금융 보고서 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_report_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_report_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines_report = f_en.read().splitlines()\n",
    "    ko_lines_report = f_ko.read().splitlines()\n",
    "\n",
    "limit = 1000 # 데이터 개수 제한 (예: 100개)\n",
    "en_lines_report = en_lines_report[:limit]\n",
    "ko_lines_report = ko_lines_report[:limit]\n",
    "\n",
    "split_idx = int(len(en_lines_article) * 0.9) # train/valid split(90 : 10)\n",
    "train_en_report, valid_en_report = en_lines_report[:split_idx], en_lines_report[split_idx:]\n",
    "train_ko_report, valid_ko_report = ko_lines_report[:split_idx], ko_lines_report[split_idx:]\n",
    "print('금융 보고서 : ', len(train_en_report), len(valid_en_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efef8f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미 존재하는 폴더: ./llm_data/ai_hub_finance_classification\n",
      "article 데이터셋 저장 완료: train/valid CSV\n",
      "disclosure 데이터셋 저장 완료: train/valid CSV\n",
      "news 데이터셋 저장 완료: train/valid CSV\n",
      "regulation 데이터셋 저장 완료: train/valid CSV\n",
      "report 데이터셋 저장 완료: train/valid CSV\n"
     ]
    }
   ],
   "source": [
    "# 병렬 데이터 생성 - 편향되지 않은 데이터셋 생성\n",
    "# 영어->한국어, 한국어->영어, \n",
    "# label -> 학술논문(0)/공시정보(1)/뉴스(2)/규정(3)/보고서(4) 분류\n",
    "\n",
    "# 출력 디렉토리\n",
    "out_dir = './llm_data/ai_hub_finance_classification'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f'폴더 생성 완료: {out_dir}')\n",
    "else:\n",
    "    print(f'이미 존재하는 폴더: {out_dir}')    \n",
    "\n",
    "# 문서 유형별 라벨 정의\n",
    "label_map = {\n",
    "    'article': 0,       # 금융 학술 논문\n",
    "    'disclosure': 1,    # 금융 공시 정보\n",
    "    'news': 2,          # 금융 뉴스\n",
    "    'regulation': 3,    # 금융 규정\n",
    "    'report': 4         # 금융 보고서\n",
    "}\n",
    "\n",
    "# 데이터셋 생성 함수\n",
    "def create_csv(train_en, train_ko, valid_en, valid_ko, doc_type):\n",
    "    num_labels = label_map[doc_type]\n",
    "    # train\n",
    "    train_file = os.path.join(out_dir, f'train_{doc_type}.csv')\n",
    "    with open(train_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang', 'label'])\n",
    "        for en, ko in zip(train_en, train_ko):\n",
    "            if not en or not ko:\n",
    "                continue\n",
    "            # 영어 -> 한국어\n",
    "            writer.writerow([en, ko, 'en', 'ko', num_labels])\n",
    "            # 한국어 -> 영어\n",
    "            writer.writerow([ko, en, 'ko', 'en', num_labels])\n",
    "    \n",
    "    # valid\n",
    "    valid_file = os.path.join(out_dir, f'valid_{doc_type}.csv')\n",
    "    with open(valid_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang', 'label'])\n",
    "        for en, ko in zip(valid_en, valid_ko):\n",
    "            if not en or not ko:\n",
    "                continue\n",
    "            # 영어 -> 한국어\n",
    "            writer.writerow([en, ko, 'en', 'ko', num_labels])\n",
    "            # 한국어 -> 영어\n",
    "            writer.writerow([ko, en, 'ko', 'en', num_labels])\n",
    "    \n",
    "    print(f\"{doc_type} 데이터셋 저장 완료: train/valid CSV\")\n",
    "\n",
    "# 데이터셋 생성 호출\n",
    "create_csv( # 금융 학술 논문 데이터셋 생성\n",
    "    train_en=train_en_article, \n",
    "    train_ko=train_ko_article,\n",
    "    valid_en=valid_en_article, \n",
    "    valid_ko=valid_ko_article,\n",
    "    doc_type='article'\n",
    ")\n",
    "create_csv( # 금융 공시 정보 데이터셋 생성\n",
    "    train_en=train_en_disclosure, \n",
    "    train_ko=train_ko_disclosure,\n",
    "    valid_en=valid_en_disclosure, \n",
    "    valid_ko=valid_ko_disclosure,\n",
    "    doc_type='disclosure'\n",
    ")\n",
    "create_csv( # 금융 뉴스 데이터셋 생성\n",
    "    train_en=train_en_news, \n",
    "    train_ko=train_ko_news,\n",
    "    valid_en=valid_en_news, \n",
    "    valid_ko=valid_ko_news,\n",
    "    doc_type='news'\n",
    ")\n",
    "create_csv( # 금융 규정 데이터셋 생성\n",
    "    train_en=train_en_regulation, \n",
    "    train_ko=train_ko_regulation,\n",
    "    valid_en=valid_en_regulation, \n",
    "    valid_ko=valid_ko_regulation,\n",
    "    doc_type='regulation'\n",
    ")\n",
    "create_csv( # 금융 보고서 데이터셋 생성\n",
    "    train_en=train_en_report, \n",
    "    train_ko=train_ko_report,\n",
    "    valid_en=valid_en_report, \n",
    "    valid_ko=valid_ko_report,\n",
    "    doc_type='report'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "807d26f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "통합 학습 데이터셋 생성 완료: ./llm_data/ai_hub_finance_classification\\train_all.csv\n",
      "통합 검증 데이터셋 생성 완료: ./llm_data/ai_hub_finance_classification\\valid_all.csv\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 - csv 파일 통합\n",
    "\n",
    "# 출력 디렉토리\n",
    "out_dir = './llm_data/ai_hub_finance_classification'\n",
    "\n",
    "# 통합 파일 경로\n",
    "train_all_file = os.path.join(out_dir, 'train_all.csv')\n",
    "valid_all_file = os.path.join(out_dir, 'valid_all.csv')\n",
    "\n",
    "# 공통 헤더\n",
    "header = ['src', 'tgt', 'src_lang', 'tgt_lang', 'label']\n",
    "\n",
    "# train_all.csv 생성\n",
    "with open(train_all_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # train_*.csv 파일들 읽어서 합치기\n",
    "    for file in glob.glob(os.path.join(out_dir, 'train_*.csv')):\n",
    "        with open(file, 'r', encoding='utf-8') as f_in:\n",
    "            reader = csv.reader(f_in)\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                if row:\n",
    "                    writer.writerow(row)\n",
    "print(f\"통합 학습 데이터셋 생성 완료: {train_all_file}\")\n",
    "\n",
    "# valid_all.csv 생성\n",
    "with open(valid_all_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # valid_*.csv 파일들 읽어서 합치기\n",
    "    for file in glob.glob(os.path.join(out_dir, 'valid_*.csv')):\n",
    "        with open(file, 'r', encoding='utf-8') as f_in:\n",
    "            reader = csv.reader(f_in)\n",
    "            for i, row in enumerate(reader):\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                if row:\n",
    "                    writer.writerow(row)\n",
    "print(f\"통합 검증 데이터셋 생성 완료: {valid_all_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7e275f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab677a1a463246a7b59b71cbc01e1da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e039e48164c042c3a7ddf9b1c8df44a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': 'From 2019, it was mandatory for companies with total assets of KRW 2 trillion or more as of the end of the previous year, and from 2020, the mandatory target was expanded to companies with total assets of KRW 500 billion or more as of the end of the previous year.', 'tgt': '2019년부터 전년말 기준 자산총액 2조 원 이상인 기업에 대해 의무화하였으며, 2020년부터는 전년말 기준 자산총액 5천 억 원 이상 기업까지 의무대상을 확대했다.', 'src_lang': 'en', 'tgt_lang': 'ko', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Dataset 로딩\n",
    "from datasets import load_dataset\n",
    "\n",
    "# csv 파일 로딩\n",
    "train_dataset = load_dataset('csv', data_files={'train': './llm_data/ai_hub_finance_classification/train_all.csv'})['train']\n",
    "valid_dataset = load_dataset('csv', data_files={'valid': './llm_data/ai_hub_finance_classification/valid_all.csv'})['valid']\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6d0b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aabe91a9c6542358d47d32a9e3e1889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AI\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e206daf08d4c048d639c2772b3dba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a975f061f25145a09008031385ab0da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d561cfb3854abaac553d2bbcc3bd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea4035247f14f6d87f0482330724cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968851bae7f34aafb98e6c531b13a308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토큰화 및 데이터셋 생성\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토큰 모델 로드 및 객체 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# 토큰화 함수\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['src'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "743075a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f71172f9434023b99ccab4042eddc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 모델 로드 및 객체 생성\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=5 # 학술논문, 공시정보, 뉴스, 규정, 보고서\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "948b89db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AI\\AppData\\Local\\Temp\\ipykernel_29384\\253437258.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_models/results_finance_classification\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f4c9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 1:09:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.476776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.177600</td>\n",
       "      <td>0.518957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089800</td>\n",
       "      <td>0.515019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1689, training_loss=0.23191845522892413, metrics={'train_runtime': 4191.0493, 'train_samples_per_second': 6.442, 'train_steps_per_second': 0.403, 'total_flos': 3552094923264000.0, 'train_loss': 0.23191845522892413, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ca5f09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llm_models/translation_model_finance_classification\\\\tokenizer_config.json',\n",
       " './llm_models/translation_model_finance_classification\\\\special_tokens_map.json',\n",
       " './llm_models/translation_model_finance_classification\\\\vocab.txt',\n",
       " './llm_models/translation_model_finance_classification\\\\added_tokens.json',\n",
       " './llm_models/translation_model_finance_classification\\\\tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 적용된 모델 저장\n",
    "model.save_pretrained(\"./llm_models/translation_model_finance_classification\")\n",
    "tokenizer.save_pretrained(\"./llm_models/translation_model_finance_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "722e4e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.515018880367279,\n",
       " 'eval_runtime': 52.7074,\n",
       " 'eval_samples_per_second': 18.973,\n",
       " 'eval_steps_per_second': 1.195,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0b6ed46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 클래스: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 추론 예시\n",
    "# 학술논문(0)/공시정보(1)/뉴스(2)/규정(3)/보고서(4)\n",
    "\n",
    "# 학술논문(0)\n",
    "text = \"국가별 대미 금리차의 요인분해\"\n",
    "text = \"실제로 이런 점을 고려하여 5대 재벌과 6～30대 재벌에 대 해 따로 더미변수를 넣어보았지만, 추정 결과에서는 둘 간에 유의한 차이가 나타나지 않았다.\"\n",
    "# 공시정보(1)\n",
    "text = \"상기 '2.사채의 권면(전자등록)총액'은 5년 조기 중도상환옵션 증권 2,090억 원, 10년 조기 중도상환옵션 증권 600억 원입니다.\"\n",
    "text = \"주식회사 창대핫멜시트(이하\"\"당사\"\")는 2003년 12월 개인사업자로 창업하여 2009년 4월에 법인으로 전환하였습니다.\"\n",
    "# 뉴스(2)\n",
    "text = \"NH농협은행 광주본부는 8일 광주시교육청, 초록우산어린이재단 광주본부에 돌봄 이웃 청소년을 위한 코로나19 감염 예방 물품(건강꾸러미)을 전달했다.\"\n",
    "text = \"코어로직은 40여년간의 주택 거래 자료 등을 토대로 집값 추세를 추적하는 코어로직 HPI지수를 산출하고 있으며 매달 보고서를 통해 예측치도 공개하고 있다.\"\n",
    "# 규정(3) -\n",
    "text = \"제1항의 규정에 의하여 협상을 의뢰한 경우 주무부서의 장은 관계부처협의 등 협상에 필요한 사항을 지원하여야 하며, 당해 사업을 담당하는 과장 또는 주무부서의 장이 임명한 자는 협상단원으로 참가하여야 한다.\"\n",
    "text = \"(나) 임대기간 : 총 50년 범위내(10년 단위로 갱신계약할 수 있음)\"\n",
    "# 보고서(4) -\n",
    "text = \"설립목적에 규정되어 있지 않더라도 지급결제의 원활화와 금융시장의 안정 유지도 실질적으로 한국은행의 중요한 정책목표라고 할 수 있다.\"\n",
    "text = \"주민세는 단독 세대수 증가, 법인･개인사업자 증가 추이와 30세 미만 미혼 세대주 과세제외 추진에 따른 감소분을 감안하여 1.1% 반영\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "predicted_class = outputs.logits.argmax(-1).item()\n",
    "print(\"예측 클래스:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
