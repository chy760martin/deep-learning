{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 다국어 기계 번역 모델 & 파인 튜닝\n",
    "# - Hugging Face 라이브러리 적용\n",
    "# - AI HUB 금융 보고서 다국어 번역 데이터셋 적용\n",
    "# - 입력된 문장을 다국어 기계 번역 모델을 통한 영어->한국어, 한국어->번역\n",
    "# 1. 학습 목표\n",
    "# - 구조 최적화 및 파이프라인 단순화\n",
    "# - AI HUB 금융 보고서 다국어 번역 데이터셋 전처리\n",
    "# - 병렬 문장쌍 데이터셋 변환 전처리\n",
    "# - 토크나이징 및 토크나이징 전처리\n",
    "# - 베이스 모델 로드\n",
    "# - LoRA(Low-Rank Adaptation) 설정, 특정 레이어에 작은 저차원 행렬(랭크 r)을 삽입해서 학습\n",
    "# - LoRA(Low-Rank Adaptation) 모델, 메모리 효율성/빠른 학습/도메인 적용, base 모델에 여러 LoRA 모듈을 붙였다 떼었다 할 수 있음\n",
    "# - 학습 args 설정\n",
    "# - Trainer 정의\n",
    "# - Trainer 실행\n",
    "# - LoRA 적용된 모델 저장, LoRA모델/토크나이저\n",
    "# - LoRA 적용된 모델 불러오기, 베이스모델/LoRA모델/토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26bf0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0 cpu\n",
      "CUDA 사용 가능 여부: False\n",
      "PyTorch CUDA 버전: None\n",
      "빌드 정보: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob, json, re, os, random, csv, zipfile\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__, device)\n",
    "\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"빌드 정보:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 - AI HUB 금융 보고서 다국어 번역 데이터셋\n",
    "zip_path = './llm_data/TL_3. 보고서_1. 영어.zip'\n",
    "extract_dir = './llm_data/ai_hub_report'\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "# 압축 풀기\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print('압축 해제 완료:', extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49046719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리 - AI HUB 금융 보고서 다국어 번역 데이터셋\n",
    "ko_lines, en_lines = [], []\n",
    "folders = [ # 폴더 리스트 정의\n",
    "    './llm_data/ai_hub_report/*.json'\n",
    "]\n",
    "\n",
    "# 모든 JSON 읽기\n",
    "for folder in folders:\n",
    "    for path in glob.glob(folder): # 특정 디렉토리에서 지정한 패턴과 일치하는 모든 파일 경로를 리스트로 반환\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            try:\n",
    "                data = json.load(f) # 파일 전체 로드(dict 구조)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "            # sents 리스트에서 문장쌍 추출\n",
    "            for sent in data.get('sents', []):\n",
    "                en = sent.get('mtpe') # 원문(영어)\n",
    "                ko = sent.get('source_cleaned') # 최종번역문(한국어) 추출\n",
    "\n",
    "                if en and ko and ko != 'N/A':\n",
    "                    en_lines.append(en.strip())\n",
    "                    ko_lines.append(ko.strip())\n",
    "\n",
    "# 1. Detokenize 함수 정의\n",
    "def detokenize_sentence(sentence: str) -> str:\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"\\s+([?.!,])\", r\"\\1\", sentence)  # \" ?\" → \"?\"\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence)            # 여러 공백 → 하나\n",
    "    return sentence\n",
    "\n",
    "# 2. 데이터셋 전처리\n",
    "en_lines = [detokenize_sentence(s) for s in en_lines]\n",
    "ko_lines = [detokenize_sentence(s) for s in ko_lines]\n",
    "\n",
    "\n",
    "print(f'총 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')\n",
    "print(ko_lines[0])\n",
    "print(en_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879c0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 - 중복 제거 및 순서 유지\n",
    "# pairs = list(set(zip(en_lines, ko_lines)))\n",
    "# en_lines, ko_lines = zip(*pairs) # 다시 분리\n",
    "seen = set()\n",
    "pairs = []\n",
    "for en, ko in zip(en_lines, ko_lines):\n",
    "    if (en, ko) not in seen:\n",
    "        # 새로운 문장쌍을 집합에 기록, 이후 같은 문장쌍이 나오면 if 조건에서 걸러져 추가되지 않는다\n",
    "        seen.add( (en, ko) )\n",
    "        \n",
    "        # 중복이 아닌 문장쌍을 리스트에 추가, 원래 순서대로 중복 없는 문장쌍 리스트가 만들어 진다\n",
    "        # - pairs는 [(\"Hello\",\"안녕\"), (\"Goodbye\",\"잘가\")] \n",
    "        pairs.append( (en, ko) )\n",
    "\n",
    "# 이를 다시 분리 - 영어 문장들만 모아 (\"Hello\",\"Goodbye\"), 한국어 문장들만 모아 (\"안녕\",\"잘가\")\n",
    "en_lines, ko_lines = zip(*pairs)\n",
    "\n",
    "print(f'중복 제거 후 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')\n",
    "print(ko_lines[0])\n",
    "print(en_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 - 샘플링 추가\n",
    "\n",
    "# 샘플링 최대 50,000 문장만 사용\n",
    "# sample_size = 50000\n",
    "sample_size = len(en_lines)\n",
    "if len(ko_lines) > sample_size:\n",
    "    indices = random.sample(range(len(ko_lines)), sample_size)\n",
    "    ko_lines = [ ko_lines[i] for i in indices ]\n",
    "    en_lines = [ en_lines[i] for i in indices ]\n",
    "\n",
    "print(f'샘플링 후 문장쌍 개수: {len(ko_lines)}, {len(en_lines)}')\n",
    "print(ko_lines[0])\n",
    "print(en_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 - 저장\n",
    "out_dir = './llm_data/ai_hub_report_translation'\n",
    "\n",
    "# 폴더 없을시 생성\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f'폴더 생성 완료: {out_dir}')\n",
    "else:\n",
    "    print(f'이미 존재하는 폴더: {out_dir}')\n",
    "\n",
    "ko_path = f'{out_dir}/train_ko.txt'\n",
    "en_path = f'{out_dir}/train_en.txt'\n",
    "\n",
    "with open(ko_path, 'w', encoding='utf-8') as fko, \\\n",
    "    open(en_path, 'w', encoding='utf-8') as fen:\n",
    "    for k, e in zip(ko_lines, en_lines):\n",
    "        fko.write(k + '\\n')\n",
    "        fen.write(e + '\\n')\n",
    "print('저장 완료', ko_path, en_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55675c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Hub 금융 보고서 데이터셋(train_ko.txt, train_en.txt) -> CSV로 변환해 파인 튜닝\n",
    "# 원본 데이터는 train_ko.txt, train_en.txt로 분리 -> 병렬 문장쌍을 만들어야 함\n",
    "# 파인튜닝시 양방향 번역을 지원하려면 같은 문장쌍은 en->ko, ko->en 두방향으로 모두 포함해야 함\n",
    "# CSV 구조 예시\n",
    "# - src,tgt,src_lang,tgt_lang\n",
    "# - You can buy it from a convenience store try it out.,편의점에서 사실 수 있으니 시도해보시길 바랍니다.,en,ko\n",
    "# - 편의점에서 사실 수 있으니 시도해보시길 바랍니다.,You can buy it from a convenience store try it out.,ko,en\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# 전체 데이터 로드 - AI Hub 보고서 데이터셋(train_ko.txt, train_en.txt)\n",
    "with open('./llm_data/ai_hub_report_translation/train_en.txt', 'r', encoding='utf-8') as f_en, \\\n",
    "    open('./llm_data/ai_hub_report_translation/train_ko.txt', 'r', encoding='utf-8') as f_ko:\n",
    "    en_lines = f_en.read().splitlines()\n",
    "    ko_lines = f_ko.read().splitlines()\n",
    "\n",
    "# 데이터 개수 제한 (예: 100개)\n",
    "limit = 500\n",
    "en_lines = en_lines[:limit]\n",
    "ko_lines = ko_lines[:limit]\n",
    "\n",
    "# train/valid split(90 : 10)\n",
    "split_idx = int(len(en_lines) * 0.9)\n",
    "train_en, valid_en = en_lines[:split_idx], en_lines[split_idx:]\n",
    "train_ko, valid_ko = ko_lines[:split_idx], ko_lines[split_idx:]\n",
    "\n",
    "print(len(train_en))\n",
    "print(len(valid_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 병렬 데이터 생성 - 편향되지 않은 데이터셋 생성\n",
    "# 영어->한국어, 한국어->영어\n",
    "\n",
    "# train.csv 생성\n",
    "with open('./llm_data/ai_hub_report_translation/train.csv', 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang'])\n",
    "\n",
    "    print(train_en[0])\n",
    "    print(train_ko[0])\n",
    "    for en, ko in zip(train_en, train_ko):\n",
    "        en, ko = en.strip(), ko.strip()\n",
    "        if not en or not ko:\n",
    "            continue\n",
    "\n",
    "        # 영어 -> 한국어\n",
    "        writer.writerow([en, ko, 'en', 'ko'])\n",
    "        # 한국어 -> 영어\n",
    "        writer.writerow([ko, en, 'ko', 'en'])\n",
    "\n",
    "# valid.csv 생성\n",
    "with open('./llm_data/ai_hub_report_translation/valid.csv', 'w', encoding='utf-8', newline='') as f_out:\n",
    "    writer = csv.writer(f_out)\n",
    "    writer.writerow(['src', 'tgt', 'src_lang', 'tgt_lang'])\n",
    "\n",
    "    for en, ko in zip(valid_en, valid_ko):\n",
    "        en, ko = en.strip(), ko.strip()\n",
    "        if not en or not ko:\n",
    "            continue\n",
    "        \n",
    "        # 영어 -> 한국어\n",
    "        writer.writerow([en, ko, 'en', 'ko'])\n",
    "        # 한국어 -> 영어\n",
    "        writer.writerow([ko, en, 'ko', 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "420188a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 전처리\n",
    "from datasets import load_dataset # Hugging Face의 데이터셋 관리 라이브러리(학습용 데이터 로딩에 사용)\n",
    "# M2M100 모델과 토크나이저, 학습 관련 유틸리티 제공\n",
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model # LoRA 설정을 위한 라이브러리(모델 파라미터 효율적 파인튜닝)\n",
    "\n",
    "# 1. tokenizer 로드, Hugging Face M2M100-418M 모델의 토크나이저 로드\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# 2. 전처리 함수 (동적 tgt_lang 설정)\n",
    "def preprocess_function(examples):\n",
    "    # 데이터셋에서 src(원문), tgt(번역문), tgt_lang(목표 언어 코드) 가져옴\n",
    "    inputs = examples[\"src\"]\n",
    "    targets = examples[\"tgt\"]\n",
    "    tgt_langs = examples[\"tgt_lang\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, # src(원문)을 토큰화\n",
    "        max_length=128,  # 최대 길이 128\n",
    "        truncation=True, # 최대 길이 초과시 잘라냄\n",
    "        padding=\"max_length\" # 길이가 부족하면 padding 처리\n",
    "    )\n",
    "\n",
    "    labels_list = []\n",
    "    for text, lang in zip(targets, tgt_langs):\n",
    "        # 각 문장마디 목표 언어(tgt_lang)\n",
    "        if lang in tokenizer.lang_code_to_id:\n",
    "            tokenizer.tgt_lang = lang\n",
    "        else:\n",
    "            tokenizer.tgt_lang = \"en\"  # 기본값\n",
    "        \n",
    "        # 타겟 문장 토큰화\n",
    "        with tokenizer.as_target_tokenizer(): # 번역 대상 문장을 토큰화할때 사용하는 모드\n",
    "            labels = tokenizer(\n",
    "                text, # tgt(번역문)\n",
    "                max_length=128, \n",
    "                truncation=True, \n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        # labels[\"input_ids\"]를 추출해서 학습용 정답(label)로 저장\n",
    "        labels_list.append(labels[\"input_ids\"])\n",
    "\n",
    "    # 입력(src)와 정답(tgt)을 모두 포함한 딕셔너리 반환, Trainer가 이 반환값을 받아서 학습에 사용\n",
    "    # {\n",
    "    #     \"input_ids\": [...],        # 원문 토큰 시퀀스\n",
    "    #     \"attention_mask\": [...],   # 패딩 여부 표시\n",
    "    #     \"labels\": [...]            # 번역문 토큰 시퀀스\n",
    "    # }\n",
    "    model_inputs[\"labels\"] = labels_list\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd609d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기\n",
    "dataset = load_dataset( # Hugging Face datasets 라이브러리를 사용해 CSV 파일을 로드\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"./llm_data/ai_hub_report_translation/train.csv\",\n",
    "        \"validation\": \"./llm_data/ai_hub_report_translation/valid.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 토크나이즈 적용\n",
    "# - 결과: 각 샘플이 {\"input_ids\": ..., \"attention_mask\": ..., \"labels\": ...} \n",
    "tokenized_dataset = dataset.map( # map() 함수로 앞서 정의한 preprocess_function을 데이터셋 전체에 적용\n",
    "    preprocess_function, \n",
    "    batched=True # 여러 샘플을 한번에 처리하여 속도 향상\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "\n",
    "# pretrained model M2M100_418M 로드, 베이스 모델로 사용\n",
    "base_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# LoRA 설정\n",
    "lora_config = LoraConfig(\n",
    "    r=8,              # 랭크 크기(저차원 행렬 크기), 작을수록 가볍고 빠르지만 표현력이 줄어듬\n",
    "    lora_alpha=32,    # 스케일링 계수, 학습된 LoRA 행렬을 원래 모델에 얼마나 반영할지 결정하는 스케일\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Attention 모듈에 적용, Query/Value 저차원 행렬만 학습\n",
    ")\n",
    "\n",
    "# LoRA 모델 생성, PEFT(Param-Efficient Fine-Tuning) 모델 생성\n",
    "model = get_peft_model(base_model, lora_config) # 전체 모델 파라미터는 그대로 두고 LoRA 모듈만 학습 대상이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4780be24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_models/results_lora_ai_hub_report\",    # 학습 결과(모델, 체크포인트) 저장 경로\n",
    "    eval_strategy=\"epoch\",          # 구버전, 매 epoch마다 평가\n",
    "    learning_rate=2e-4,             # 학습률 2e-5 - 5e-5\n",
    "    per_device_train_batch_size=16, # 학습 배치 크기\n",
    "    per_device_eval_batch_size=16,  # 평가 배치 크기\n",
    "    num_train_epochs=3,             # 학습 epoch 수\n",
    "    weight_decay=0.01,              # 가중치 감쇠(정규화)\n",
    "    save_total_limit=2,             # 체크포인트 최대 저장 개수\n",
    "    logging_dir=\"./llm_models/results_lora_logs_ai_hub_report\",           # 로그 저장 경로\n",
    "    logging_steps=100,              # 100 step마다 로그 기록\n",
    ")\n",
    "\n",
    "# Trainer 정의\n",
    "trainer = Trainer(\n",
    "    model=model,        # LoRA 적용된 모델\n",
    "    args=training_args, # 학습 설정\n",
    "    train_dataset=tokenized_dataset[\"train\"],       # 학습 데이터셋\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],   # 평가 데이터셋\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8ca31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='171' max='171' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [171/171 3:52:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.478204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.377900</td>\n",
       "      <td>5.272573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.377900</td>\n",
       "      <td>5.250446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=171, training_loss=6.0061083341899675, metrics={'train_runtime': 14030.1559, 'train_samples_per_second': 0.192, 'train_steps_per_second': 0.012, 'total_flos': 733843921305600.0, 'train_loss': 6.0061083341899675, 'epoch': 3.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 실행\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3218a337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO: 그것은 문자 그대로 생명의 기초입니다.\n",
      "KO→EN: It is literally the basis of life.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)   # 입력도 GPU로 이동\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)   # 입력도 GPU로 이동\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id)\n",
    "print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40cb8db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llm_models/translation_model_ai_hub_report_lora/tokenizer_config.json',\n",
       " './llm_models/translation_model_ai_hub_report_lora/special_tokens_map.json',\n",
       " 'llm_models/translation_model_ai_hub_report_lora/vocab.json',\n",
       " 'llm_models/translation_model_ai_hub_report_lora/sentencepiece.bpe.model',\n",
       " './llm_models/translation_model_ai_hub_report_lora/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 적용된 모델 저장\n",
    "model.save_pretrained(\"./llm_models/translation_model_ai_hub_report_lora\")\n",
    "tokenizer.save_pretrained(\"./llm_models/translation_model_ai_hub_report_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f98cc413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import M2M100Tokenizer, M2M100ForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "\n",
    "# 원본 M2M100 모델 로드\n",
    "base_model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# LoRA 어댑터 붙여서 불러오기\n",
    "model = PeftModel.from_pretrained(base_model, \"./llm_models/translation_model_ai_hub_report_lora\")\n",
    "\n",
    "# 토크나이저도 불러오기\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"./llm_models/translation_model_ai_hub_report_lora\")\n",
    "\n",
    "# 디바이스 맞추기\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48403b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO:\n",
      "The central bank decided to maintain interest rates to stabilize market expectations. → 중앙은행은 시장 기대를 안정화하기 위해 금리를 유지하기로 결정했다.\n",
      "The company announced a new investment plan focusing on renewable energy projects. → 회사는 재생 에너지 프로젝트에 초점을 맞춘 새로운 투자 계획을 발표했습니다.\n",
      "Under the revised regulation, banks must strengthen their capital reserves. → 개정 규정에 따라 은행은 자본 준비금을 강화해야 합니다.\n",
      "Consumer confidence has declined due to global economic uncertainties. → 세계경제 불확실성으로 인해 소비자 신뢰가 줄어들었다.\n",
      "The government introduced tax incentives to support small businesses. → 정부는 소규모 기업을 지원하기 위해 세제 혜택을 도입했다.\n",
      "Climate change is expected to intensify supply chain disruptions. → 기후변화는 공급망 혼란을 증가시킬 것으로 예상된다.\n",
      "The merger of two tech firms will reshape the industry landscape. → 두 개의 기술회사의 합병이 산업 구도를 재구성할 것이다.\n",
      "Remote work has improved productivity but blurred work-life boundaries. → 원격 노동은 생산성을 향상시켰지만 노동생명의 한계가 흐려졌다.\n",
      "The trade agreement aims to reduce tariffs and foster cooperation among nations. → 무역협정의 목적은 관세를 줄이고 국가 간의 협력을 촉진하는 것이다.\n",
      "The financial institution tightened lending criteria to mitigate default risks. → 금융기관은 부도 위험을 줄이기 위해 대출 기준을 강화했다.\n",
      "\n",
      "KO→EN:\n",
      "중앙은행은 시장 기대를 안정시키기 위해 금리를 유지하기로 결정했다. → Central Bank decided to maintain interest rates to stabilize market expectations.\n",
      "해당 기업은 재생에너지 프로젝트에 집중하는 새로운 투자 계획을 발표했다. → The company has announced a new investment plan to focus on renewable energy projects.\n",
      "개정된 규제에 따라 은행은 자본 적립을 강화해야 한다. → In accordance with the revised regulations, the banks must strengthen capabilities.\n",
      "글로벌 경제 불확실성으로 인해 소비자 신뢰가 하락했다. → The uncertainty of the global economy has led to a decline in consumer confidence.\n",
      "정부는 중소기업을 지원하기 위해 세제 혜택을 도입했다. → The government has introduced tax benefits to support small and medium enterprises.\n",
      "기후 변화는 공급망 혼란을 심화시킬 것으로 예상된다. → Climate change is expected to deepen supply chain disruptions.\n",
      "두 기술 기업의 합병은 업계 구도를 재편할 것이다. → The reunification of the two technological companies will reorganize the industry structure.\n",
      "원격 근무는 생산성을 높였지만 일과 삶의 경계를 흐리게 했다. → Remote work increased productivity, but blurred the boundaries of work and life.\n",
      "무역 협정은 관세를 줄이고 국가 간 협력을 촉진하는 것을 목표로 한다. → The trade agreement aims to reduce tariffs and promote cooperation between countries.\n",
      "금융 기관은 부도 위험을 줄이기 위해 대출 기준을 강화했다. → Financial institutions have strengthened their loans standards to reduce bankrupt risk.\n"
     ]
    }
   ],
   "source": [
    "# Glossary 분리\n",
    "glossary_ko = {\n",
    "    \"이자율\": \"금리\",\n",
    "    \"세금 자극\": \"세제 혜택\",\n",
    "    \"부패 위험\": \"부도 위험\",\n",
    "    \"자본 예비\": \"자본 준비금\",\n",
    "    \"자본 보유량\": \"자본 준비금\",\n",
    "    \"공급망 장애\": \"공급망 혼란\",\n",
    "    \"산업의 풍경\": \"산업 구도\",\n",
    "    \"노동-생명 경계\": \"일과 삶의 경계\",\n",
    "    \"금리을\": \"금리를\",\n",
    "    \"혼란를\": \"혼란을\",\n",
    "    \"구도을\": \"구도를\",\n",
    "    \"좁아졌다\": \"흐려졌다\"\n",
    "}\n",
    "\n",
    "glossary_en = {\n",
    "    \"capital earnings\": \"capital reserves\",\n",
    "    \"consumer trust\": \"consumer confidence\",\n",
    "    \"inter-national\": \"international\",\n",
    "    \"supply chain confusion\": \"supply chain disruptions\",\n",
    "    \"unemployment risks\": \"default risks\"\n",
    "}\n",
    "\n",
    "def postprocess_translation(text: str, glossary: dict) -> str:\n",
    "    # 1. Glossary 치환\n",
    "    for wrong, correct in glossary.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "\n",
    "    # 2. 조사 중복 교정\n",
    "    text = text.replace(\"을을\", \"을\")\n",
    "    text = text.replace(\"를를\", \"를\")\n",
    "    text = text.replace(\"을를\", \"를\")\n",
    "    text = text.replace(\"를을\", \"을\")\n",
    "\n",
    "    # 3. 금융 보고서 톤 교정\n",
    "    # text = text.replace(\"떨어졌다\", \"하락했다\")\n",
    "    # text = text.replace(\"올랐다\", \"상승했다\")\n",
    "    # text = text.replace(\"줄였다\", \"축소했다\")\n",
    "    # text = text.replace(\"늘어났다\", \"확대됐다\")\n",
    "    # text = text.replace(\"~할 것입니다\", \"~할 것으로 예상된다\")\n",
    "    # text = text.replace(\"~될 것입니다\", \"~될 것으로 예상된다\")\n",
    "\n",
    "    # 4. 띄어쓰기/표현 보정\n",
    "    # text = text.replace(\"했 습니다\", \"했습니다\")\n",
    "    # text = text.replace(\"했 다\", \"했다\")\n",
    "    # text = text.replace(\"것 입니다\", \"것입니다\")\n",
    "    # text = text.replace(\"재구성 할\", \"재구성할\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "texts_en = [\n",
    "    \"The central bank decided to maintain interest rates to stabilize market expectations.\",\n",
    "    \"The company announced a new investment plan focusing on renewable energy projects.\",\n",
    "    \"Under the revised regulation, banks must strengthen their capital reserves.\",\n",
    "    \"Consumer confidence has declined due to global economic uncertainties.\",\n",
    "    \"The government introduced tax incentives to support small businesses.\",\n",
    "    \"Climate change is expected to intensify supply chain disruptions.\",\n",
    "    \"The merger of two tech firms will reshape the industry landscape.\",\n",
    "    \"Remote work has improved productivity but blurred work-life boundaries.\",\n",
    "    \"The trade agreement aims to reduce tariffs and foster cooperation among nations.\",\n",
    "    \"The financial institution tightened lending criteria to mitigate default risks.\"\n",
    "]\n",
    "inputs = tokenizer(texts_en, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"EN→KO:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    processed = postprocess_translation(decoded, glossary_ko) # 후처리 적용\n",
    "    print(f\"{texts_en[i]} → {processed}\")\n",
    "    # print(f\"{texts[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "texts_ko = [\n",
    "    \"중앙은행은 시장 기대를 안정시키기 위해 금리를 유지하기로 결정했다.\",\n",
    "    \"해당 기업은 재생에너지 프로젝트에 집중하는 새로운 투자 계획을 발표했다.\",\n",
    "    \"개정된 규제에 따라 은행은 자본 적립을 강화해야 한다.\",\n",
    "    \"글로벌 경제 불확실성으로 인해 소비자 신뢰가 하락했다.\",\n",
    "    \"정부는 중소기업을 지원하기 위해 세제 혜택을 도입했다.\",\n",
    "    \"기후 변화는 공급망 혼란을 심화시킬 것으로 예상된다.\",\n",
    "    \"두 기술 기업의 합병은 업계 구도를 재편할 것이다.\",\n",
    "    \"원격 근무는 생산성을 높였지만 일과 삶의 경계를 흐리게 했다.\",\n",
    "    \"무역 협정은 관세를 줄이고 국가 간 협력을 촉진하는 것을 목표로 한다.\",\n",
    "    \"금융 기관은 부도 위험을 줄이기 위해 대출 기준을 강화했다.\"\n",
    "]\n",
    "inputs = tokenizer(texts_ko, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\\nKO→EN:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    processed = postprocess_translation(decoded, glossary_en) # 후처리 적용\n",
    "    print(f\"{texts_ko[i]} → {processed}\")\n",
    "\n",
    "    # print(f\"{texts_ko[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6c8d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO:\n",
      "The central bank decided to maintain interest rates to stabilize market expectations. → 중앙은행은 시장 기대를 안정시키기 위해 이자율을 유지하기로 결정했다.\n",
      "The company announced a new investment plan focusing on renewable energy projects. → 회사는 재생 에너지 프로젝트에 초점을 맞춘 새로운 투자 계획을 발표했습니다.\n",
      "Under the revised regulation, banks must strengthen their capital reserves. → 개정된 규정에 따라 은행은 자본 예비를 강화해야 합니다.\n",
      "Consumer confidence has declined due to global economic uncertainties. → 세계 경제 불확실성으로 인해 소비자 신뢰가 떨어졌습니다.\n",
      "The government introduced tax incentives to support small businesses. → 정부는 소규모 기업을 지원하기 위해 세금 자극을 도입했습니다.\n",
      "Climate change is expected to intensify supply chain disruptions. → 기후 변화는 공급망 장애를 증가시킬 것으로 예상된다.\n",
      "The merger of two tech firms will reshape the industry landscape. → 두 개의 기술 회사의 합병은 산업의 풍경을 재구성 할 것입니다.\n",
      "Remote work has improved productivity but blurred work-life boundaries. → 원격 작업은 생산성을 향상시켰지만 노동-생명 경계가 좁아졌습니다.\n",
      "The trade agreement aims to reduce tariffs and foster cooperation among nations. → 무역협정의 목적은 관세를 줄이고 국가 간의 협력을 촉진하는 것입니다.\n",
      "The financial institution tightened lending criteria to mitigate default risks. → 금융기관은 부패 위험을 줄이기 위해 대출 기준을 강화했습니다.\n",
      "\n",
      "KO→EN:\n",
      "중앙은행은 시장 기대를 안정시키기 위해 금리를 유지하기로 결정했다. → The central bank decided to maintain interest rates to stabilize market expectations.\n",
      "해당 기업은 재생에너지 프로젝트에 집중하는 새로운 투자 계획을 발표했다. → The company has announced a new investment plan to focus on the renewable energy project.\n",
      "개정된 규제에 따라 은행은 자본 적립을 강화해야 한다. → In accordance with the amended regulations, the bank must strengthen capital earnings.\n",
      "글로벌 경제 불확실성으로 인해 소비자 신뢰가 하락했다. → Due to global economic uncertainty, consumer trust has fallen.\n",
      "정부는 중소기업을 지원하기 위해 세제 혜택을 도입했다. → The government introduced tax benefits to support small and medium enterprises.\n",
      "기후 변화는 공급망 혼란을 심화시킬 것으로 예상된다. → Climate change is expected to deepen supply chain confusion.\n",
      "두 기술 기업의 합병은 업계 구도를 재편할 것이다. → The merger of the two technology companies will reorganize the industry.\n",
      "원격 근무는 생산성을 높였지만 일과 삶의 경계를 흐리게 했다. → Remote work increased productivity but blurred the boundaries of work and life.\n",
      "무역 협정은 관세를 줄이고 국가 간 협력을 촉진하는 것을 목표로 한다. → The trade agreement aims to reduce tariffs and promote inter-national cooperation.\n",
      "금융 기관은 부도 위험을 줄이기 위해 대출 기준을 강화했다. → The financial institutions have strengthened the loan standards to reduce unemployment risks.\n"
     ]
    }
   ],
   "source": [
    "# 베이스 모델 테스트\n",
    "import torch\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 영어 → 한국어\n",
    "text = \"It is literally the basis of life.\"\n",
    "texts = [\n",
    "    \"The central bank decided to maintain interest rates to stabilize market expectations.\",\n",
    "    \"The company announced a new investment plan focusing on renewable energy projects.\",\n",
    "    \"Under the revised regulation, banks must strengthen their capital reserves.\",\n",
    "    \"Consumer confidence has declined due to global economic uncertainties.\",\n",
    "    \"The government introduced tax incentives to support small businesses.\",\n",
    "    \"Climate change is expected to intensify supply chain disruptions.\",\n",
    "    \"The merger of two tech firms will reshape the industry landscape.\",\n",
    "    \"Remote work has improved productivity but blurred work-life boundaries.\",\n",
    "    \"The trade agreement aims to reduce tariffs and foster cooperation among nations.\",\n",
    "    \"The financial institution tightened lending criteria to mitigate default risks.\"\n",
    "]\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"ko\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"EN→KO:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"EN→KO:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n",
    "\n",
    "\n",
    "# 한국어 → 영어\n",
    "text = \"말 그대로 삶의 기초입니다.\"\n",
    "texts_ko = [\n",
    "    \"중앙은행은 시장 기대를 안정시키기 위해 금리를 유지하기로 결정했다.\",\n",
    "    \"해당 기업은 재생에너지 프로젝트에 집중하는 새로운 투자 계획을 발표했다.\",\n",
    "    \"개정된 규제에 따라 은행은 자본 적립을 강화해야 한다.\",\n",
    "    \"글로벌 경제 불확실성으로 인해 소비자 신뢰가 하락했다.\",\n",
    "    \"정부는 중소기업을 지원하기 위해 세제 혜택을 도입했다.\",\n",
    "    \"기후 변화는 공급망 혼란을 심화시킬 것으로 예상된다.\",\n",
    "    \"두 기술 기업의 합병은 업계 구도를 재편할 것이다.\",\n",
    "    \"원격 근무는 생산성을 높였지만 일과 삶의 경계를 흐리게 했다.\",\n",
    "    \"무역 협정은 관세를 줄이고 국가 간 협력을 촉진하는 것을 목표로 한다.\",\n",
    "    \"금융 기관은 부도 위험을 줄이기 위해 대출 기준을 강화했다.\"\n",
    "]\n",
    "inputs = tokenizer(texts_ko, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "# outputs = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id, num_beams=5, max_length=128)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=forced_bos_token_id,\n",
    "    num_beams=5,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# print(\"KO→EN:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\\nKO→EN:\")\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"{texts_ko[i]} → {tokenizer.decode(output, skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b185fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN→KO chrF Score:\n",
      "chrF2 = 59.34\n",
      "\n",
      "KO→EN chrF Score:\n",
      "chrF2 = 69.97\n"
     ]
    }
   ],
   "source": [
    "# sacrebleu.corpus_chrf는 chrF 점수를 계산하는 함수로, 번역된 문장(hypotheses)과 정답 문장(references) 간의 문자 단위 F-score를 측정\n",
    "# BLEU 보다 문장 길이나 형태 변화에 덜 민감해서 번역 품질 평가에 자주 쓰인다\n",
    "import sacrebleu\n",
    "\n",
    "# LoRA 파인 튜닝 정수\n",
    "# - EN→KO chrF Score:\n",
    "# - chrF2 = 59.34\n",
    "# - KO→EN chrF Score:\n",
    "# - chrF2 = 69.97\n",
    "\n",
    "# Base 모델 점수\n",
    "# - EN→KO chrF Score:\n",
    "# - chrF2 = 59.74\n",
    "# - KO→EN chrF Score:\n",
    "# - chrF2 = 70.77\n",
    "\n",
    "# 영어 → 한국어 평가\n",
    "references_ko = [ # 영어 문장의 정답 한국어 번역\n",
    "    \"중앙은행은 인플레이션을 억제하기 위해 금리를 인상하기로 결정했다.\",\n",
    "    \"외국인 투자자들은 한국 국채에 강한 관심을 보이고 있다.\",\n",
    "    \"회사는 수출 증가로 인해 분기별 수익이 크게 증가했다고 보고했다.\",\n",
    "    \"신용 리스크 관리는 금융 안정성을 유지하는 데 필수적이다.\",\n",
    "    \"새로운 규제 발표 이후 주식 시장은 높은 변동성을 경험했다.\",\n",
    "    \"환율 변동은 다국적 기업에 큰 영향을 미친다.\",\n",
    "    \"은행은 소매 고객을 위한 새로운 디지털 플랫폼을 도입했다.\",\n",
    "    \"유동성 부족은 금융 부문에서 시스템 리스크로 이어질 수 있다.\",\n",
    "    \"두 은행의 합병은 경쟁력을 향상시킬 것으로 예상된다.\",\n",
    "    \"글로벌 금융 기관들은 미국의 통화 정책 영향을 면밀히 모니터링하고 있다.\"\n",
    "]\n",
    "\n",
    "hypotheses_ko = [ # 모델이 생성한 한국어 번역\n",
    "    \"중앙은행은 인플레이션을 통제하기 위해 이자율을 올리기로 결정했다.\",\n",
    "    \"외국인 투자자들은 한국 정부의 부채에 강한 관심을 보이고 있습니다.\",\n",
    "    \"회사는 더 높은 수출으로 인해 분기 수익이 상당히 증가했습니다.\",\n",
    "    \"신용 위험 관리는 금융 안정성을 유지하는 데 필수적입니다.\",\n",
    "    \"주식 시장은 새로운 규정 발표 후 높은 변동을 경험했습니다.\",\n",
    "    \"교환율 변동은 다국적 기업에 큰 영향을 미칩니다.\",\n",
    "    \"은행은 소매 고객을위한 새로운 디지털 플랫폼을 도입했습니다.\",\n",
    "    \"유동성 결핍은 금융 부문에서 체계적인 위험을 초래할 수 있습니다.\",\n",
    "    \"두 은행의 합병은 경쟁력을 향상시킬 것으로 예상된다.\",\n",
    "    \"글로벌 금융 기관은 미국의 통화 정책의 영향을 철저히 모니터링하고 있습니다.\"\n",
    "]\n",
    "\n",
    "hypotheses_ko_lora= [\n",
    "    \"중앙은행은 인플레이션을 통제하기 위해 이자율을 올리기로 결정했다.\",\n",
    "    \"외국인 투자자들은 한국 정부의 부채에 강한 관심을 보이고 있다.\",\n",
    "    \"회사는 더 높은 수출으로 인해 분기 수익이 상당히 증가했습니다.\",\n",
    "    \"신용 위험 관리는 금융 안정성을 유지하는 데 필수적입니다.\",\n",
    "    \"주식 시장은 새로운 규정 발표 후 높은 변동을 경험했습니다.\",\n",
    "    \"교환율 변동은 다국적 기업에 큰 영향을 미칩니다.\",\n",
    "    \"은행은 소매 고객을위한 새로운 디지털 플랫폼을 도입했습니다.\",\n",
    "    \"유동성 결핍은 금융 부문에서 체계적인 위험을 초래할 수 있습니다.\",\n",
    "    \"두 은행의 합병은 경쟁력을 향상시킬 것으로 예상된다.\",\n",
    "    \"세계 금융 기관은 미국의 통화 정책의 영향을 철저히 모니터링하고 있습니다.\"\n",
    "]\n",
    "\n",
    "print(\"EN→KO chrF Score:\")\n",
    "# chrF 점수를 계산\n",
    "# chrf_ko = sacrebleu.corpus_chrf(hypotheses_ko, [references_ko])\n",
    "chrf_ko = sacrebleu.corpus_chrf(hypotheses_ko_lora, [references_ko])\n",
    "print(chrf_ko)\n",
    "\n",
    "\n",
    "# 한국어 → 영어 평가\n",
    "references_en = [\n",
    "    \"The central bank decided to raise interest rates to control inflation.\",\n",
    "    \"Foreign investors are showing strong interest in Korean government bonds.\",\n",
    "    \"The company reported a significant increase in quarterly earnings due to higher exports.\",\n",
    "    \"Credit risk management is essential for maintaining financial stability.\",\n",
    "    \"The stock market experienced high volatility after the announcement of new regulations.\",\n",
    "    \"Exchange rate fluctuations have a major impact on multinational corporations.\",\n",
    "    \"The bank introduced a new digital platform for retail customers.\",\n",
    "    \"Liquidity shortages can lead to systemic risks in the financial sector.\",\n",
    "    \"The merger between the two banks is expected to improve competitiveness.\",\n",
    "    \"Global financial institutions are closely monitoring the impact of U.S. monetary policy.\"\n",
    "]\n",
    "\n",
    "hypotheses_en = [\n",
    "    \"The central bank has decided to raise interest rates to suppress inflation.\",\n",
    "    \"Foreign investors are very interested in the Korean national debt.\",\n",
    "    \"The company that a significant increase in quarterly revenue due to increased exports.\",\n",
    "    \"Credit risk management is essential toining financial stability.\",\n",
    "    \"Since the announcement of new regulations, the stock market has experienced high volatility.\",\n",
    "    \"Currency rate variations have a great impact on multinational enterprises.\",\n",
    "    \"The bank has introduced a new digital platform for retail customers.\",\n",
    "    \"Lack of liquidity can lead to system risk in the financial sector.\",\n",
    "    \"The fusion of the two banks is expected to boost competitiveness.\",\n",
    "    \"Global financial institutions are closely monitoring the influence of U.S. currency policy.\"\n",
    "]\n",
    "\n",
    "hypotheses_en_lora= [\n",
    "    \"The central bank has decided to raise interest rates to suppress inflation.\",\n",
    "    \"Foreign investors are very interested in the Korean national debt.\",\n",
    "    \"The company that the quarterly revenue was significantly increased due to increased exports.\",\n",
    "    \"Credit risk management is essential to maintain financial stability.\",\n",
    "    \"After the announcement of new regulations, the stock market has experienced high volatility.\",\n",
    "    \"The change in exchange rates has a significant impact on multinational companies.\",\n",
    "    \"The bank has introduced a new digital platform for retail customers.\",\n",
    "    \"Lack of liquidity can lead to system risk in the financial sector.\",\n",
    "    \"The fusion of the two banks is expected to boost competitiveness.\",\n",
    "    \"Global financial institutions are closely monitoring the influence of U.S. currency policy.\"\n",
    "]\n",
    "\n",
    "print(\"\\nKO→EN chrF Score:\")\n",
    "# chrf_en = sacrebleu.corpus_chrf(hypotheses_en, [references_en])\n",
    "chrf_en = sacrebleu.corpus_chrf(hypotheses_en_lora, [references_en])\n",
    "print(chrf_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
