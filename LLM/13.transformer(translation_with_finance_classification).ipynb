{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765acad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 다국어 기계 번역 모델 분류기 및 다국어 기계 번역 \n",
    "# - Hugging Face 라이브러리 적용\n",
    "# - 입력된 문장을 다국어 기계 번역 모델 분류기을 통한 학술논문(0)/공시정보(1)/뉴스(2)/규정(3)/보고서(4) 분류\n",
    "# - 입력 문장의 언어 분류 -> 입력 문장에 해당하는 기계 번역 모델 분류 -> 입력 문장에 해당하는 기계 번역\n",
    "# 1. 학습 목표\n",
    "# - 구조 최적화 및 파이프라인 단순화\n",
    "# - AI HUB 금융 학술논문/공시정보/뉴스/규정/보고서 다국어 번역 데이터셋 학습 보강\n",
    "# - 토크나이징 및 토크나이징 전처리\n",
    "# - LoRA 적용된 모델 불러오기, 베이스모델/LoRA모델/토크나이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26bf0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu129 cuda\n",
      "CUDA 사용 가능 여부: True\n",
      "PyTorch CUDA 버전: 12.9\n",
      "빌드 정보: 2.8.0+cu129\n",
      "사용 중인 GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tqdm as notebook_tqdm\n",
    "import numpy as np\n",
    "import glob, json, re, os, random, csv, zipfile\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.__version__, device)\n",
    "\n",
    "print(\"CUDA 사용 가능 여부:\", torch.cuda.is_available())\n",
    "print(\"PyTorch CUDA 버전:\", torch.version.cuda)\n",
    "print(\"빌드 정보:\", torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"사용 중인 GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d55b8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# - Hugging Face의 pipeline(\"translation\", ...)은 단순 PeftModel 객체를 지원 않는다\n",
    "# - 대신 PeftModelForSeq2SeqLM 클래스를 사용해야 한다\n",
    "# - 즉, LoRA를 적용한 Seq2Seq 모델을 불러올 때는 PeftModelForSeq2SeqLM.from_pretrained()을 써야 한다\n",
    "from peft import PeftModel, PeftModelForSeq2SeqLM\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91a0cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './llm_models/translation_model_finance_classification' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# 분류기 모델 로드\n",
    "classifier_tokenizer = AutoTokenizer.from_pretrained('./llm_models/translation_model_finance_classification')\n",
    "classifier_model = AutoModelForSequenceClassification.from_pretrained('./llm_models/translation_model_finance_classification')\n",
    "classifier = pipeline('text-classification', model=classifier_model, tokenizer=classifier_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c448d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 번역 모델(LoRA 적용 M2M100) 로드\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/m2m100_418M\")\n",
    "\n",
    "# 각 라벨별로 LoRA weight를 적용한 모델을 불러와서 딕셔너리에 저장\n",
    "# - 분류 결과가 label=2라면 translation_models[2]를 선택해서 번역 수행\n",
    "translation_models = {\n",
    "    0: (\n",
    "        PeftModelForSeq2SeqLM.from_pretrained(base_model, './llm_models/translation_model_ai_hub_article_lora'),     # 금융 학술 논문(0)\n",
    "        AutoTokenizer.from_pretrained('./llm_models/translation_model_ai_hub_article_lora')\n",
    "    ),\n",
    "    1: (\n",
    "        PeftModelForSeq2SeqLM.from_pretrained(base_model, './llm_models/translation_model_ai_hub_disclosure'),       # 금융 공시 정보(1)\n",
    "        AutoTokenizer.from_pretrained('./llm_models/translation_model_ai_hub_disclosure')\n",
    "    ),\n",
    "    2: (\n",
    "        PeftModelForSeq2SeqLM.from_pretrained(base_model, './llm_models/translation_model_ai_hub_news_lora'),        # 금융 뉴스(2)\n",
    "        AutoTokenizer.from_pretrained('./llm_models/translation_model_ai_hub_news_lora')\n",
    "    ),\n",
    "    3: (\n",
    "        PeftModelForSeq2SeqLM.from_pretrained(base_model, './llm_models/translation_model_ai_hub_regulation_lora'),  # 금융 규정(3)\n",
    "        AutoTokenizer.from_pretrained('./llm_models/translation_model_ai_hub_regulation_lora')\n",
    "    ),\n",
    "    4: (\n",
    "        PeftModelForSeq2SeqLM.from_pretrained(base_model, './llm_models/translation_model_ai_hub_report_lora'),      # 금융 보고서(4)\n",
    "        AutoTokenizer.from_pretrained('./llm_models/translation_model_ai_hub_report_lora')\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b814f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 파이프라인\n",
    "def inference_pipeline(text):\n",
    "    # 분류\n",
    "    row_label = classifier(text)[0]['label']\n",
    "    # print('row_label : ', row_label)\n",
    "    label = int(row_label.replace('LABEL_', \"\")) # LABEL_0 -> 0\n",
    "\n",
    "    # 해당 라벨의 번역 모델 + 토크나이저 선택\n",
    "    model, tokenizer = translation_models[label]\n",
    "    translator = pipeline('translation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # 언어 판별\n",
    "    lang = detect(text)\n",
    "\n",
    "    # 번역 수행\n",
    "    if lang == 'en':\n",
    "        translated = translator(text, src_lang='en', tgt_lang='ko')[0]['translation_text']    \n",
    "    elif lang == 'ko':\n",
    "        translated = translator(text, src_lang='ko', tgt_lang='en')[0]['translation_text']\n",
    "    else:\n",
    "        translated = text\n",
    "    return {'label': label, 'translated_text': translated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63ffdd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'translated_text': '이 연구는 변압기 모델을 제안한다.'}\n",
      "{'label': 0, 'translated_text': 'This study proposes a transformer-based model.'}\n"
     ]
    }
   ],
   "source": [
    "# 추론\n",
    "text_en = \"This research proposes a Transformer model.\"\n",
    "result_en = inference_pipeline(text=text_en)\n",
    "print(result_en)\n",
    "\n",
    "text_ko = \"이 연구는 Transformer 기반 모델을 제안한다.\"\n",
    "result_ko = inference_pipeline(text=text_ko)\n",
    "print(result_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2a92bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학술논문(0): {'label': 0, 'translated_text': '이 논문에서는 금융 예측을 위한 새로운 깊은 학습 아키텍처를 소개한다.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공시정보(1): {'label': 2, 'translated_text': 'Samsung Electronics announced its fourth quarter results for 2025.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "뉴스(2): {'label': 4, 'translated_text': '글로벌 주식시장은 투자자들이 새로운 정책에 긍정적으로 반응했을 때 오늘 상승했다.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "규정(3): {'label': 4, 'translated_text': 'Financial institutions must comply with the regulations on capital stability.'}\n",
      "보고서(4): {'label': 4, 'translated_text': '연례 재무 보고서에서는 은행계의 핵심 추세를 강조한다.'}\n"
     ]
    }
   ],
   "source": [
    "# 학술논문 (0)\n",
    "text_article = \"This paper introduces a novel deep learning architecture for financial forecasting.\"\n",
    "result_article = inference_pipeline(text_article)\n",
    "print(\"학술논문(0):\", result_article)\n",
    "\n",
    "# 공시정보 (1)\n",
    "text_disclosure = \"삼성전자는 2025년 4분기 실적을 공시하였다.\"\n",
    "result_disclosure = inference_pipeline(text_disclosure)\n",
    "print(\"공시정보(1):\", result_disclosure)\n",
    "\n",
    "# 뉴스 (2)\n",
    "text_news = \"Global stock markets rose today as investors reacted positively to the new policy.\"\n",
    "result_news = inference_pipeline(text_news)\n",
    "print(\"뉴스(2):\", result_news)\n",
    "\n",
    "# 규정 (3)\n",
    "text_regulation = \"금융기관은 자본적정성 규정을 준수해야 한다.\"\n",
    "result_regulation = inference_pipeline(text_regulation)\n",
    "print(\"규정(3):\", result_regulation)\n",
    "\n",
    "# 보고서 (4)\n",
    "text_report = \"The annual financial report highlights key trends in the banking sector.\"\n",
    "result_report = inference_pipeline(text_report)\n",
    "print(\"보고서(4):\", result_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
