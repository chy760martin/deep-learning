{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP(Multi-Layer Perceptron) - 필기체손글씨 MNIST Dataset\n",
    "\n",
    "# 딥러닝 MLP 아키텍처\n",
    "# - 학습데이터 -> 입력층>은닉층>ReLU>Dropout>출력층>Softmax -> 손실함수\n",
    "\n",
    "# 입력층\n",
    "# - (28,28) 크기를 가지는 2차원 이미지를 784개(28x28)의 길이를 갖는 1차원 벡터로 변환해서 은닉층으로 전달\n",
    "# 은닉층\n",
    "# - 일반적으로 1개 이상의 은닉층으로 이루어지며 각각의 은닉층은 내부에 많은 노드(node)로 구성\n",
    "# - 은닉층 개수과 노드 개수는 학습데이터와 성능을 고려하여 최적의 값을 찾아야 하는 하이퍼 파라미터\n",
    "# ReLU\n",
    "# - 활성화함수로서 입력값이 0보다 크면 입력값 그대로 내보내고, 0보다 작으면 0을 출력하는 비선형함수\n",
    "# Dropout\n",
    "# - 학습데이터에 대해 과적합(overfitting)을 줄이기 위해 신경망 뉴런을 주어진 확률값에 따라 부분적으로 제거(drop)해주는 함수\n",
    "# 출력층\n",
    "# - 출력층 노드 개수는 정답(label) 개수와 같은 10개로 설정\n",
    "# - 학습데이터의 정답은 0~9까지 총 10개 가운데 하나의 숫자로 표현되기 때문에 출력층 노드 개수가 또한 정답 개수와 동일한 10개\n",
    "# Softmax\n",
    "# - 다중 클래스 분류 모델을 만들때 사용\n",
    "# - 입력된 벡터의 각 요소를 0~1 사이의 확률값으로 변환해주는 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bf6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import, GPU 설정\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print('pytorch version : ', torch.__version__, ', device : ', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5225e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset 다운로드\n",
    "train_dataset = datasets.MNIST(root='.\\\\data\\\\MNIST_data',\n",
    "                               train=True, # train\n",
    "                               transform=transforms.ToTensor(), # 이미지 픽셀값 0~255까지의 값을 0~1 사이의 값을 변환\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='.\\\\data\\\\MNIST_data',\n",
    "                               train=False, # test\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67858b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11102efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset 분리(train : validation = 85% : 15%)\n",
    "train_dataset_size = int(len(train_dataset) * 0.85) # train 85%\n",
    "validation_dataset_size = int(len(train_dataset) * 0.15) # validation 15%\n",
    "train_dataset, validation_dataset = random_split(dataset=train_dataset,\n",
    "                                                 lengths=[train_dataset_size, validation_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분리된 데이터 확인\n",
    "print(len(train_dataset), len(validation_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aa53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, Dataloader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset_loader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=True)\n",
    "\n",
    "validation_dataset_loader = DataLoader(dataset=validation_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=False)\n",
    "\n",
    "test_dataset_loader = DataLoader(dataset=test_dataset,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b361c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset 1개 배치 데이터 확인\n",
    "images, labels = next(iter(train_dataset_loader))\n",
    "\n",
    "# labels map 생성\n",
    "labels_map = { v:k for k, v in train_dataset.dataset.class_to_idx.items() }\n",
    "print(labels_map)\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "cols, rows = 4, 4\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(images), size=(1,)).item()\n",
    "    img, label = images[sample_idx], labels[sample_idx].item()\n",
    "    \n",
    "    plt.subplot(cols, rows, i)\n",
    "    plt.imshow(torch.permute(img, (1,2,0)))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model\n",
    "class MLPDeepLearningModel(nn.Module):\n",
    "    # model 정의 - 아키텍처를 구성하는 다양한 계층(layer)을 정의\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # 입력층\n",
    "        x = self.fc1(x) # 은닉층\n",
    "        x = self.relu(x) # 활성화함수 ReLU(비선형함수)\n",
    "        x = self.dropout(x) # overfitting 방지\n",
    "        x = self.fc2(x) # 출력층\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7599bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 객체 생성\n",
    "model = MLPDeepLearningModel().to(DEVICE)\n",
    "\n",
    "# loss function, CrossEntropyLoss 손실함수에는 Softmax 함수 포함되어 있음\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b3ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model train 함수\n",
    "def model_train(dataloader, model, loss_function, optimizer):\n",
    "    model.train() # 확습모드\n",
    "\n",
    "    train_loss_sum = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    total_train_batch = len(dataloader)\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        # 처음 크기는 (batch_size, 1, 28, 28) -> (batch_size, 784) 1차원 벡터로 변환\n",
    "        x_train = images.view(-1, 28 * 28).to(DEVICE)\n",
    "        y_train = labels.to(DEVICE)\n",
    "\n",
    "        # 모델 예측값 게산\n",
    "        outputs = model(x_train)\n",
    "\n",
    "        # 손실함수값 계산\n",
    "        loss = loss_function(outputs, y_train)\n",
    "\n",
    "        # 오차역전파\n",
    "        optimizer.zero_grad() # 미분 연산 초기화\n",
    "        loss.backward() # 미분 연산\n",
    "        optimizer.step() # 미분 연산 후 가중치 바이어스 파라미터 업데이트\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        train_total += y_train.size(0)\n",
    "        train_correct += (torch.argmax(outputs, 1) == y_train).sum().item()\n",
    "    train_avg_loss = train_loss_sum / total_train_batch # 평균 오차 계산\n",
    "    train_avg_accuracy = 100 * train_correct / train_total # 평균 정확도 계산\n",
    "\n",
    "    return train_avg_loss, train_avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36dd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluate 함수\n",
    "def model_evaluate(dataloader, model, loss_function, optimizer):\n",
    "    model.eval() # 추론모드\n",
    "\n",
    "    with torch.no_grad(): # 미분 연산 하지 않음\n",
    "        val_loss_sum = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        total_val_batch = len(dataloader)\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            # 처음 크기는 (batch_size, 1, 28, 28) -> (batch_size, 784) 1차원 벡터로 변환\n",
    "            x_val = images.view(-1, 28 * 28).to(DEVICE)\n",
    "            y_val = labels.to(DEVICE)\n",
    "\n",
    "            # 모델 예측값 게산\n",
    "            outputs = model(x_val)\n",
    "\n",
    "            # 손실함수값 계산\n",
    "            loss = loss_function(outputs, y_val)\n",
    "\n",
    "            # 오차역전파\n",
    "            # optimizer.zero_grad() # 미분 연산 초기화\n",
    "            # loss.backward() # 미분 연산\n",
    "            # optimizer.step() # 미분 연산 후 가중치 바이어스 파라미터 업데이트\n",
    "\n",
    "            val_loss_sum += loss.item()\n",
    "            val_total += y_val.size(0)\n",
    "            val_correct += (torch.argmax(outputs, 1) == y_val).sum().item()\n",
    "        val_avg_loss = val_loss_sum / total_val_batch # 평균 오차 계산\n",
    "        val_avg_accuracy = 100 * val_correct / val_total # 평균 정확도 계산\n",
    "\n",
    "        return val_avg_loss, val_avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model test 함수\n",
    "def model_test(dataloader, model):\n",
    "    model.eval() # 추론모드\n",
    "\n",
    "    with torch.no_grad(): # 미분 연산 하지 않음\n",
    "        test_loss_sum = 0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        total_test_batch = len(dataloader)\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            # 처음 크기는 (batch_size, 1, 28, 28) -> (batch_size, 784) 1차원 벡터로 변환\n",
    "            x_test = images.view(-1, 28 * 28).to(DEVICE)\n",
    "            y_test = labels.to(DEVICE)\n",
    "\n",
    "            # 모델 예측값 게산\n",
    "            outputs = model(x_test)\n",
    "\n",
    "            # 손실함수값 계산\n",
    "            loss = loss_function(outputs, y_test)\n",
    "\n",
    "            # 오차역전파\n",
    "            # optimizer.zero_grad() # 미분 연산 초기화\n",
    "            # loss.backward() # 미분 연산\n",
    "            # optimizer.step() # 미분 연산 후 가중치 바이어스 파라미터 업데이트\n",
    "\n",
    "            test_loss_sum += loss.item()\n",
    "            test_total += y_test.size(0)\n",
    "            test_correct += (torch.argmax(outputs, 1) == y_test).sum().item()\n",
    "        test_avg_loss = test_loss_sum / total_test_batch\n",
    "        test_avg_accuracy = 100 * test_correct / test_total\n",
    "\n",
    "        return test_avg_loss, test_avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb9be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 학습\n",
    "train_loss_list = []\n",
    "train_accuracy_list = []\n",
    "\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # train\n",
    "    train_avg_loss, train_avg_accuracy = model_train(dataloader=train_dataset_loader,\n",
    "                                                     model=model,\n",
    "                                                     loss_function=loss_function,\n",
    "                                                     optimizer=optimizer)\n",
    "    train_loss_list.append(train_avg_loss)\n",
    "    train_accuracy_list.append(train_avg_accuracy)\n",
    "\n",
    "    # evaluate\n",
    "    val_avg_loss, val_avg_accuracy = model_evaluate(dataloader=validation_dataset_loader,\n",
    "                                                     model=model,\n",
    "                                                     loss_function=loss_function,\n",
    "                                                     optimizer=optimizer)\n",
    "    val_loss_list.append(val_avg_loss)\n",
    "    val_accuracy_list.append(val_avg_accuracy)\n",
    "\n",
    "    # print\n",
    "    print(\n",
    "        'epoch : ', '%02d' % (epoch + 1),\n",
    "        'train loss = ', '{:4f}'.format(train_avg_loss), ', train acc = ', '{:4f}'.format(train_avg_accuracy),\n",
    "        'val loss = ', '{:4f}'.format(val_avg_loss), ', val acc = ', '{:4f}'.format(val_avg_accuracy)\n",
    "    )\n",
    "\n",
    "# time\n",
    "end_time = datetime.now()\n",
    "print('elapsed time => ', end_time -start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Dataset 정확도, 오차 테스트\n",
    "model_test(dataloader=test_dataset_loader,\n",
    "            model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c7ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 - 손실함수\n",
    "plt.title('Loss Trend')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_loss_list, label='train loss')\n",
    "plt.plot(val_loss_list, label='validation loss')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e9612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 - 정확도\n",
    "plt.title('Accuracy Trend')\n",
    "plt.xlabel('accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(train_accuracy_list, label='train accuracy')\n",
    "plt.plot(val_accuracy_list, label='validation accuracy')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
