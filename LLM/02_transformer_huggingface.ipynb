{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f90abaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers 패키지 설치\n",
    "# !pip install transformers==4.50.0 datasets==3.5.0 huggingface_hub==0.29.0 -qqq \n",
    "# !pip install --upgrade transformers tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e50da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 15:51:11.056924: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# BERT 와 GPT-2 모델을 활용할때 허깅페이스 트랜스포머 코드 비교\n",
    "\n",
    "# 허깅페이스 트렌스포머를 활용하면 서로 다른 조직에서 개발한 BERT와 GPT-2 모델을 거의 동인한 인터페이스로 활용 가능\n",
    "# AutoModel, AutoTokenizer 클래스를 사용해 BERT 및 GPT-2 모델과 토큰나이저를 불러오고 토큰화를 수행해서 모델에 입력으로 넣어준다\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "text = 'What is Huggingface Transformers?'\n",
    "\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased') # pre-trained model(BERT) 불러오기\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # 토크나이저 불러오기\n",
    "encoded_input =  bert_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "bert_output = bert_model(**encoded_input) # 모델에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6438e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 모델 활용\n",
    "gpt_model = AutoModel.from_pretrained('gpt2') # pre-trained model(GPT) 불러오기\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained('gpt2') # 토크나이저 불러오기\n",
    "encoded_input = gpt_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "gpt_output = gpt_model(**encoded_input) # 모델에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366d15d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 허깅페이스 트랜스포머 라미이브러리\n",
    "# - 허깅페이스에는 모델을 바디(body), 헤드(head)로 구분, 같은 바디를 사용하면서 다른 작업에 사용 할 수 이도록 만들기 위함\n",
    "# 예) 구글 BERT 모델을 사용하지만 다른 헤드를 사용 할 수가 있다\n",
    "# - 문장 전체가 긍정인지 부정인지를 분류하는 모델, 이때는 바디가 반환하는 여러 잠재 상태(hidden state)중 가장 앞에 있는 [CLS] 토큰의 데이터만 받아 예측\n",
    "# - 각 토큰이 사람이나 장소에 해당하는지 판단하는 개처명 인식(named entity recognition) 모델인데, 각 토큰에 대해 판단해야 하기 때문에 모든 토튼의\n",
    "# - 데이터를 받아 각각 사람인지 장소인지를 예측\n",
    "\n",
    "# 모델 아이디로 모델 불러오기 코드 - 모댈의 바디만 불어오는 코드\n",
    "from transformers import AutoModel\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model.save_pretrained('./transformers_models/klue/roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae1c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류 헤드가 포함된 모델 불러오기 - SamLowe/roberta-base-go_emotions 저장소에서 텍스트 분류 모델을 내려 받아 classification_model 변수에 저장\n",
    "# SamLowe/roberta-base-go_emotions 모델은 분류 헤드가 포함(입력 문장이 어떤 감성을 나타내는지 분류하는데, 감탄/즐거움/화 등과 같은 감정이 포함되어 있는지 분류)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model.save_pretrained('./transformers_models/SamLowe/roberta-base-go_emotions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8815b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 분류 헤드가 랜덤으로 초기화된 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model.save_pretrained('./transformers_models/klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcf76ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰나이저 불러오기\n",
    "# AutoTokenizer 클래스를 통해 앞서 모델을 불러올때도 사용한 klue/roberta-base 저장소의 토큰나이저를 불러온다\n",
    "# config_json 파일에서 확인\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b16055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 1793, 2855, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 18, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토', '##큰', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '.', '[SEP]']\n",
      "[CLS] 토큰나이저는 텍스트를 토큰 단위로 나눈다. [SEP]\n",
      "토큰나이저는 텍스트를 토큰 단위로 나눈다.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 사용하기\n",
    "\n",
    "# input_ids는 토큰화했을때 각 토큰이 토크나이저 사전에 몇번째 항목인지 나타냄\n",
    "# input_ids의 첫번째 항목은 0이고 두번째 항목은 9157인데 각각 [CLS]와 토크에 대응되는 것을 확인 할 수 있다\n",
    "tokenized = tokenizer('토큰나이저는 텍스트를 토큰 단위로 나눈다.')\n",
    "print(tokenized)\n",
    "# {'input_ids': [0, 1793, 2855, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 18, 2], \n",
    "# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "\n",
    "# attention mask가 1이면 패딩이 아닌 실제 토큰임을 의미, token_type_ids가 0이면 일반적으로 첫번째 문장임을 의미\n",
    "# attention mask는 실제 텍스트 길이를 맞추기 위해 추가한 패딩인지 알려주는 mask\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "# ['[CLS]', '토', '##큰', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '.', '[SEP]']\n",
    "\n",
    "# 토큰 아이디를 다시 텍스트로 돌리고 싶다면 토크나이저의 decode 메서드를 사용\n",
    "print(tokenizer.decode(tokenized['input_ids']))\n",
    "# [CLS] 토큰나이저는 텍스트를 토큰 단위로 나눈다. [SEP]\n",
    "\n",
    "# 특수 문자 토큰을 제외하고 싶다면 skip_special_tokens=True 설정\n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e6a3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저에 여러 문장 넣기\n",
    "# 토크나이저에 한번에 여러 문장을 처리 할 수도 있다, '첫 번째 문장', '두 번째 문장'을 리스트로 함께 넣었다\n",
    "# 출력 결과를 확인하면 모두 각 문장을 토큰화해 2개 리스트를 반환\n",
    "tokenizer(['첫 번째 문장', '두 번째 문장'])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe8c3489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하나의 데이터에 여러 문장이 들어가는 경우\n",
    "# 한번에 2개의 문장을 모델에 넣어야 하는 경우 - 2개의 문장이 서로 원인과 결과 관계인지 학습시키기 싶다면, 두 문장을 한번에 모델에 입력해야됨\n",
    "# 이때 2개의 문장이 하나의 데이터라는 것을 표시하기 위해 리스트로 한번더 감싸준다\n",
    "tokenizer([ ['첫 번째 문장', '두 번째 문장'] ])\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2e93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]]\n",
      "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
      "[[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]]\n",
      "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 토큰 아이디를 문자열로 복원\n",
    "\n",
    "# 토크나이저의 batch_decode() 메서드를 사용하면 input_ids부분의 토큰 아이디를 문자열로 복월 할 수 있는데\n",
    "# 기본적으로 토큰화를 하면 ['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
    "# 2개의 문자을 한번에 토큰화하면 ['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n",
    "\n",
    "first_tokenized_result = tokenizer( ['첫 번째 문장', '두 번째 문장'] )['input_ids']\n",
    "print(first_tokenized_result)\n",
    "print(tokenizer.batch_decode(first_tokenized_result))\n",
    "\n",
    "second_tokenized_result = tokenizer( [ ['첫 번째 문장', '두 번째 문장'] ] )['input_ids']\n",
    "print(second_tokenized_result)\n",
    "print(tokenizer.batch_decode(second_tokenized_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7707a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저와 RoBERTa 토크나이저\n",
    "\n",
    "# BERT는 학습할때 2개의 문장이 서로 이어지는지 맞추는 NSP(next sentence prediction) 작업을 수행하기위해 문장을 구분하는 토큰 타입 아이디를 만들었다\n",
    "# 첫번째 문장의 토큰 아이디는 0, 두번째 문장의 토큰 아이디 1\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "print(bert_tokenizer( [ ['첫 번째 문장', '두 번째 문장'] ] ))\n",
    "# {'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "# RoBERTa 모델의 경우 NSP 작업을 학습 과정에서 제거\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "print(roberta_tokenizer( [ ['첫 번째 문장', '두 번째 문장'] ] ))\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "\n",
    "# 실제 영어 버전의 roberta-base 모델에는 항목 자체가 없음\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(en_roberta_tokenizer( [ ['first sentence', 'second sentence'] ] ))\n",
    "# {'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6764dd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] 첫 번째 문장은 짧다. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] 두 번째 문장은 첫 번째 문장보다 더 길다. [SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention_mask 확인\n",
    "# 해당 토큰이 패딩 토큰인지 실제 데이터인지에 대한 정보, \n",
    "# 패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해 추가하는 토큰, padding='longest' 입력한 문장중 가장 긴 문장에 맞춰 패딩 토큰을 추가\n",
    "\n",
    "tokenizer_result = tokenizer( ['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest' )['input_ids']\n",
    "# {'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], \n",
    "# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], \n",
    "# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
    "print(tokenizer( ['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest' ))\n",
    "\n",
    "tokenizer.batch_decode(tokenizer_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be2edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# KLUE MRC 데이터셋 다운로드 코드\n",
    "from datasets import load_dataset\n",
    "\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc') # 데이터셋 이름 klue, 서브셋 이름 mrc를 load_dataset 함수에 인자로 전달\n",
    "# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train') # split='train' 학습, split='validation' 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383e54a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['a'],\n",
      "    num_rows: 3\n",
      "})\n",
      "Dataset({\n",
      "    features: ['a'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 로컬의 데이터 활용 코드\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 로컬 csv 데이터 파일을 활용\n",
    "# dataset = load_dataset('csv', data_files='my_file.csv')\n",
    "\n",
    "# 파이썬 딕셔너리 활용\n",
    "from datasets import Dataset\n",
    "my_dict = {'a' : [1, 2, 3]}\n",
    "dataset = Dataset.from_dict(my_dict)\n",
    "print(dataset)\n",
    "\n",
    "# 판다스 데이터프레임 활용\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame( {'a' : [1, 2, 3]} )\n",
    "dataset = Dataset.from_pandas(df)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c9088ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 - 한국어 기사 제목을 바탕으로 기사의 카테고리를 분류하는 텍스트 분류 모델을 학습하는 실습\n",
    "# 데이터셋 준비 -> 모델, 토크나이저 로드 -> 학습(허깅페이스 트렌스포머 API) -> 모델 저장 -> 허깅페이스 허브에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3e1d34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
      "    num_rows: 45678\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "# 모델 학습에 사용할 연합 뉴스 데이터세 다운로드\n",
    "from datasets import load_dataset\n",
    "\n",
    "klue_tc_train = load_dataset('klue', 'ynat', split='train') # KLUE 의 YNAT 학습 및 검증 데이터셋 다운로드, train\n",
    "klue_tc_eval = load_dataset('klue', 'ynat', split='validation') # validation\n",
    "print(klue_tc_train)\n",
    "\n",
    "# train 데이터는 뉴스 제목(title), 뉴스가 속한 카테고리(label) 등의 컬럼으로 이뤄진 총 45,678 개 데이터\n",
    "# Dataset({\n",
    "#     features: ['guid', 'title', 'label', 'url', 'date'],\n",
    "#     num_rows: 45678\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23983b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 데이터 형태를 보기 위한 첫번째 데이터\n",
    "klue_tc_train[0]\n",
    "\n",
    "# 데이터의 고유 ID, {'guid': 'ynat-v1_train_00000',\n",
    "# 뉴스 제목,  'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
    "# 속한 카테고리 ID, 'label': 3,\n",
    "# 뉴스 링크, 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
    "# 뉴스 입력 시간, 'date': '2016.06.30. 오전 10:36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7489e920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 데이터 확인, feature 속성에서 label 컬럼 데이터 확인\n",
    "klue_tc_train.features['label'].names\n",
    "\n",
    "# ['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e335db66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 - 사용하지 않는 불필요한 컬럼 제거\n",
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_train\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['title', 'label'],\n",
    "#     num_rows: 45678\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bee76aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 전처리 - 카테고리를 문자로 표기한 label_str 컬럼 추가\n",
    "klue_tc_train.features['label']\n",
    "# ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)\n",
    "\n",
    "klue_tc_train.features['label'].int2str(1) # ID -> 카테고리 변환 메서드\n",
    "\n",
    "klue_tc_label = klue_tc_train.features['label']\n",
    "\n",
    "def make_str_label(batch):\n",
    "    # print('batch : ', batch)\n",
    "    batch['label_str'] = klue_tc_label.int2str(batch['label']) # ID -> 카테고리 변환 메서드\n",
    "    return batch\n",
    "\n",
    "# train map 데이터 추가\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "\n",
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9672cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 - 빠른 실습 진행을 위해 학습 데이터 10,000개만 추출해 사용\n",
    "\n",
    "# print(len(klue_tc_train), len(klue_tc_eval))\n",
    "\n",
    "# train 10,000개 추출\n",
    "train_dataset = klue_tc_train.train_test_split(test_size=200, shuffle=True, seed=42)['test']\n",
    "\n",
    "# test 1,000개 추출\n",
    "dataset = klue_tc_eval.train_test_split(test_size=50, shuffle=True, seed=42)\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# validation 1,000개 추출\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=50, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc8487c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e08c0be2ba545f2974defe3c4b01e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f549c30ae6c34ef4bdf7e742fe91de82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7f5fb73d6f4806a13bf16149fbdbb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 트레이너 API를 사용해 학습 - 허깅페이스는 학습에 필요한 다양한 기능(데이터로더 준비, 로깅, 평가, 저장 등) API 제공\n",
    "\n",
    "# Trainer를 사용한 학습 1)준비\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# title 컬럼에 토큰화 수행\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['title'], padding='max_length', truncation=True)\n",
    "\n",
    "# 모델 로드\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels = len(train_dataset.features['label'].names) # 분류 헤드의 분류 클래스 수를 지정\n",
    "    )\n",
    "\n",
    "# 토크나이저\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf2ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer를 사용한 학습 2)학습 인자와 평가 함수 정의\n",
    "# 에포크 수는 1, 배치 크기는 8, 결과는 results 폴더에 저장\n",
    "# 한 에포크 학습이 끝날때마다 검증 데이터셋에 대한 평가를 수행하도록 eval_strategy = 'epoch' 설정 \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy = 'epoch',\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "# 학습이 잘 이뤄지고 있는지 확인할때 사용할 평가지표를 정의\n",
    "# 모델의 예측결과인 eval_pred를 입력으로 받아 예측 결과중 가장 큰값을 갖는 클래스를 np.argmax 함수로 뽑아 prediction 변수에 저장\n",
    "# prediction 와 정답이 저장된 labels 가 같은 값을 갖는 결과의 비율을 정확도(accuracy)로 결과 딕셔너리에 저장 및 반환\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    prediction = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy' : (prediction == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a2b60ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rn/0m3hx_j94mg95hzztn5yj3940000gn/T/ipykernel_2820/608994561.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 11:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.107182</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.090075969696045,\n",
       " 'eval_accuracy': 0.14,\n",
       " 'eval_runtime': 26.864,\n",
       " 'eval_samples_per_second': 1.861,\n",
       " 'eval_steps_per_second': 0.261,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer를 사용한 학습 3)학습 진행\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, # pre-trained model(klue/roberta-base)\n",
    "    args=training_args, # 에포크 수는 1, 배치 크기는 8, 결과는 results 폴더에 저장, 한 에포크 학습이 끝날때마다 검증 데이터셋에 대한 평가를 수행하도록 eval_strategy = 'epoch' 설정\n",
    "    train_dataset=train_dataset, # train dataset\n",
    "    eval_dataset=valid_dataset, # validation dataset\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
