{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Language Model(SLLM) \n",
    "\n",
    "# Step 02 - 트랜스포머 허깅페이스 데이터셋(WikiText-2)을 적용하여 Small Language Model(SLLM) 구축 및 텍스트 생성\n",
    "# 데이터셋 -> 데이터로더 -> GPU 설정 -> SLLM 모델 정의 -> 학습 -> 텍스트 생성 -> 테스트\n",
    "\n",
    "# 핵심 문제 - 영어 기반이라 감성적인 한국어 여행 블로그를 생성하기엔 한계\n",
    "# - 단어 반복 (“jenkin”) : WikiText-2에 자주 등장하는 고유명사\n",
    "# - 문맥 불일치 : 프롬프트는 감성 여행, 데이터는 역사/스포츠\n",
    "# - 의미 연결 부족 : 모델이 감성적 흐름을 학습하지 못함\n",
    "\n",
    "# 기능 추가\n",
    "# - 샘플링 방식(top-k, top-p, temperature) 적용: 다양성 확보\n",
    "# - 프롬프트 튜닝: 감성적 블로그 스타일 유도\n",
    "# - 후처리(clean_text): 출력 품질 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6575fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version : 2.7.1+cu118, Device : cuda\n"
     ]
    }
   ],
   "source": [
    "# import 및 GPU 설정\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset # HuggingFace Datasets\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'Pytorch Version : {torch.__version__}, Device : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e01b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([34149, 17976, 24038,  5065, 34149, 12461, 12020, 17976, 47135, 61420,\n",
      "        35282, 24038,  5540, 12403, 61420, 53022,  9905,  3716, 15240, 17976,\n",
      "        25903, 65432, 18565, 47135,   294,  9905,  7136, 43561, 30097,  2925,\n",
      "        17976, 24038]), tensor([17976, 24038,  5065, 34149, 12461, 12020, 17976, 47135, 61420, 35282,\n",
      "        24038,  5540, 12403, 61420, 53022,  9905,  3716, 15240, 17976, 25903,\n",
      "        65432, 18565, 47135,   294,  9905,  7136, 43561, 30097,  2925, 17976,\n",
      "        24038,  5065]))]\n"
     ]
    }
   ],
   "source": [
    "# WikiText-2 데이터셋 로딩 및 전처리\n",
    "# - Step 02 에서는 WikiText-2 데이터셋를 사용해 토큰화하고, 시퀀시를 만들어 학습용으로 변환한다.\n",
    "\n",
    "# Hugging Face에서 WikiText-2 로드 및 다운로드\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "# print('dataset : ', dataset)\n",
    "# dataset :  Dataset({\n",
    "#     features: ['text'],\n",
    "#     num_rows: 36718\n",
    "# })\n",
    "\n",
    "# 간단한 토크나이저 사용 - 단어 단위\n",
    "from collections import Counter\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# 전체 텍스트 토큰화\n",
    "tokens = []\n",
    "for item in dataset:\n",
    "    tokens.extend(tokenizer(item['text']))\n",
    "\n",
    "# 단어 사전 생성\n",
    "vocab = list(set(tokens)) # 분리된 단어 -> 리스트\n",
    "# print(vacab)\n",
    "word2idx = { word:idx for idx, word in enumerate(vocab) } # 단어 리스트 -> 딕셔너리(value:key), index 값 출력\n",
    "# print(word2idx)\n",
    "idx2word = { idx:word for word, idx in word2idx.items() } # 딕셔너리(key:value), word value 값 출력\n",
    "# print(idx2word)\n",
    "\n",
    "# 시퀀스 생성(입력: [32 단어], 출력: [32 단어] 등)\n",
    "sequence_length = 32\n",
    "data = []\n",
    "for i in range(len(tokens) - sequence_length):\n",
    "    input_seq = tokens[i:i+sequence_length]\n",
    "    # print('input_seq : ', input_seq)\n",
    "    target_seq = tokens[i+1:i+sequence_length+1]\n",
    "    # print('target_seq : ', target_seq)\n",
    "    data.append((\n",
    "        torch.tensor([ word2idx[word] for word in input_seq ]),\n",
    "        torch.tensor([ word2idx[word] for word in target_seq ]) \n",
    "    ))\n",
    "# print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "545d649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 및 데이터로더 생성\n",
    "class WikiTextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# dataloader, 10000 -> 30000\n",
    "dataloader = DataLoader(WikiTextDataset(data[:40000]), batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "365cbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 기반 SLLM 모델 정의\n",
    "class TransformerSLLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)\n",
    "        x = self.token_embedding(x) + self.pos_embedding(positions)\n",
    "        x = self.transformer(x)\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1869d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AI\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 20760.9848\n",
      "Epoch 2, Loss: 16605.1588\n",
      "Epoch 3, Loss: 16237.2135\n",
      "Epoch 4, Loss: 16043.6521\n",
      "Epoch 5, Loss: 15893.6894\n",
      "Epoch 6, Loss: 15786.4156\n",
      "Epoch 7, Loss: 15696.4497\n",
      "Epoch 8, Loss: 15621.5932\n",
      "Epoch 9, Loss: 15557.1036\n",
      "Epoch 10, Loss: 15501.5902\n",
      "Epoch 11, Loss: 15452.1242\n",
      "Epoch 12, Loss: 15419.3927\n",
      "Epoch 13, Loss: 15387.9014\n",
      "Epoch 14, Loss: 15367.2654\n",
      "Epoch 15, Loss: 15350.9408\n",
      "Epoch 16, Loss: 15339.1018\n",
      "Epoch 17, Loss: 15325.2627\n",
      "Epoch 18, Loss: 15313.0765\n"
     ]
    }
   ],
   "source": [
    "# 학습\n",
    "model = TransformerSLLM(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    max_len=sequence_length\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(20):\n",
    "    total_loss = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs) # 모델 예측\n",
    "        loss = criterion(outputs.view(-1, len(vocab)), targets.view(-1)) # 손실함수 값\n",
    "\n",
    "        # 오차역전파\n",
    "        optimizer.zero_grad() # 미분 파리미터 초기화\n",
    "        loss.backward() # 미분 연산\n",
    "        optimizer.step() # 미분 연산 후 가중치,바이어스 파라미터 업데이트\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ae32a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 함수 추가\n",
    "# - 이 함수는 start_text를 기반으로 최대 50개의 단어를 이어서 생성합니다. 단어 사전에 없는 경우는 0으로 처리되며, \n",
    "# 학습된 문맥을 따라 다음 단어를 예측합니다.\n",
    "# 샘플링 방식 추가 (top-k, top-p, temperature) - temperature=0.8 정도로 설정하면 더 다양하고 자연스러운 문장이 나올수 있음\n",
    "# def generate_text(model, start_text, word2idx, idx2word, max_length=50, temperature=0.7):\n",
    "def generate_text(model, start_text, word2idx, idx2word, max_length=50, temperature=0.7, top_k=50, top_p=0.9):\n",
    "    model.eval()\n",
    "    tokens = start_text.lower().split()\n",
    "    input_ids = [ word2idx.get(word, 0) for word in tokens ] # unknown word -> 0\n",
    "\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            next_token_logits = output[:, -1, :] / temperature # 마지막 위치의 출력\n",
    "            logits = top_k_filtering(next_token_logits, top_k)\n",
    "            logits = top_p_filtering(logits, top_p)\n",
    "            probabilities = torch.softmax(logits, dim=-1)\n",
    "            next_token_id = torch.multinomial(probabilities, num_samples=1)\n",
    "            # next_token_id = torch.argmax(next_token_logits, dim=-1).item() # argmax 방식으로 가장 확률 높은 단어만 선택\n",
    "            input_ids.append(next_token_id)\n",
    "\n",
    "            input_tensor = torch.tensor(input_ids[-sequence_length:], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # 텐서를 정수로 변환 후 단어로 매핑\n",
    "    generated_words = [idx2word.get(int(idx), \"[UNK]\") for idx in input_ids]\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# 현재는 torch.multinomial()로 확률 기반 샘플링을 하고 있지만, 더 정교한 제어를 위해 top-k 또는 top-p 필터링을 추가\n",
    "def top_k_filtering(logits, top_k=50):\n",
    "    values, _ = torch.topk(logits, top_k)\n",
    "    min_value = values[:, -1].unsqueeze(1)\n",
    "    return torch.where(logits < min_value, torch.full_like(logits, float('-inf')), logits)\n",
    "\n",
    "# top-p 샘플링 추가 (nucleus sampling)\n",
    "def top_p_filtering(logits, top_p=0.9):\n",
    "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "    logits[0, indices_to_remove] = float('-inf')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f014981",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text.strip()\n\u001b[32m     18\u001b[39m start_text = \u001b[33m\"\u001b[39m\u001b[33mWrite a sentimental blog post about spring travel in Seoul. Mention cherry blossoms, street food, and quiet alleys.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m generated = generate_text(\u001b[43mmodel\u001b[49m, start_text, word2idx, idx2word, max_length=\u001b[32m50\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# print('Generated Text:\\n', generated)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated Text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, clean_text(generated))\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 테스트 실행 예시\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" '\", \"'\")\n",
    "#     text = text.replace('\" \"', '\"').replace(\"  \", \" \")\n",
    "#     text = text.replace(\"= = = = = = =\", \"\")  # 반복 기호 제거\n",
    "#     return text.strip()\n",
    "\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"(= ){3,}\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r'\"\\s*,', ',', text)\n",
    "    text = re.sub(r'\\s*\"\\s*', '', text)\n",
    "    text = re.sub(r'\\s*\\.\\s*', '. ', text)\n",
    "    return text.strip()\n",
    "\n",
    "start_text = \"Write a sentimental blog post about spring travel in Seoul. Mention cherry blossoms, street food, and quiet alleys.\"\n",
    "\n",
    "generated = generate_text(model, start_text, word2idx, idx2word, max_length=50)\n",
    "# print('Generated Text:\\n', generated)\n",
    "print(\"Generated Text:\\n\", clean_text(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
