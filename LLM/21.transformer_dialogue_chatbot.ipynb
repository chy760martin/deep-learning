{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e468aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer 대화형 챗봇(KoDialogBench Chatbot) 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터셋 로드 및 데이터 분리\n",
    "# - AI Hub 대화데이터: 한국어 SNS 멀티턴 대화 데이터\n",
    "# - 데이터 분리: train, validation\n",
    "# - 2. 토크나이저, 데이터셋, 전처리 적용\n",
    "# - 전처리 함수: 질문 + 문맥 토큰화 + 정답 스팬(offsets 위치 정보: offset_mapping 구조 생성)매핑\n",
    "# - 데이터셋 적용, batched=True\n",
    "# - 3. collate_fn 정의 및 DataLoader 생성\n",
    "# - collate_fn: 데이터로더 batch 데이터->텐서->스택 쌓아 리턴\n",
    "# - DataLoader 생성\n",
    "# - 4. 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "# - 최적화 설정: optimizer, GradScaler, autocast\n",
    "# - Early Stopping 클래스 정의\n",
    "# - 최적 모델 가중치 저장\n",
    "# - 5. 학습/검증 루프\n",
    "# - 딕셔너리 형태 학습데이터를 그대로 모델에 전달하는 코드로 정리, 코드가 깔끔하고 범용적으로 사용한다\n",
    "# - Early Stopping 객체 사용하여 적용\n",
    "# - 6. 전체 평가 파이프라인\n",
    "# - F1/EM 평가\n",
    "# - 7. 추론 단일 테스트\n",
    "# - 8. 추론 다중 테스트\n",
    "# - 9. 문장 추론: Fast API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4539cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf1aa4e1629476097e251a2a7d797e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b562bdcdd8e74f0a9b2840da7ca92340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676d4fc3bdea4b54b8ce1642f2602986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/661 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5e9f3e3c3e46cbaf55db3d4589cffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/82 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c30e1055628f4f10afa42133ef581110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4d406f63df41848a4aced4cb2f06fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터셋 로드 \n",
    "# - AI Hub 대화데이터: 한국어 SNS 멀티턴 대화 데이터\n",
    "# - 데이터 분리: train, validation\n",
    "from datasets import load_dataset\n",
    "import glob, random\n",
    "\n",
    "train_files = glob.glob('llm_data/ai_hub_dialogue_session2/train/*.json')\n",
    "valid_files = glob.glob('llm_data/ai_hub_dialogue_session2/validation/*.json')\n",
    "\n",
    "# 10%만 샘플링\n",
    "sample_train = random.sample(train_files, int(len(train_files) * 0.02))\n",
    "sample_valid = random.sample(valid_files, int(len(valid_files) * 0.02))\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files={\n",
    "        'train': sample_train,\n",
    "        'validation': sample_valid\n",
    "    },\n",
    "    field='sessionInfo'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4d975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ed61570e604ffa9f68e5e7264378bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b10ef66de2774f09be115bf8b6cab439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '안녕. 나 처음 이용해보는데 질문해도 될까?', 'response': '만나서 반가워요. 무엇을 도와드릴까요?'}\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리 및 대화 쌍 만들기\n",
    "# - train/validation/test 분리\n",
    "# - utterance 쌍(이전대화, 현재대화) 추출\n",
    "\n",
    "# 대화 파싱 함수: map(batched=True)로 여러 쌍을 한번에 반환 할 수 있다\n",
    "def extract_pairs(batch):\n",
    "    contexts = []\n",
    "    responses = []\n",
    "    for example in batch['dialog']:\n",
    "        for i in range(1, len(example)):\n",
    "            contexts.append(example[i-1]['utterance']) # 이전 대화\n",
    "            responses.append(example[i]['utterance']) # 현재 대화\n",
    "    return {\n",
    "        'context': contexts,\n",
    "        'response': responses\n",
    "    }\n",
    "\n",
    "# utterance 쌍 추출, map 사용: 리스트 컬럼 생성\n",
    "train_pairs = dataset['train'].map(\n",
    "    extract_pairs, \n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    batched=True\n",
    ")\n",
    "valid_pairs = dataset['validation'].map(\n",
    "    extract_pairs, \n",
    "    remove_columns=dataset['validation'].column_names,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# flatten으로 리스트 풀기\n",
    "train_pairs = train_pairs.flatten()\n",
    "valid_pairs = valid_pairs.flatten()\n",
    "\n",
    "# validation -> validation/test 분리\n",
    "split_dataset = valid_pairs.train_test_split(test_size=0.5, seed=42)\n",
    "valid_pairs = split_dataset['train']\n",
    "test_pairs = split_dataset['test']\n",
    "\n",
    "print(train_pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42335820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "torch.Size([16, 128])\n",
      "학습 데이터 개수: 19394\n",
      "검증 데이터 개수: 1207\n",
      "배치 크기: 16\n",
      "학습 로더 길이: 1213\n",
      "검증 로더 길이: 76\n"
     ]
    }
   ],
   "source": [
    "# Dataset, DataLoader 생성\n",
    "# - Hugging Face Dataset -> PyTorch Dataset 클래스로 감싸기\n",
    "# - DataLoader 생성\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 준비: MT5 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=128): # 파라미터 초기 셋팅\n",
    "        self.dataset=hf_dataset\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "    \n",
    "    def __len__(self): # dataset 길이 리턴\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        context = item['context']\n",
    "        response = item['response']\n",
    "\n",
    "        # 토크나이즈\n",
    "        inputs = self.tokenizer(\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            response,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0), # 불필요한 squeeze(0) 앞 배치 차원만 제거\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': labels['input_ids'].squeeze(0)\n",
    "        }\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = DialogueDataset(train_pairs, tokenizer)\n",
    "valid_dataset = DialogueDataset(valid_pairs, tokenizer)\n",
    "test_dataset = DialogueDataset(test_pairs, tokenizer)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=16\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# 데이터 확인\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape) # torch.Size([32, 128])\n",
    "print(batch['attention_mask'].shape) # torch.Size([32, 128])\n",
    "print(batch['labels'].shape) # torch.Size([32, 128])\n",
    "\n",
    "print(\"학습 데이터 개수:\", len(train_dataset))\n",
    "print(\"검증 데이터 개수:\", len(valid_dataset))\n",
    "print(\"배치 크기:\", train_loader.batch_size)\n",
    "print(\"학습 로더 길이:\", len(train_loader))\n",
    "print(\"검증 로더 길이:\", len(valid_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7b67e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129, Device: cuda\n",
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MT5ForConditionalGeneration(\n",
      "      (shared): Embedding(250112, 512)\n",
      "      (encoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "# - 모델 본체 동결 처리: Feature Extraction\n",
    "# - LoRA 파인튜닝 적용\n",
    "# - Early Stopping 적용\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small') # MT5 모델 불러온다\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "# 모델 전체를 GPU/CPU 디바이스 메모리로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 모델 본체 동결 처리: Feature Extraction\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# # LoRA 대상 모듈 자동 추출\n",
    "# target_modules = []\n",
    "# for name, module in model.named_modules():\n",
    "#     if any(x in name for x in [\"SelfAttention.q\", \"SelfAttention.k\", \"SelfAttention.v\", \"SelfAttention.o\"]):\n",
    "#         target_modules.append(name)\n",
    "\n",
    "# print(\"LoRA target modules:\", target_modules)\n",
    "\n",
    "# LoRA 적용: LoRA 모듈만 학습되도록 설정(경량 파인튜닝)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # 작은 rank(r=8)로 효율적인 파인튜닝 가능\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    # target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM' # 대화 응답 생성은 Seq2Seq LM\n",
    ")\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# Automatic Mixed Precision(AMP) 학습을 위한 GradScaler 준비\n",
    "# Automatic Mixed Precision(AMP)은 모델 파라미터는 FP32로 유지하면서 연산(곱셈·덧셈 등)만 FP16으로 자동 전환하여, \n",
    "# 정밀도 손실 없이 메모리와 연산 효율을 극대화하는 기술\n",
    "scaler = GradScaler()\n",
    "num_epochs = 3\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/21_transformer_dialogue_chatbot/best_model.pt'):\n",
    "        self.patience=patience\n",
    "        self.min_delta=min_delta\n",
    "        self.best_loss=None\n",
    "        self.counter=0\n",
    "        self.early_stop=False\n",
    "        self.path=path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        # 최조 손실값에 해당하는 모델 가중치 저장\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss=valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        \n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss=valid_loss\n",
    "            self.counter=0\n",
    "            self.save_checkpoint(model)\n",
    "        \n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop=True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !='' and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        # 모델 가중치 저장\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f' Best model saved at {self.path}')\n",
    "\n",
    "# Early Stopping 객체 생성\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# 모델 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a68a5035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   6%|▌         | 69/1213 [06:49<1:53:02,  5.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     outputs = model(**batch) \u001b[38;5;66;03m# 딕셔너리 형태 학습데이터를 그대로 모델에 전달, 코드가 깔끔하고 범용적으로 사용한다\u001b[39;00m\n\u001b[32m     16\u001b[39m     loss = outputs.loss \u001b[38;5;66;03m# 손실값\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 미분 연산\u001b[39;00m\n\u001b[32m     18\u001b[39m scaler.step(optimizer) \u001b[38;5;66;03m# 미분 연산 후 가중치/바이어스 파라미터 업데이트\u001b[39;00m\n\u001b[32m     19\u001b[39m scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Train Loop\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]'):\n",
    "        batch = { k: v.to(device) for k, v in batch.items() } # 딕셔너리 형태로 생성, 학습데이터 GPU 지정\n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            outputs = model(**batch) # 딕셔너리 형태 학습데이터를 그대로 모델에 전달, 코드가 깔끔하고 범용적으로 사용한다\n",
    "            loss = outputs.loss # 손실값\n",
    "        scaler.scale(loss).backward() # 미분 연산\n",
    "        scaler.step(optimizer) # 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item() # 손실 누적\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval() # 검증/추론 모드 지정\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'Epoch {epoch+1} [Valid]'):\n",
    "            batch = { k: v.to(device) for k, v in batch.items() }\n",
    "\n",
    "            # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "            with autocast(device_type='cuda', dtype=torch.float16):\n",
    "                outputs = model(**batch) # 검증 모델 예측\n",
    "                loss = outputs.loss # 검증 손실값\n",
    "            total_val_loss += loss.item() # 검증 손실값 누적\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    print(f'Epoch {epoch+1}, Valid Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(valid_loss=avg_val_loss, model=model)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping triggered.')\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
