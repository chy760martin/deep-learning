{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer 질의응답(QA) 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22922b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변 시작 인덱스: 18\n",
      "답변 끝 인덱스: 22\n",
      "전체 토큰 시퀀스: ['[CLS]', '서울', '##은', '어디', '##에', '있', '##나', '##요', '?', '[SEP]', '서울', '##은', '대한민국', '##의', '수도', '##이', '##며', ',', '한반도', '##의', '중서', '##부', '##에', '위치', '##해', '있', '##습', '##니다', '.', '[SEP]']\n",
      "답변 토큰: ['한반도', '##의', '중서', '##부']\n",
      "최종 답변: 한반도의 중서부\n"
     ]
    }
   ],
   "source": [
    "# QA Pre-trained 모델 테스트\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "base_model = 'monologg/koelectra-base-v3-finetuned-korquad' # 한국어 KorQuAD 데이터셋으로 파인 튜닝된 KoELECTRA QA 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model) # 토크나이저 로드\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(base_model) # QA 태스크용 헤드가 포함된 모델 로드\n",
    "\n",
    "question = '서울은 어디에 있나요?'\n",
    "context = '서울은 대한민국의 수도이며, 한반도의 중서부에 위치해 있습니다.'\n",
    "\n",
    "# QA 모델 구조: BERT 기반 QS 모델은 입력을 [CLS]질문[SEP]문맥[SEP] 형태로 받는다\n",
    "# 질문과 문맥을 하나의 입력으로 합쳐서 토큰화 결과를 모델에 전달해야 답변을 얻을 수 있는 구조\n",
    "# [CLS] 서울 은 어디 에 있 나요 ? [SEP] 서울 은 대한민국 의 수도 이며 , 한반도 의 중서부 에 위치 해 있습니다 . [SEP]\n",
    "inputs = tokenizer(question, context, return_tensors='pt') # (batch_size,seq_len) (1,seq_len)\n",
    "\n",
    "# 모델에 입력을 전달하여 start_logits, end_logits 출력\n",
    "# - start_logits: 답변 시작 위치에 대한 확률 분포\n",
    "# - end_logits: 답변 끝 위치에 대한 확률 분포\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# torch.argmax() 가장 확률이 높은 인덱스를 선택\n",
    "# - answer_start: 답변 시작 토큰 위치, answer_end: 답변 끝 토큰 위치\n",
    "answer_start = torch.argmax(outputs.start_logits)\n",
    "answer_end = torch.argmax(outputs.end_logits) + 1 # 마지막 인덱스 포함\n",
    "\n",
    "# 토큰을 문자열로 변환\n",
    "# - tokenizer.convert_ids_to_tokens(): 토큰 ID->토큰 문자열\n",
    "# - tokenizer.convert_tokens_to_string(): 토큰 문자열->사람이 읽을 수 있는 문장\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    ")\n",
    "\n",
    "# Debug 확인\n",
    "print('답변 시작 인덱스:', answer_start.item())\n",
    "print('답변 끝 인덱스:', answer_end.item())\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]) # 전체 토큰 확인\n",
    "print('전체 토큰 시퀀스:', tokens)\n",
    "\n",
    "answer_tokens = tokens[answer_start:answer_end] # 답변 토큰 범위 확인\n",
    "print('답변 토큰:', answer_tokens)\n",
    "\n",
    "answer = tokenizer.convert_tokens_to_string(answer_tokens) # 최종 답변\n",
    "print('최종 답변:', answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9836527d42a4804a97c46a607817c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AI\\.cache\\huggingface\\hub\\datasets--squad_v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e459c0b832b44ed3b1e97095cd83db05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "squad_v2/train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fc7e7a92804ae09140b6501776a009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "squad_v2/validation-00000-of-00001.parqu(…):   0%|          | 0.00/1.35M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc5c3b7e00b4f83819a10a5d4c044b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a70f5fca31a4bfc84a5bdc251a89373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 130319\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 11873\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드: Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 한국어 KorQuAD 데이터셋: train(60,407), validation(5,774)\n",
    "korquad = load_dataset('squad_kor_v1')\n",
    "print(korquad)\n",
    "\n",
    "# 영어 SQuAD 2.0: train(130,319), validation(11,873)\n",
    "squad = load_dataset('squad_v2')\n",
    "print(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a55c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
