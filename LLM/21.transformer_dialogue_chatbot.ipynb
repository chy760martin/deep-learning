{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 모델 구축 - Transformer 대화형 챗봇(Dialogue Chatbot) 모델\n",
    "# 학습 목표 - 실무에서 사용되는 파이프라인 이해 및 적용\n",
    "# - 1. 데이터셋 로드 및 데이터 분리\n",
    "# - AI Hub 대화데이터: 한국어 SNS 멀티턴 대화 데이터\n",
    "# - 데이터 분리: train, validation\n",
    "# - 2. 데이터 분리 및 대화 쌍 만들기\n",
    "# - train/validation/test 분리\n",
    "# - utterance 쌍(이전대화, 현재대화) 추출\n",
    "# - 3. Dataset, DataLoader 생성\n",
    "# - Hugging Face Dataset -> PyTorch Dataset 클래스로 감싸기\n",
    "# - DataLoader 생성\n",
    "# - 4. 모델 정의\n",
    "# - Feature Extraction + LoRA Fine-tuning 조합\n",
    "# - 최적화 설정: optimizer, GradScaler, autocast\n",
    "# - Early Stopping 클래스 정의\n",
    "# - 최적 모델 가중치 저장\n",
    "# - 5. 학습/검증 루프\n",
    "# - 딕셔너리 형태 학습데이터를 그대로 모델에 전달하는 코드로 정리, 코드가 깔끔하고 범용적으로 사용한다\n",
    "# - Early Stopping 객체 사용하여 적용\n",
    "# - AMP torch.float32 사용(메모리 사용 증가, torch.float16 사용시 loss가 너무 작아저 nan 발생)\n",
    "# - 6. 전체 평가 파이프라인\n",
    "# - F1/EM 평가\n",
    "# - 7. 모델 정의 및 최적화 모델 로드\n",
    "# - 8. 멀티 답변 생성\n",
    "# - 9. 문장 추론: Fast API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4539cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31d12df927140beba5c9d758ab5e3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1985 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d139bba06ff842db9a6659c74e7fa9ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f900187b2e9b4b41b94da174dd509f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/1985 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ef37015ce04335970e45641b18614d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/124 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535a0cae5fc541f68e30cc78f9d26a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b10f6a4bbd4af48933b0098c369137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3970, 10)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드 \n",
    "# - AI Hub 대화데이터: 한국어 SNS 멀티턴 대화 데이터\n",
    "# - 데이터 분리: train, validation\n",
    "from datasets import load_dataset\n",
    "import glob, random\n",
    "\n",
    "train_files = glob.glob('llm_data/ai_hub_dialogue_session2/train/*.json')\n",
    "valid_files = glob.glob('llm_data/ai_hub_dialogue_session2/validation/*.json')\n",
    "\n",
    "# 6%만 샘플링\n",
    "sample_train = random.sample(train_files, int(len(train_files) * 0.06))\n",
    "sample_valid = random.sample(valid_files, int(len(valid_files) * 0.03))\n",
    "\n",
    "dataset = load_dataset(\n",
    "    'json',\n",
    "    data_files={\n",
    "        'train': sample_train,\n",
    "        'validation': sample_valid\n",
    "    },\n",
    "    field='sessionInfo'\n",
    ")\n",
    "print(dataset['train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd4d975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f944b336f951485fbc5f4086fe789fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a36bbad7094d74b6d64718a26b2dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': '안녕하세요! 요즘 부모님과 마찰이 생기는 일이 많아서 힘드네요.', 'response': '그럴 때는 상황을 잘 다루는 것이 중요한 것 같아요. 어떤 상황에서 마찰이 생기는지 알려주세요.'}\n",
      "(58173, 2)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리 및 대화 쌍 만들기\n",
    "# - train/validation/test 분리\n",
    "# - utterance 쌍(이전대화, 현재대화) 추출\n",
    "\n",
    "# 대화 파싱 함수: map(batched=True)로 여러 쌍을 한번에 반환 할 수 있다\n",
    "def extract_pairs(batch):\n",
    "    contexts = []\n",
    "    responses = []\n",
    "    for example in batch['dialog']:\n",
    "        for i in range(1, len(example)):\n",
    "            contexts.append(example[i-1]['utterance']) # 이전 대화\n",
    "            responses.append(example[i]['utterance']) # 현재 대화\n",
    "    return {\n",
    "        'context': contexts,\n",
    "        'response': responses\n",
    "    }\n",
    "\n",
    "# utterance 쌍 추출, map 사용: 리스트 컬럼 생성\n",
    "train_pairs = dataset['train'].map(\n",
    "    extract_pairs, \n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    batched=True\n",
    ")\n",
    "valid_pairs = dataset['validation'].map(\n",
    "    extract_pairs, \n",
    "    remove_columns=dataset['validation'].column_names,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# flatten으로 리스트 풀기\n",
    "train_pairs = train_pairs.flatten()\n",
    "valid_pairs = valid_pairs.flatten()\n",
    "\n",
    "# validation -> validation/test 분리\n",
    "split_dataset = valid_pairs.train_test_split(test_size=0.5, seed=42)\n",
    "valid_pairs = split_dataset['train']\n",
    "test_pairs = split_dataset['test']\n",
    "\n",
    "print(train_pairs[0])\n",
    "print(train_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42335820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "d:\\AI\\Pytorch\\deep-learning\\.venv\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64])\n",
      "torch.Size([16, 64])\n",
      "torch.Size([16, 64])\n",
      "학습 데이터 개수: 58173\n",
      "검증 데이터 개수: 1826\n",
      "배치 크기: 16\n",
      "학습 로더 길이: 3636\n",
      "검증 로더 길이: 115\n"
     ]
    }
   ],
   "source": [
    "# Dataset, DataLoader 생성\n",
    "# - Hugging Face Dataset -> PyTorch Dataset 클래스로 감싸기\n",
    "# - DataLoader 생성\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 토크나이저 준비: MT5 모델\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=64): # 파라미터 초기 셋팅\n",
    "        self.dataset=hf_dataset\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "    \n",
    "    def __len__(self): # dataset 길이 리턴\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        context = item['context']\n",
    "        response = item['response']\n",
    "\n",
    "        # 토크나이즈\n",
    "        inputs = self.tokenizer(\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        labels = self.tokenizer(\n",
    "            response,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        # pad_token_id → -100 (loss 계산에서 무시)\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        # <extra_id_0> 제거 (선택)\n",
    "        extra_id_0 = self.tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "        if labels[0] == extra_id_0:\n",
    "            labels = labels[1:]\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0), # 불필요한 squeeze(0) 앞 배치 차원만 제거\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Dataset 생성\n",
    "train_dataset = DialogueDataset(train_pairs, tokenizer)\n",
    "valid_dataset = DialogueDataset(valid_pairs, tokenizer)\n",
    "test_dataset = DialogueDataset(test_pairs, tokenizer)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=16\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# 데이터 확인\n",
    "batch = next(iter(train_loader))\n",
    "print(batch['input_ids'].shape) # torch.Size([32, 128])\n",
    "print(batch['attention_mask'].shape) # torch.Size([32, 128])\n",
    "print(batch['labels'].shape) # torch.Size([32, 128])\n",
    "\n",
    "print(\"학습 데이터 개수:\", len(train_dataset))\n",
    "print(\"검증 데이터 개수:\", len(valid_dataset))\n",
    "print(\"배치 크기:\", train_loader.batch_size)\n",
    "print(\"학습 로더 길이:\", len(train_loader))\n",
    "print(\"검증 로더 길이:\", len(valid_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b7b67e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129, Device: cuda\n",
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MT5ForConditionalGeneration(\n",
      "      (shared): Embedding(250112, 512)\n",
      "      (encoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "# - 모델 본체 동결 처리: Feature Extraction\n",
    "# - LoRA 파인튜닝 적용\n",
    "# - Early Stopping 적용\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.amp import GradScaler, autocast\n",
    "import os\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small') # MT5 모델 불러온다\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "# 모델 전체를 GPU/CPU 디바이스 메모리로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 모델 본체 동결 처리: Feature Extraction\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "# # LoRA 대상 모듈 자동 추출\n",
    "# target_modules = []\n",
    "# for name, module in model.named_modules():\n",
    "#     if any(x in name for x in [\"SelfAttention.q\", \"SelfAttention.k\", \"SelfAttention.v\", \"SelfAttention.o\"]):\n",
    "#         target_modules.append(name)\n",
    "\n",
    "# print(\"LoRA target modules:\", target_modules)\n",
    "\n",
    "# LoRA 적용: LoRA 모듈만 학습되도록 설정(경량 파인튜닝)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # 작은 rank(r=8)로 효율적인 파인튜닝 가능\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    # target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM' # 대화 응답 생성은 Seq2Seq LM\n",
    ")\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# Automatic Mixed Precision(AMP) 학습을 위한 GradScaler 준비\n",
    "# Automatic Mixed Precision(AMP)은 모델 파라미터는 FP32로 유지하면서 연산(곱셈·덧셈 등)만 FP16으로 자동 전환하여, \n",
    "# 정밀도 손실 없이 메모리와 연산 효율을 극대화하는 기술\n",
    "scaler = GradScaler()\n",
    "num_epochs = 3\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.0, path='./llm_models/21_transformer_dialogue_chatbot/best_model.pt'):\n",
    "        self.patience=patience\n",
    "        self.min_delta=min_delta\n",
    "        self.best_loss=None\n",
    "        self.counter=0\n",
    "        self.early_stop=False\n",
    "        self.path=path\n",
    "    \n",
    "    def __call__(self, valid_loss, model):\n",
    "        # 최조 손실값에 해당하는 모델 가중치 저장\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss=valid_loss\n",
    "            self.save_checkpoint(model)\n",
    "        \n",
    "        # 성능 개선 -> 최적 모델 갱신\n",
    "        elif valid_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss=valid_loss\n",
    "            self.counter=0\n",
    "            self.save_checkpoint(model)\n",
    "        \n",
    "        # 개선 없음\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop=True\n",
    "    \n",
    "    def save_checkpoint(self, model):\n",
    "        # 디렉토리만 생성\n",
    "        folder = os.path.dirname(self.path)\n",
    "        if folder !='' and not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        # 모델 가중치 저장\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        print(f' Best model saved at {self.path}')\n",
    "\n",
    "# Early Stopping 객체 생성\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "# 모델 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68a5035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 3636/3636 [2:20:32<00:00,  2.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 7.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Valid]: 100%|██████████| 115/115 [00:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Valid Loss: 4.1618\n",
      " Best model saved at ./llm_models/21_transformer_dialogue_chatbot/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 3636/3636 [2:20:31<00:00,  2.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 5.2923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Valid]: 100%|██████████| 115/115 [00:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Valid Loss: 3.8294\n",
      " Best model saved at ./llm_models/21_transformer_dialogue_chatbot/best_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 3636/3636 [2:20:33<00:00,  2.32s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 4.7710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Valid]: 100%|██████████| 115/115 [00:57<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Valid Loss: 3.4802\n",
      " Best model saved at ./llm_models/21_transformer_dialogue_chatbot/best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# - 학습/검증 루프\n",
    "# - 딕셔너리 형태 학습데이터를 그대로 모델에 전달하는 코드로 정리, 코드가 깔끔하고 범용적으로 사용한다\n",
    "# - Early Stopping 객체 사용하여 적용\n",
    "# - AMP torch.float32 사용(메모리 사용 증가, torch.float16 사용시 loss가 너무 작아저 nan 발생)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    model.train() # 학습 모드 지정\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Train Loop\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]'):\n",
    "        batch = { k: v.to(device) for k, v in batch.items() } # 딕셔너리 형태로 생성, 학습데이터 GPU 지정\n",
    "        optimizer.zero_grad() # 오차역전파 코드, 미분 전 가중치/바이어스 파라미터 초기화\n",
    "\n",
    "        # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "        # with autocast(device_type='cuda', dtype=torch.float16):\n",
    "        #     outputs = model(**batch) # 딕셔너리 형태 학습데이터를 그대로 모델에 전달, 코드가 깔끔하고 범용적으로 사용한다\n",
    "        #     loss = outputs.loss # 손실값\n",
    "        # scaler.scale(loss).backward() # 미분 연산\n",
    "        # scaler.step(optimizer) # 미분 연산 후 가중치/바이어스 파라미터 업데이트\n",
    "        # scaler.update()\n",
    "\n",
    "        with autocast(device_type='cuda', enabled=False):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() # 손실 누적\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval() # 검증/추론 모드 지정\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader, desc=f'Epoch {epoch+1} [Valid]'):\n",
    "            batch = { k: v.to(device) for k, v in batch.items() }\n",
    "\n",
    "            # AMP(Automatic Mixed Precision) GPU에서 연산 속도와 메모리 효율 향상\n",
    "            # with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            #     outputs = model(**batch) # 검증 모델 예측\n",
    "            #     loss = outputs.loss # 검증 손실값\n",
    "\n",
    "            with autocast(device_type='cuda', enabled=False):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item() # 검증 손실값 누적    \n",
    "    avg_val_loss = total_val_loss / len(valid_loader)\n",
    "    print(f'Epoch {epoch+1}, Valid Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    early_stopping(valid_loss=avg_val_loss, model=model)\n",
    "    if early_stopping.early_stop:\n",
    "        print('Early stopping triggered.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 평가 파이프라인: F1/EM 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fad4dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.8.0+cu129, Device: cuda\n",
      "PeftModelForSeq2SeqLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): MT5ForConditionalGeneration(\n",
      "      (shared): Embedding(250112, 512)\n",
      "      (encoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (decoder): MT5Stack(\n",
      "        (embed_tokens): Embedding(250112, 512)\n",
      "        (block): ModuleList(\n",
      "          (0): MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 6)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-7): 7 x MT5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): MT5LayerSelfAttention(\n",
      "                (SelfAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): MT5LayerCrossAttention(\n",
      "                (EncDecAttention): MT5Attention(\n",
      "                  (q): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "                  (v): lora.Linear(\n",
      "                    (base_layer): Linear(in_features=512, out_features=384, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Dropout(p=0.1, inplace=False)\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=384, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (2): MT5LayerFF(\n",
      "                (DenseReluDense): MT5DenseGatedActDense(\n",
      "                  (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "                  (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): NewGELUActivation()\n",
      "                )\n",
      "                (layer_norm): MT5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): MT5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의 및 최적화 모델 로드\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f'PyTorch Version: {torch.__version__}, Device: {device}')\n",
    "\n",
    "# 같은 구조의 모델 초기화\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/mt5-small')\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/mt5-small') # MT5 모델 불러온다\n",
    "\n",
    "# 모델 본체 동결 처리: Feature Extraction\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "        \n",
    "# LoRA 적용: LoRA 모듈만 학습되도록 설정(경량 파인튜닝)\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # 작은 rank(r=8)로 효율적인 파인튜닝 가능\n",
    "    lora_alpha=32, # LoRA scaling factor\n",
    "    # target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    target_modules=['q', 'v'], # Attention 모듈의 Query/Value 부분에 LoRA 레이어 추가\n",
    "    lora_dropout=0.1, # 드롭아웃\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM' # 대화 응답 생성은 Seq2Seq LM\n",
    ")\n",
    "# LoRA 모델 생성\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 저장된 state_dict 불러오기\n",
    "model.load_state_dict(torch.load('./llm_models/21_transformer_dialogue_chatbot/best_model.pt'))\n",
    "\n",
    "# 추론/검증 모드 적용\n",
    "model.eval()\n",
    "\n",
    "# 모델 전체를 GPU/CPU 디바이스 메모리로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 모델 확인\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8f640e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변 1: , 남산타워 등의 유명 관광지가 경복궁입니다.\n",
      "답변 2: , 남산타워 등의 유명 관광지가 경복궁입니다. 흥미로운 관광지가 있어요.\n",
      "답변 3: , 남산타워 등의 유명 관광지가 경복궁입니다. 흥미로운 관광지가 있나요?\n"
     ]
    }
   ],
   "source": [
    "# 멀티 답변 생성\n",
    "\n",
    "import re\n",
    "\n",
    "# context = \"서울에서 유명한 관광지 알려줄래?\"\n",
    "context = \"서울의 유명 관광지는 경복궁, 남산타워, 명동입니다.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    context,\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "\n",
    "# Beam search + Sampling 혼합\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=128,\n",
    "    min_length=8, # 최소 길이 강제\n",
    "\n",
    "    num_beams=5, # 5~7 정도가 대화형 모델에서는 가장 균형이 좋다\n",
    "    early_stopping=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.85, # 0.9 → 0.6로 줄여서 불필요한 변형을 줄인다, 변형 단어 출력 감소\n",
    "    temperature=0.8, # 0.7 → 0.5 정도로 낮추면 더 안정적인 답변을 얻을 수 있다\n",
    "    num_return_sequences=3,\n",
    "\n",
    "    repetition_penalty=2.0, # 반복 억제\n",
    "    no_repeat_ngram_size=4, # 4-gram 반복 금지\n",
    "    length_penalty=1.2 # 1.2~2.0 정도로 설정하면, 모델이 불필요하게 길게 반복하는 걸 줄인다\n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # <extra_id_n> 토큰 제거\n",
    "    decoded = re.sub(r'<extra_id_\\d+>', '', decoded).strip()\n",
    "    decoded = decoded.replace('..', '.').replace('??', '?')\n",
    "    decoded = re.sub(r'북산타워|서산타워|여산타워|한산타워', '남산타워', decoded)\n",
    "    decoded = re.sub(r'^,?\\s*의 유명 관광지', '서울의 유명 관광지는', decoded)\n",
    "    decoded = re.sub(r'(국립공원|남산타워)(,\\s*\\1)+', r'\\1', decoded)  # 중복 제거\n",
    "    decoded = re.sub(r'궁금한 관광지입니다', '관광 명소로 잘 알려져 있습니다', decoded)\n",
    "\n",
    "    print(f'답변 {i+1}: {decoded}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
